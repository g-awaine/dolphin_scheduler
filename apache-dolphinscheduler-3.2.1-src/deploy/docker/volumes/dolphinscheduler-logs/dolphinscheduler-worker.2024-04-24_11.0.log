[WI-0][TI-0] - [INFO] 2024-04-24 11:00:40.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7810650887573964 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:04.882 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7439446366782008 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:08.103 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1443, taskName=search for intake JSON files, firstSubmitTime=1713927668089, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=110, appIds=null, processInstanceId=578, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1443'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424110108'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='578'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.106 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.106 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.108 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713927668109
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 578_1443
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1443,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713927668089,
  "startTime" : 1713927668109,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/110/578/1443.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 110,
  "processInstanceId" : 578,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1443"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424110108"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "578"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "578_1443",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.109 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.110 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.112 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1443 check successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.122 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.123 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.123 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.124 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.125 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1443/578_1443.sh
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:08.128 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1500
[WI-0][TI-1443] - [INFO] 2024-04-24 11:01:08.772 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1443, success=true)
[WI-0][TI-1443] - [INFO] 2024-04-24 11:01:08.779 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1443)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:09.129 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.135 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1443, processId:1500 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.137 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.137 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.137 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.137 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.151 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1443
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.152 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1443
[WI-578][TI-1443] - [INFO] 2024-04-24 11:01:09.153 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1443] - [INFO] 2024-04-24 11:01:09.765 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1443, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:10.889 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1445, taskName=move to processing, firstSubmitTime=1713927670881, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=110, appIds=null, processInstanceId=578, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1445'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424110110'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='578'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.890 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.891 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.892 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.892 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713927670893
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 578_1445
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1445,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713927670881,
  "startTime" : 1713927670893,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/110/578/1445.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 110,
  "processInstanceId" : 578,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1445"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424110110"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "578"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "578_1445",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.893 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.907 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.909 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.909 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1445 check successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.910 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.911 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.912 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.912 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.913 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.913 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.913 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.914 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.914 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.915 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.916 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.916 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.917 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.917 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1445/578_1445.sh
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:10.920 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1514
[WI-0][TI-1445] - [INFO] 2024-04-24 11:01:11.762 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1445, success=true)
[WI-0][TI-1445] - [INFO] 2024-04-24 11:01:11.769 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1445)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:11.922 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.924 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1445, processId:1514 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.925 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.925 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.925 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.926 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.931 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.932 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.932 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1445
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.932 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1445
[WI-578][TI-1445] - [INFO] 2024-04-24 11:01:11.933 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1445] - [INFO] 2024-04-24 11:01:12.782 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1445, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:12.859 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1446, taskName=spark preprocessing, firstSubmitTime=1713927672854, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=110, appIds=null, processInstanceId=578, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    -packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null},{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1446'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424110112'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='578'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.860 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.861 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.862 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.862 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713927672862
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 578_1446
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.862 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1446,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713927672854,
  "startTime" : 1713927672862,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/110/578/1446.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 110,
  "processInstanceId" : 578,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    -packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null},{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1446"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424110112"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "578"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "578_1446",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.863 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.864 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.864 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.866 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.866 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446 check successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.867 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446/spark_reddit_preprocessing.py), file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar, resourceRelativePath=spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar)})
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.871 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.872 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    -packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  }, {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar",
    "res" : null
  } ]
}
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.872 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.881 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.882 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/500-20240423.json \
    -packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.883 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.883 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446/578_1446.sh
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:12.888 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1526
[WI-0][TI-1446] - [INFO] 2024-04-24 11:01:13.780 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1446, success=true)
[WI-0][TI-1446] - [INFO] 2024-04-24 11:01:13.812 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1446)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:13.889 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	Error: Unrecognized option: -packages
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:14.900 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
	Usage: spark-submit --kill [submission ID] --master [spark://...]
	Usage: spark-submit --status [submission ID] --master [spark://...]
	Usage: spark-submit run-example [options] example-class [example args]
	
	Options:
	  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,
	                              k8s://https://host:port, or local (Default: local[*]).
	  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally ("client") or
	                              on one of the worker machines inside the cluster ("cluster")
	                              (Default: client).
	  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
	  --name NAME                 A name of your application.
	  --jars JARS                 Comma-separated list of jars to include on the driver
	                              and executor classpaths.
	  --packages                  Comma-separated list of maven coordinates of jars to include
	                              on the driver and executor classpaths. Will search the local
	                              maven repo, then maven central and any additional remote
	                              repositories given by --repositories. The format for the
	                              coordinates should be groupId:artifactId:version.
	  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while
	                              resolving the dependencies provided in --packages to avoid
	                              dependency conflicts.
	  --repositories              Comma-separated list of additional remote repositories to
	                              search for the maven coordinates given with --packages.
	  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place
	                              on the PYTHONPATH for Python apps.
	  --files FILES               Comma-separated list of files to be placed in the working
	                              directory of each executor. File paths of these files
	                              in executors can be accessed via SparkFiles.get(fileName).
	  --archives ARCHIVES         Comma-separated list of archives to be extracted into the
	                              working directory of each executor.
	
	  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.
	  --properties-file FILE      Path to a file from which to load extra properties. If not
	                              specified, this will look for conf/spark-defaults.conf.
	
	  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).
	  --driver-java-options       Extra Java options to pass to the driver.
	  --driver-library-path       Extra library path entries to pass to the driver.
	  --driver-class-path         Extra class path entries to pass to the driver. Note that
	                              jars added with --jars are automatically included in the
	                              classpath.
	
	  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).
	
	  --proxy-user NAME           User to impersonate when submitting the application.
	                              This argument does not work with --principal / --keytab.
	
	  --help, -h                  Show this help message and exit.
	  --verbose, -v               Print additional debug output.
	  --version,                  Print the version of current Spark.
	
	 Spark Connect only:
	   --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,
	                              sc://host:port. --master and --deploy-mode cannot be set
	                              together with this option. This option is experimental, and
	                              might change between minor releases.
	
	 Cluster deploy mode only:
	  --driver-cores NUM          Number of cores used by the driver, only in cluster mode
	                              (Default: 1).
	
	 Spark standalone or Mesos with cluster deploy mode only:
	  --supervise                 If given, restarts the driver on failure.
	
	 Spark standalone, Mesos or K8s with cluster deploy mode only:
	  --kill SUBMISSION_ID        If given, kills the driver specified.
	  --status SUBMISSION_ID      If given, requests the status of the driver specified.
	
	 Spark standalone and Mesos only:
	  --total-executor-cores NUM  Total cores for all executors.
	
	 Spark standalone, YARN and Kubernetes only:
	  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in
	                              YARN and K8S modes, or all available cores on the worker
	                              in standalone mode).
	
	 Spark on YARN and Kubernetes only:
	  --num-executors NUM         Number of executors to launch (Default: 2).
	                              If dynamic allocation is enabled, the initial number of
	                              executors will be at least NUM.
	  --principal PRINCIPAL       Principal to be used to login to KDC.
	  --keytab KEYTAB             The full path to the file that contains the keytab for the
	                              principal specified above.
	
	 Spark on YARN only:
	  --queue QUEUE_NAME          The YARN queue to submit to (Default: "default").
	      
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.904 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446, processId:1526 ,exitStatusCode:1 ,processWaitForStatus:true ,processExitValue:1
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.904 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.905 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.906 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.939 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: FAILURE to master : 172.18.1.1:1234
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.940 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.947 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.970 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_110/578/1446
[WI-578][TI-1446] - [INFO] 2024-04-24 11:01:14.977 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1446] - [INFO] 2024-04-24 11:01:15.766 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1446, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:32.022 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7177177177177178 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:36.250 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1447, taskName=search for intake JSON files, firstSubmitTime=1713927696236, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=579, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1447'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424110136'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='579'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.251 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.251 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.256 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.256 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.256 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.256 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713927696256
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.256 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 579_1447
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.256 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1447,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713927696236,
  "startTime" : 1713927696256,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/579/1447.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 579,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1447"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424110136"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "579"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "579_1447",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.257 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.257 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.257 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.261 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1447 check successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.263 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.264 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.264 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.265 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.265 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.267 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.267 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.268 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.268 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.269 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.269 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.270 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1447/579_1447.sh
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:36.273 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1574
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:36.282 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1447] - [INFO] 2024-04-24 11:01:36.843 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1447, success=true)
[WI-0][TI-1447] - [INFO] 2024-04-24 11:01:36.868 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1447)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:37.284 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.285 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1447, processId:1574 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.290 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1447
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1447
[WI-579][TI-1447] - [INFO] 2024-04-24 11:01:37.291 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1447] - [INFO] 2024-04-24 11:01:37.840 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1447, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:38.960 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1449, taskName=move to processing, firstSubmitTime=1713927698937, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=579, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1449'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424110138'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='579'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.962 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.963 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.965 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.965 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.965 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713927698965
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.966 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 579_1449
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.966 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1449,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713927698937,
  "startTime" : 1713927698965,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/579/1449.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 579,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1449"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424110138"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "579"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "579_1449",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.967 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.969 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.969 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.975 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.976 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1449 check successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.976 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.976 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.977 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.978 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.979 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.979 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.979 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.979 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1449/579_1449.sh
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:38.988 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1587
[WI-0][TI-1449] - [INFO] 2024-04-24 11:01:39.847 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1449, success=true)
[WI-0][TI-1449] - [INFO] 2024-04-24 11:01:39.862 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1449)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:39.995 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:39.999 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1449, processId:1587 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:39.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:39.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:39.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:40.000 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:40.016 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:40.016 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:40.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1449
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:40.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1449
[WI-579][TI-1449] - [INFO] 2024-04-24 11:01:40.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1449] - [INFO] 2024-04-24 11:01:40.869 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1449, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:40.999 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1450, taskName=spark preprocessing, firstSubmitTime=1713927700989, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=579, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null},{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1450'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424110140'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='579'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.000 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.001 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.002 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.002 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713927701002
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 579_1450
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.002 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1450,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713927700989,
  "startTime" : 1713927701002,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/579/1450.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 579,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null},{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1450"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424110140"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "579"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "579_1450",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.003 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.010 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.011 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450 check successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.017 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_reddit_preprocessing.py), file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar, resourceRelativePath=spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar)})
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.018 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  }, {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar",
    "res" : null
  } ]
}
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.018 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.019 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.019 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.019 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.020 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/500-20240423.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.020 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.021 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/579_1450.sh
[WI-579][TI-1450] - [INFO] 2024-04-24 11:01:41.024 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1599
[WI-0][TI-1450] - [INFO] 2024-04-24 11:01:41.879 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1450, success=true)
[WI-0][TI-1450] - [INFO] 2024-04-24 11:01:41.886 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1450)
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:42.025 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:43.037 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-4041cb43-2c12-4d3f-bc65-ec9d67170bbf;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:44.039 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:48.053 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:49.056 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
		[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (419ms)
	:: resolution report :: resolve 5205ms :: artifacts dl 437ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   1   |   1   |   0   ||   12  |   1   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-4041cb43-2c12-4d3f-bc65-ec9d67170bbf
		confs: [default]
		12 artifacts copied, 0 already retrieved (57876kB/73ms)
	24/04/24 11:01:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:51.071 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:50 INFO SparkContext: Running Spark version 3.5.1
	24/04/24 11:01:50 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:01:50 INFO SparkContext: Java version 1.8.0_402
	24/04/24 11:01:50 INFO ResourceUtils: ==============================================================
	24/04/24 11:01:50 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/24 11:01:50 INFO ResourceUtils: ==============================================================
	24/04/24 11:01:50 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/24 11:01:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/24 11:01:50 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/24 11:01:50 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/24 11:01:50 INFO SecurityManager: Changing view acls to: default
	24/04/24 11:01:50 INFO SecurityManager: Changing modify acls to: default
	24/04/24 11:01:50 INFO SecurityManager: Changing view acls groups to: 
	24/04/24 11:01:50 INFO SecurityManager: Changing modify acls groups to: 
	24/04/24 11:01:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/24 11:01:50 INFO Utils: Successfully started service 'sparkDriver' on port 39031.
	24/04/24 11:01:50 INFO SparkEnv: Registering MapOutputTracker
	24/04/24 11:01:50 INFO SparkEnv: Registering BlockManagerMaster
	24/04/24 11:01:50 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/24 11:01:50 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/24 11:01:50 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/24 11:01:51 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e0c842e0-b646-4246-b7fc-f930b53e1bfa
	24/04/24 11:01:51 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:51.177 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8032258064516129 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:52.077 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:51 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/24 11:01:51 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/24 11:01:51 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:39031/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:39031/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://6b0330d1bc18:39031/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://6b0330d1bc18:39031/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://6b0330d1bc18:39031/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://6b0330d1bc18:39031/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://6b0330d1bc18:39031/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://6b0330d1bc18:39031/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://6b0330d1bc18:39031/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://6b0330d1bc18:39031/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://6b0330d1bc18:39031/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://6b0330d1bc18:39031/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO SparkContext: Added file file:///local_storage/reddit/processing/500-20240423.json at file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /local_storage/reddit/processing/500-20240423.json to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/500-20240423.json
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:01:51 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:01:51 INFO Executor: Starting executor ID driver on host 6b0330d1bc18
	24/04/24 11:01:51 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:01:51 INFO Executor: Java version 1.8.0_402
	24/04/24 11:01:51 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/24 11:01:51 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@fccfca5 for default.
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /local_storage/reddit/processing/500-20240423.json has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/500-20240423.json
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:01:51 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:51 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:53.081 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:52 INFO TransportClientFactory: Successfully created connection to 6b0330d1bc18/172.18.1.1:39031 after 81 ms (0 ms spent in bootstraps)
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp2418835458312320597.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp2418835458312320597.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp6972581839957253849.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp6972581839957253849.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp6874452777967531596.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp6874452777967531596.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp6129694987652124760.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp6129694987652124760.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp3992210038831716542.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp3992210038831716542.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp3267190799552081036.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp3267190799552081036.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp7803492776524188623.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp7803492776524188623.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp3275611748444264958.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp3275611748444264958.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp4369429145124555260.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp4369429145124555260.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp4188447568268642986.tmp
	24/04/24 11:01:52 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp4188447568268642986.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:01:52 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/24 11:01:52 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713927710051
	24/04/24 11:01:52 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp8185346946548661243.tmp
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:54.096 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:53 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp8185346946548661243.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:01:53 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/24 11:01:53 INFO Executor: Fetching spark://6b0330d1bc18:39031/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713927710051
	24/04/24 11:01:53 INFO Utils: Fetching spark://6b0330d1bc18:39031/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp8928799603736238374.tmp
	24/04/24 11:01:53 INFO Utils: /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/fetchFileTemp8928799603736238374.tmp has been previously copied to /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:01:53 INFO Executor: Adding file:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/24 11:01:53 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46775.
	24/04/24 11:01:53 INFO NettyBlockTransferService: Server created on 6b0330d1bc18:46775
	24/04/24 11:01:53 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/24 11:01:53 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6b0330d1bc18, 46775, None)
	24/04/24 11:01:53 INFO BlockManagerMasterEndpoint: Registering block manager 6b0330d1bc18:46775 with 366.3 MiB RAM, BlockManagerId(driver, 6b0330d1bc18, 46775, None)
	24/04/24 11:01:53 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6b0330d1bc18, 46775, None)
	24/04/24 11:01:53 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6b0330d1bc18, 46775, None)
	
	
	
	 /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/500-20240423.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:55.099 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:54 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.6 KiB, free 366.0 MiB)
	24/04/24 11:01:54 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/04/24 11:01:54 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6b0330d1bc18:46775 (size: 32.7 KiB, free: 366.3 MiB)
	24/04/24 11:01:54 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/24 11:01:54 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/24 11:01:54 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:55.241 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8267045454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:01:59.101 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:58 INFO CodeGenerator: Code generated in 240.719573 ms
	24/04/24 11:01:58 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:01:58 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:01:58 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/24 11:01:58 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:01:58 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/24 11:01:58 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:01:58 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:01:58 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:01:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.8 KiB, free 365.9 MiB)
	24/04/24 11:01:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.9 MiB)
	24/04/24 11:01:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 6b0330d1bc18:46775 (size: 12.3 KiB, free: 366.3 MiB)
	24/04/24 11:01:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:01:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:01:58 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/24 11:01:58 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:01:58 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:00.103 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:01:59 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/500-20240423.json:0+147333
	24/04/24 11:01:59 INFO CodeGenerator: Code generated in 98.045834 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:01.111 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:02:00 INFO PythonRunner: Times: total = 764, boot = 434, init = 289, finish = 41
	24/04/24 11:02:00 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2990 bytes result sent to driver
	24/04/24 11:02:00 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1256 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:02:00 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/24 11:02:00 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 40261
	24/04/24 11:02:00 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.412 s
	24/04/24 11:02:00 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:02:00 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/24 11:02:00 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.503376 s
	
	
	
	 1 
	
	
	
	
	
	
	 2 
	
	
	
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
	24/04/24 11:02:00 INFO CodeGenerator: Code generated in 50.852258 ms
	24/04/24 11:02:00 INFO CodeGenerator: Code generated in 10.20758 ms
	24/04/24 11:02:01 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_reddit_preprocessing.py:84
	24/04/24 11:02:01 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/24 11:02:01 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_reddit_preprocessing.py:84)
	24/04/24 11:02:01 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:02:01 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:02:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:02:01 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 68.1 KiB, free 365.8 MiB)
	24/04/24 11:02:01 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.3 KiB, free 365.8 MiB)
	24/04/24 11:02:01 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 6b0330d1bc18:46775 (size: 25.3 KiB, free: 366.2 MiB)
	24/04/24 11:02:01 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:02:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:02:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/24 11:02:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:02:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:02.118 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:02:01 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/userFiles-9601d021-4531-4dc9-93fd-c969ac3d5069/500-20240423.json:0+147333
	24/04/24 11:02:01 INFO CodeGenerator: Code generated in 75.737398 ms
	24/04/24 11:02:01 INFO CodeGenerator: Code generated in 12.047985 ms
	24/04/24 11:02:01 INFO CodeGenerator: Code generated in 33.607474 ms
	24/04/24 11:02:01 INFO CodeGenerator: Code generated in 11.74942 ms
	24/04/24 11:02:01 INFO PythonRunner: Times: total = 176, boot = -1072, init = 1225, finish = 23
	24/04/24 11:02:02 INFO CodeGenerator: Code generated in 40.955381 ms
	24/04/24 11:02:02 INFO CodeGenerator: Code generated in 38.839889 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:03.120 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:02:02 INFO PythonUDFRunner: Times: total = 667, boot = 499, init = 166, finish = 2
	24/04/24 11:02:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 135926 bytes result sent to driver
	24/04/24 11:02:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1075 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:02:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/24 11:02:02 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_reddit_preprocessing.py:84) finished in 1.088 s
	24/04/24 11:02:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:02:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/24 11:02:02 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450/spark_reddit_preprocessing.py:84, took 1.095395 s
	{"user":"NekoIdo","content":"https://preview.redd.it/u68bjywya5wc1.png?width=808&format=png&auto=webp&s=d42529d6566d93e3df2f8be398afd887a258d020\n\nFound the deleted post [here](https://www.reddit.com/r/SGExams/comments/1capzon/hi_i_am_the_driver_of_snh7z_i_would_like_to_clear/).  \nMain content has been deleted by mods but the comments are definitely not buying his excuse.\n\nAlso someone else posted another captured instance of this same driver being a road menace here:  \n[https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/](https://forums.hardwarezone.com.sg/threads/gvgt-snh7z-mercedes-glc43-tailgating-cambike-try-to-force-cambike-off-the-road.7022346/)\n\nPersonally I think it's really lucky for this Benz driver that the SAAB driver survived, or the spotlight would shine solely on him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":341,"subreddit":"singapore","type":"post","id":"1cau62k","datetime":"2024-04-23 11:06:00"}
	{"user":"dylank999","content":"Speeding up to prevent people from changing lane/overtaking. \n\nNumber 1 ego of Singapore drivers. It's a pandemic on our roads.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9els/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":395,"subreddit":"singapore","type":"comment","id":"l0u9els","datetime":"2024-04-23 11:38:17"}
	{"user":"pureeyes","content":"All he had to do was say nothing","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uau6z/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":197,"subreddit":"singapore","type":"comment","id":"l0uau6z","datetime":"2024-04-23 11:49:38"}
	{"user":"PhantomWolf83","content":">my thought was to tell the SAAB driver to slow down, hence I sped up while he was overtaking\n\n???????","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u5ykg/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":365,"subreddit":"singapore","type":"comment","id":"l0u5ykg","datetime":"2024-04-23 11:11:50"}
	{"user":"I_Miss_Every_Shot","content":"Basic Theory or Advance Theory should include a psycho-emotional assessment. People who registered increased heart rate/ BP/ stress or inclination to speed up/ overtake when overtaken or have their lane cut into should undergo mandatory counselling or be denied their license altogether.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uas8s/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":115,"subreddit":"singapore","type":"comment","id":"l0uas8s","datetime":"2024-04-23 11:49:11"}
	{"user":"BinaryDoom","content":"It's precisely because there are immature drivers on the road, such behavior would only make driving in Singapore unpleasant. The white Merc driver is not the only one. It could have been someone else. Anyway, based on the footage, from the way he has executed it, I wouldn't buy his claim of wanting the black driver to slow down by speeding up. Only when you practice enough of this, it becomes a natural reaction.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uaru6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":49,"subreddit":"singapore","type":"comment","id":"l0uaru6","datetime":"2024-04-23 11:49:06"}
	{"user":"testercheong","content":"I'm more curious as to why its posted on sgexams though","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uamri/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":72,"subreddit":"singapore","type":"comment","id":"l0uamri","datetime":"2024-04-23 11:47:59"}
	{"user":"adept1onreddit","content":"Regardless of what he thinks about the whole situation, the smart option would be to keep his mouth shut. Who knows what kind of legal issues may arise.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucey8/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":27,"subreddit":"singapore","type":"comment","id":"l0ucey8","datetime":"2024-04-23 12:02:25"}
	{"user":"Background_Tax_1985","content":"His reasons are bs but that does not deviate from the fact that the accident was 100% caused by the saab driver (sd). \n\nEven before the mercedes driver (md) sped up to block the sd, sd was already speeding. Just a side note, if md and sd were racing each other we would have seen it from the first vehicle camera footage of the accident from the other merc, which we did not. \n\nIs md wrong and a dick? Yes. Did md commit an offence by driving in that manner? Maybe. Did he cause the accident? No.\n\nBecause after side swiping md, sd could have stopped. He had all the time and distance in the world to stop but he decided to speed on into the distance instead. \n\nThere is literally 2 school zones after that junction where the accident took place. If he had not gotten into an accident there, he could potentially have plowed into alot of kids, since the accident took place at school start time.\n\nSd really needs to be serverly punished for this.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u7xwv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":193,"subreddit":"singapore","type":"comment","id":"l0u7xwv","datetime":"2024-04-23 11:26:49"}
	{"user":"blitzmango","content":"So desperate to defend his actions that he created 3 threads but looks like its gonna back fire instead. If it weren't for the second video incident, I might have given him the benefit of doubt, but sorry no.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u7mlk/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":57,"subreddit":"singapore","type":"comment","id":"l0u7mlk","datetime":"2024-04-23 11:24:24"}
	{"user":"top1global","content":"In the end, he is posting it to defend himself and blame others. 0 apology or remorse.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubg1n/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":48,"subreddit":"singapore","type":"comment","id":"l0ubg1n","datetime":"2024-04-23 11:54:35"}
	{"user":"Stripey7676","content":"There’s a huge debate over at EDMW whether any blame should be placed white merc. Now, the saab is 120% at fault. No doubt about that. However, while white merc is not criminally liable, he definitely contributed to the chain of events leading to the accident due to his ungracious, undefensive driving. \n\nFrom the vid, 2 things were evident \n\n1. Saab panic after clipping merc (who sped up to prevent overtaking). \n2. Saab went crazy fast after clipping as if to escape confrontation. \n\nI’m betting saab driver was distracted (likely eyes on rear view mirror),  checking merc reaction  before ploughing through the junction. Looking at the force of the Saab during the accident, No one sane will cheong thru a junction full of vehicles. Saab’s eyes were clearly off the road. \n\nThings would have been very different is merc slowed down, like a defensive driver would, when encountering siao langs on the road.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ud13l/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":42,"subreddit":"singapore","type":"comment","id":"l0ud13l","datetime":"2024-04-23 12:07:31"}
	{"user":"FalseAgent","content":"yet another retarded sg driver. so what if others are speeding? you stick to the speed limit and drive safe. End of story. Everyone do this and play their part, we get safer roads. No need to be hero","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uep1i/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":11,"subreddit":"singapore","type":"comment","id":"l0uep1i","datetime":"2024-04-23 12:21:36"}
	{"user":"Ceeja_y","content":"he wanted to post here but his acc didnt meet the requirements lol. oh well, he would still get an earful even if he did post here","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u77vi/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":34,"subreddit":"singapore","type":"comment","id":"l0u77vi","datetime":"2024-04-23 11:21:18"}
	{"user":"BakeMate","content":"Hope the guilt eats him from inside. With more video surfacing of his driving behavior, he can write 10000 words essay all he wants, still a piece of shit\n\n> assuming I'm always like that\n\nYes you are. Nothing changed. It's a matter of whether you're caught on camera or not, and whether people want to upload","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u7lcc/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":76,"subreddit":"singapore","type":"comment","id":"l0u7lcc","datetime":"2024-04-23 11:24:08"}
	{"user":"MyColdDeadHandz","content":">As for the motorcycle incident, the motorcyclist was hogging the road, preventing me from going forward\n\nBut that was a single lane? Motorcyclist had all the right to be there in the middle of that lane. I've seen many drivers file up in two lines (when the road was wide enough, like in the clip) when filtering into an expressway, but this is the first time I've seen someone feel entitled in doing it.\n\nSounds like a real piece of work.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uahrp/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":19,"subreddit":"singapore","type":"comment","id":"l0uahrp","datetime":"2024-04-23 11:46:52"}
	{"user":"tolifeonline","content":"One young life was ended prematurely. The saying \"no parents should ever have to bury their child\" rings heavy in this incident. Imagine how inconsolable the father will be after waking up to find out his child has been taken away forever from him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uc2j2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":18,"subreddit":"singapore","type":"comment","id":"l0uc2j2","datetime":"2024-04-23 11:59:38"}
	{"user":"dibidi","content":"his lawyers seeing his post: $$$$$$$","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udihn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":22,"subreddit":"singapore","type":"comment","id":"l0udihn","datetime":"2024-04-23 12:11:30"}
	{"user":"Initial_E","content":"In the 2-3 seconds of the incident both drivers were not making a conscious decision to goad each other, instead they were doing what they conditioned themselves to do. \nThere's no point asking \"what was going through his head\", it's what goes through his head every time he is behind the wheel that counts.\n\n\nSo it is with defensive driving. If it is a hat that you wear on your head, it should go on as automatically as wearing your seatbelt, should stay on for about just as long as you are wearing that seatbelt. And you can condition yourself to make it automatic. See, you already did it to wear a seatbelt.\n\nYou can't practice defensive driving only when you are nearly doing something dangerous, that's not how it works.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhxwn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0uhxwn","datetime":"2024-04-23 12:48:53"}
	{"user":"Common-Metal8578","content":"Heard other videos of him being an asshole driver are surfacing","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufo4s/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":7,"subreddit":"singapore","type":"comment","id":"l0ufo4s","datetime":"2024-04-23 12:30:00"}
	{"user":"KneeGal","content":"This Benz driver is an idiot but if you look at his [dashcam](https://imgur.com/a/YuwFzNz), after the contact, there is plenty of road left for the SAAB driver to brake. Instead it seems he sped up and ran directly into traffic. \n\nThe point of contact happened so far away from the junction that his involvement would have made no difference to the outcome.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uea8q/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":24,"subreddit":"singapore","type":"comment","id":"l0uea8q","datetime":"2024-04-23 12:18:02"}
	{"user":"ProfessionalMottsman","content":"Wow what a fool, never say anything in an accident as directed by your insurance company, they can refuse to cover you if you admit liability","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubboi/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":13,"subreddit":"singapore","type":"comment","id":"l0ubboi","datetime":"2024-04-23 11:53:37"}
	{"user":"RiskDry6267","content":"This merc also scum, open mouth to dig ownself bigger hole, edmw hurry up name and shame !","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhzil/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":6,"subreddit":"singapore","type":"comment","id":"l0uhzil","datetime":"2024-04-23 12:49:17"}
	{"user":"Yokies","content":"Dude can't keep his mouth shut to save himself","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9hwk/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":19,"subreddit":"singapore","type":"comment","id":"l0u9hwk","datetime":"2024-04-23 11:38:59"}
	{"user":"isleftisright","content":"Sd caused the accident by himself. He clearly had time to stop at the light but didnt. \n\nMd is a rude driver nonethless","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubdok/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":12,"subreddit":"singapore","type":"comment","id":"l0ubdok","datetime":"2024-04-23 11:54:03"}
	{"user":"CedaraThursday1314","content":"Self saboing.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uawb2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":10,"subreddit":"singapore","type":"comment","id":"l0uawb2","datetime":"2024-04-23 11:50:06"}
	{"user":"dibidi","content":"the root cause here is that there is no driving culture of “giving way”. \n\nevery driver drives like a race car driver. bec they see any pass by any other road user as a slight and sign of disrespect. \n\nroad safety will never be addressed in Singapore as long as gov doesn’t address this culture.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udrb4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":10,"subreddit":"singapore","type":"comment","id":"l0udrb4","datetime":"2024-04-23 12:13:32"}
	{"user":"Formal_Judgment6481","content":"One of the most dumbass excuses. Hope this accident will forever haunt you","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugemm/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0ugemm","datetime":"2024-04-23 12:35:58"}
	{"user":"peterthewiserock","content":"Benz driver’s lawyer is going to have a field day with that post.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugfe6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":6,"subreddit":"singapore","type":"comment","id":"l0ugfe6","datetime":"2024-04-23 12:36:07"}
	{"user":"subzephyr","content":"why tf he posting on a kids subreddit","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucyms/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":9,"subreddit":"singapore","type":"comment","id":"l0ucyms","datetime":"2024-04-23 12:06:57"}
	{"user":"Ok-Tap4277","content":"Terrible cost to his behaviour escalating a bad situation","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufo9n/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0ufo9n","datetime":"2024-04-23 12:30:02"}
	{"user":"pudding567","content":"What if the driver decides to sue for defamation? There is a Defence of Justification (truth) and Fair Comment though.\nhttps://irblaw.com.sg/learning-centre/defamation-in-singapore","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh2cj/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uh2cj","datetime":"2024-04-23 12:41:17"}
	{"user":"meister00","content":"He might or might not have indirect involvement in the accident (the accident might have still occurred even if he wasn't there), but regardless of that the merc driver is sure one of those asshole drivers.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh9dc/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uh9dc","datetime":"2024-04-23 12:42:55"}
	{"user":"raymmm","content":"Just like how he sped up to \"tell\" the other driver to slow down. People are treating him like shit to \"tell\" him to stop being an asshole to other motorists.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhdcb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uhdcb","datetime":"2024-04-23 12:43:52"}
	{"user":"DiscipleOfYeshua","content":"1. This the guy behind the one who rushed into red light, yes?\n\nThe one behind directly (on right) or the one on left?\n\n2. “Motorcycle prevented me moving forward” means “too slow for my taste” / “made me lose 12.3 seconds on way to red light”? I mean, motorcycle didn’t outright stop in the road, was just 10 kph too slow for this guy?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhv2v/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uhv2v","datetime":"2024-04-23 12:48:11"}
	{"user":"jayaxe79","content":"Seems like the more he explained, more questions arises.\n\n- So he tried to push the blame away yet doesn't blame everyone for pointing the finger at him?\n\n- Did his actions actually indirectly caused the Saab driver to go crazy? By just speeding up to prevent overtaking? \n\nHe claimed that he isn't \"like the person outside these incidents\". Who the hell believes this statement?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u8jiu/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":13,"subreddit":"singapore","type":"comment","id":"l0u8jiu","datetime":"2024-04-23 11:31:31"}
	{"user":"okayokaycancan","content":"Take both off the roads and it will be a safer place.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ua4b7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0ua4b7","datetime":"2024-04-23 11:43:52"}
	{"user":"stickytofw","content":"Everything he does from his driving, both during the time leading up to and outside of the recent tragedy, to his Reddit “apology” message just screams “ego”.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugc7m/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0ugc7m","datetime":"2024-04-23 12:35:27"}
	{"user":"xthekonmanx","content":"And still not a single apology or sympathy for the accident victims… super lanjiao lang! He did apologize for the long post though… 🙄","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh9tv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uh9tv","datetime":"2024-04-23 12:43:02"}
	{"user":"AdministrativeGas324","content":"This guy is so smart that he bid for his car licence plate number using his IQ score.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ui1li/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0ui1li","datetime":"2024-04-23 12:49:48"}
	{"user":"Merecat-litters","content":"uh why this bastard what to \"Self Justification\" here. fk you mate...give Self Justification when giving your testimony in court you donkey see if they like.\n\nYou only saying this because you were one of the factor of the accident and were caught in the act you and are not sorry for it to happen but sorry for getting caught in the act. The way I am interpreting the writing is  to make you not be responsible fully but blame it on others. knnccb\n\nHopefully mod dont remove this too...and hopefully we can see this fucker face when they go to court cant wait to see the case.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ui2c0/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0ui2c0","datetime":"2024-04-23 12:49:59"}
	{"user":"crassina","content":"Can he also be charged with dangerous driving causing death? Because his actions seemed pretty dangerous to me","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uicnq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uicnq","datetime":"2024-04-23 12:52:34"}
	{"user":"random_thoughts5","content":"Why did he post it in the sgexams subreddit? Isn’t that a subreddit for secondary and JC students?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9tgk/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":7,"subreddit":"singapore","type":"comment","id":"l0u9tgk","datetime":"2024-04-23 11:41:32"}
	{"user":"noelsupertramp","content":"Two cars racing each other on the premise of trying to slow the other party down. Nobody is gonna buy this reason.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubkkn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0ubkkn","datetime":"2024-04-23 11:55:38"}
	{"user":"AsuraPhantoma","content":"The moment you sped up near a cross junction (or really, speeding up in general) you already lost the right to argue or say anything","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uiai2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0uiai2","datetime":"2024-04-23 12:52:01"}
	{"user":"laubase","content":"I have never seen a Merc being driven competently on Singapore roads in 37 years. No signal before switching lanes? Probably a Merc. Intentionally cutting queues? Probably a Merc. Not checking blind spot before switching lanes? Probably a Merc. \"Lane splitting\" to deny others from overtaking them? Probably a Merc.The list goes on and on. \n\nI swear something drives (pun intended) these kinda people to buy Mercs, probably because they're trying to signal (again, pun intended) their superiority to the average Singaporean. \"Look at what I can afford, you plebs better give way to me because I am THAT much more important than you.\".\n\nAnyone buying a Merc should be subject to a compulsory TP road test.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uevgx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0uevgx","datetime":"2024-04-23 12:23:07"}
	{"user":"freshcheesepie","content":"Good I can sleep well knowing that the comments are affecting him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u6rnh/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":9,"subreddit":"singapore","type":"comment","id":"l0u6rnh","datetime":"2024-04-23 11:17:47"}
	{"user":"DeeKayNineNine","content":"To be fair, the Merc driver didn't cause the accident and didn't contribute to the cause of the accident. It would have happen with or without the Merc driver. The Saab was already speeding. \n\nWe still don't know what is wrong with that Saab driver. Was he drunk? Under drug? No license? Or just some reckless driver? We don't know yet. Let's wait for the authorities to release more info. \n\nThe merc driver is a jerk. You can see that from the other video. And I'm sure there are more. But he didn't cause the accident. And I hope this accident is a wake up call for him and will make him change to be a better driver from now on. Else we can be sure we'll see him online again.\n\nBy the way, I wonder if the Merc has car camera. I think he got an unblock view of the entire accident.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufjru/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":6,"subreddit":"singapore","type":"comment","id":"l0ufjru","datetime":"2024-04-23 12:28:57"}
	{"user":"Prov0st","content":"Why is everyone’s eyes on the white mercs?? The SAAB was doing the speeding. The white mercs is not free of sin but ultimately the direct culprit is the saab.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ua3pq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":10,"subreddit":"singapore","type":"comment","id":"l0ua3pq","datetime":"2024-04-23 11:43:44"}
	{"user":"Dulio_rosward","content":"If he truly felt guilty, then he should have been reflecting more on his actions since he contributed to the loss of those 2 lives.\n\nInstead he chose to spend his time trying to rationalise his actions to netizens.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u8inb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":12,"subreddit":"singapore","type":"comment","id":"l0u8inb","datetime":"2024-04-23 11:31:20"}
	{"user":"Chrissylumpy21","content":"This guy needs to be banned from driving. He’s just sorry he got caught that’s all.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9s9o/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":7,"subreddit":"singapore","type":"comment","id":"l0u9s9o","datetime":"2024-04-23 11:41:16"}
	{"user":"deangsana","content":"he doth protest too much","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uegtx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uegtx","datetime":"2024-04-23 12:19:40"}
	{"user":"ThenCheesecake","content":"shouldn’t this “explanation” be taken with a grain of salt cause we can’t confirm it was actually written by the driver themselves?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufqt7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0ufqt7","datetime":"2024-04-23 12:30:37"}
	{"user":"PartTimeBomoh","content":"Streisand effect. I didn’t think anything of him till he decided to write this post","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugo92/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0ugo92","datetime":"2024-04-23 12:38:01"}
	{"user":"canceler80","content":"No proof that user is actual driver. Can be a troll account","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uixvo/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uixvo","datetime":"2024-04-23 12:58:00"}
	{"user":"livinglifeingrieve","content":"Lawyer: okay. 2 years jail. If good boy can come out in 8 months. 5 years ban on driving.\nJudge: okeh","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uj8x8/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uj8x8","datetime":"2024-04-23 13:00:50"}
	{"user":"Familiar_Guava_2860","content":"Where is the logic in one ‘speeding up ‘ to tell \nan overtaking driver to ‘slow down’?!","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uj9pb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uj9pb","datetime":"2024-04-23 13:01:03"}
	{"user":"FitCranberry","content":"just another reminder that there can be people out there with a completely different headspace thats incomprehensible to yourself","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujkc6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujkc6","datetime":"2024-04-23 13:03:48"}
	{"user":"Noobcakes19","content":"some of my previous post about this asshole was downvoted.\n\nwe saw his action on various dashcam including his own, He's indeed a major egoistic asshole.\n\neveryone : White merc driver is an asshole\n\nwhite merc driver:  Hold my beer..... I AM AN asshole!","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujtzb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujtzb","datetime":"2024-04-23 13:06:18"}
	{"user":"Dizzy_Boysenberry499","content":"If you check One Motoring, he deregistered his car on 22 April","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uk0k6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uk0k6","datetime":"2024-04-23 13:08:02"}
	{"user":"KBDMASS","content":"surprisingly, most drivers have such mentality of “speeding” so that the other person wont over take.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uklgb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uklgb","datetime":"2024-04-23 13:13:37"}
	{"user":"signinj","content":"“Oh no! The internet is judging me for my driving! Better come up with a WOT. That surely will make things better”","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ule7o/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ule7o","datetime":"2024-04-23 13:21:28"}
	{"user":"heyearthdude","content":"His stupidity knows no bounds. Who uses words like “overkill” when writing about a fatal accident.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ultcm/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ultcm","datetime":"2024-04-23 13:25:41"}
	{"user":"Vindicted1501","content":"Just surrender to TP and they will investigate how much of a safe driver he is.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0umjes/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0umjes","datetime":"2024-04-23 13:33:00"}
	{"user":"EnvironmentalExam784","content":"Someone need to CSI this douchebag face out please.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0umzee/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0umzee","datetime":"2024-04-23 13:37:38"}
	{"user":"andrew_marc","content":"just by the way it is written you can tell he/she is self entitled/ultra defensive and someone who is never in the wrong","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0umzwi/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0umzwi","datetime":"2024-04-23 13:37:46"}
	{"user":"Frequent-Switch-5699","content":"to the op: what you said, how you explain yourself is no longer useful now. Get ready for a session of formal investigation and get yourself a good lawyer to defend for yourself if you can afford. We shall let the evidence speak for itself.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0un2t4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0un2t4","datetime":"2024-04-23 13:38:37"}
	{"user":"LT-Ghastly","content":"Just arrest this guy as well, for the safety of others and road users.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubcnm/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0ubcnm","datetime":"2024-04-23 11:53:49"}
	{"user":"HalcyoNighT","content":"Wait why tf did the merc driver make his post on r/sgexams? This is not TP exam","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubc60/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0ubc60","datetime":"2024-04-23 11:53:43"}
	{"user":"pyroSeven","content":"Ask him to slow down by speeding up. Very good, didn’t know they taught this at driving school 🙄\n\n\nSuck my lanjiao la.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udw03/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0udw03","datetime":"2024-04-23 12:14:40"}
	{"user":"jackgermeister","content":"Can ask him to post the unmuted version of the dashcam, then? Since he claimed to ‘toot’ the motorcyclist, or ask the Saab driver to slowdown.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uedza/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uedza","datetime":"2024-04-23 12:18:59"}
	{"user":"Alive_Ad7522","content":"What the bull shit. Absolutely no remorse at all. Sick mindset.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u8v1q/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0u8v1q","datetime":"2024-04-23 11:33:55"}
	{"user":"Spiritual-Okra-7836","content":"oh my god this guy is a fucking moron. Take his license away NOW.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ua8ut/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0ua8ut","datetime":"2024-04-23 11:44:52"}
	{"user":"lvofdifficulties","content":"Why did he post at /r/SGExams? He's a student?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9954/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0u9954","datetime":"2024-04-23 11:37:02"}
	{"user":"hawk_199","content":"Haha Streisand effect in play...if he didn't post this within a few days/weeks he would have been forgotten.\n\nGo and post this stupid justification and shoot himself","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhs1l/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uhs1l","datetime":"2024-04-23 12:47:25"}
	{"user":"CrossfittJesus","content":"Might be a hot take here but while the Merz is an asshole driver, I see the precious video of him a couple of days before the Tampines incident and the incident itself as mutually exclusive. IMO he didn’t have that big a role to play in the Saab incident.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uk9qq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uk9qq","datetime":"2024-04-23 13:10:30"}
	{"user":"smurflings","content":"Not agreeing or disagreeing with this guy but why are people chewing him out for that incident? It's entirely the Saab driver's fault.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u8rzg/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0u8rzg","datetime":"2024-04-23 11:33:16"}
	{"user":"[deleted]","content":"[removed]","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ue69a/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ue69a","datetime":"2024-04-23 12:17:05"}
	{"user":"Winner_takesitall","content":"A sad by-product of our hyper competitive society: being unable to give, only know how to take..","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uja1t/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uja1t","datetime":"2024-04-23 13:01:08"}
	{"user":"foodloveroftheworld","content":"**When your 1000-word essay is due but you've only got 100 words of pure self-justification that miss the whole point.**","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukuo6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukuo6","datetime":"2024-04-23 13:16:05"}
	{"user":"MurkyConsideration98","content":"This chap is delulu.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uma4k/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uma4k","datetime":"2024-04-23 13:30:22"}
	{"user":"darrenoloGy","content":"typical narcissistic response.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0un645/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0un645","datetime":"2024-04-23 13:39:37"}
	{"user":"appealban","content":"this here is peak example of egoism/narcissism. gold medal mental gymnastics. hopefully the driver reads this and wake up his idea.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0unvuv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0unvuv","datetime":"2024-04-23 13:47:11"}
	{"user":"PerformanceCheap4074","content":"Gua Lan C**t with his GLC Mercs","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uoefq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uoefq","datetime":"2024-04-23 13:52:42"}
	{"user":"khaosdd","content":"A rich, GoonduLoserCunt-class driving asshole posting on an obscure subreddit?\n\nAlso glc camcar footage was uploaded by one Anthony Soon on fb who already locked his profile and disabled all comments. From the look of his profile and style, he doesn't seem like those who will browse reddit, much less r/SGExams.\n\nSo the op who posted there seems kinda sus to me. Could be an impersonator.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujb9c/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujb9c","datetime":"2024-04-23 13:01:26"}
	{"user":"buffyfluffy","content":"Yall gotta stop finding reasons to blame someone. Speeding on roads happen every single day in every single part of SG. Yes it's wrong, but that's not the reason for the 2 deaths. Speeding dont cause people to speed through a cross junction on a red light. Even on expressways you see those typical Ah beng BMW/VW/Mini speeders, they cut in and out dangerously but still manage to brake on time and cut in when the gap is right. This SAAB driver decided to try out GTA in real life but people still trying to find rationalize and find more people to blame in order to cope with this tragedy.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugr9t/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0ugr9t","datetime":"2024-04-23 12:38:42"}
	{"user":"JakeAndRay","content":"When i first made comments about this guy i get downvoted like no one’s business, now everyone shitting on him? Very good guys. Supreme echo chamber in here. \n\nSame shit happened during the initial covid outbreak. I say just stay home, not difficult exercise at home etc. Learn to think for yourselves guys. Pls downvote me no problem","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ulxz7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ulxz7","datetime":"2024-04-23 13:26:58"}
	{"user":"meowinbox","content":"How can you be so sure that's him though? Could be some reddit troll out to rage bait. Did he give any proof of identity?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u7su1/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-9,"subreddit":"singapore","type":"comment","id":"l0u7su1","datetime":"2024-04-23 11:25:45"}
	{"user":"Calamity_B4_Storm","content":"He thought he is playing uno but it all turn out to be oh no… people Jia sai he also Jia Sai meh.. now he really KNS","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u99sb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-2,"subreddit":"singapore","type":"comment","id":"l0u99sb","datetime":"2024-04-23 11:37:10"}
	{"user":"Ambitious-Kick6468","content":"We should have a law to charge people who speed up when others are overtaking.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uejf9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0uejf9","datetime":"2024-04-23 12:20:17"}
	{"user":"imivan111","content":"Give this cunt the death penalty","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugo5w/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0ugo5w","datetime":"2024-04-23 12:38:00"}
	{"user":"snookajam","content":"All the comments on driving should come with an identifier on how long the commentor has been driving on sg roads. Reading too many ridiculous things on the issue from people who probably don’t know about the intricacies of driving on sg roads, or that road in particular.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ueo81/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-3,"subreddit":"singapore","type":"comment","id":"l0ueo81","datetime":"2024-04-23 12:21:24"}
	{"user":"ProfessionalMottsman","content":"This is so common and it’s so dangerous. Do people not know there doing it ?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ub6wi/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":47,"subreddit":"singapore","type":"comment","id":"l0ub6wi","datetime":"2024-04-23 11:52:31"}
	{"user":"DiscipleOfYeshua","content":"Also boarding bus. \n\nWhen the side-loader aunties and occasional uncle pretend to “not notice” we all queuing, I sometimes hold my hands up to the sunlight to check whether I’ve turned to a transparent ghost.\n\nLess risk of injury and death. Similar mindset.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh7b7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":22,"subreddit":"singapore","type":"comment","id":"l0uh7b7","datetime":"2024-04-23 12:42:27"}
	{"user":"RecognitionIcy7396","content":"I have driven in many countries and my observation is that Singaporeans are quite nice in real life but are really selfish drivers. How is it logical for someone to think that speeding up is going to get someone else to slow down?\n\nAnd if someone wants to come into your lane, maybe just let them? This guy keeps saying he had an emergency to attend to but maybe he should have left earlier or prepared more in advance so that he doesn’t have to use these dangerous tactics. His poor planning shouldn’t put everyone else’s lives in danger.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugndy/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":16,"subreddit":"singapore","type":"comment","id":"l0ugndy","datetime":"2024-04-23 12:37:50"}
	{"user":"Lv3_Potato_Farmer","content":"Just because everyone is doing it doesn’t make it right","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uawvv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":14,"subreddit":"singapore","type":"comment","id":"l0uawvv","datetime":"2024-04-23 11:50:14"}
	{"user":"Realistoliberato","content":"It's of waste of fuel. And fuel is bloody expensive in Singapore","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uizax/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uizax","datetime":"2024-04-23 12:58:22"}
	{"user":"CriticizeSpectacle7","content":"This is the reason sg drivers don't signal. Once u signal, ppl will simply speed up.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ue2rg/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":14,"subreddit":"singapore","type":"comment","id":"l0ue2rg","datetime":"2024-04-23 12:16:16"}
	{"user":"MessyBoomer","content":"I swear some drivers speed up when I approach a pedestrian crossing to try and go through before I start crossing too","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uj6nk/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uj6nk","datetime":"2024-04-23 13:00:15"}
	{"user":"Diligent_Soup_1069","content":"Definitely,many a times I have signaled to exit out of the highway but motorbikes and cars on the slower lanes will just speed up past you suddenly as if you signaling is a sign that there's a GSS right up ahead. \n\nThe mercedes has nothing to do with the accident persay but I do feel TP should at least look into his reckless driving habits and ego issues on the road.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uk6k4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uk6k4","datetime":"2024-04-23 13:09:38"}
	{"user":"MicTest_1212","content":"Now he admitted publicly in black and white that he did speed up to prevent the Saab from taking over. \n\n Stupid, Egoistic, and Narcissistic 💀","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uerc7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":135,"subreddit":"singapore","type":"comment","id":"l0uerc7","datetime":"2024-04-23 12:22:08"}
	{"user":"drinkingbobatea","content":"He had to.... bc he is trying his absolute hardest to be a genuinely nice person...","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucbip/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":45,"subreddit":"singapore","type":"comment","id":"l0ucbip","datetime":"2024-04-23 12:01:39"}
	{"user":"transcendcosmos","content":"I think he had to cos his name was already doxxed in HWZ. Had a separate incident a few days before where he behaved terribly too.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufnzd/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":21,"subreddit":"singapore","type":"comment","id":"l0ufnzd","datetime":"2024-04-23 12:29:57"}
	{"user":"Brief_Worldliness162","content":"Smh","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufl9p/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0ufl9p","datetime":"2024-04-23 12:29:19"}
	{"user":"Fensirulfr","content":"I agree. From the available videos, it is not exactly clear what was the white car's intention at first. He probably should have asked a lawyer to read through what he was about to post online.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ulykc/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ulykc","datetime":"2024-04-23 13:27:08"}
	{"user":"uniquely_ad","content":"I was right all along, drive long enough and I know what the merc was doing","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0un92f/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0un92f","datetime":"2024-04-23 13:40:27"}
	{"user":"fredczar","content":"It’s likely fake? How sure are we that this is the actual driver?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uj10r/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uj10r","datetime":"2024-04-23 12:58:47"}
	{"user":"dylank999","content":"Better to close your mouth and let people think you're stupid than open it and prove it.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujtay/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujtay","datetime":"2024-04-23 13:06:07"}
	{"user":"beklog","content":"same logic as with my ex:\n\n\"I cheated on u so u can pay attention to me\" ¯⁠\\⁠_⁠ಠ⁠_⁠ಠ⁠_⁠/⁠¯","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u6owe/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":225,"subreddit":"singapore","type":"comment","id":"l0u6owe","datetime":"2024-04-23 11:17:12"}
	{"user":"icwiener25","content":"'Don't speed up if you see another vehicle trying to overtake you' is literally one of the first things that is taught in Basic Theory when taking one's license.\n\n  \nAuthorities should probably check whether he even has a valid drivers' license.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udsxf/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":46,"subreddit":"singapore","type":"comment","id":"l0udsxf","datetime":"2024-04-23 12:13:55"}
	{"user":"Prov0st","content":"Typical mindset of some Singaporean drivers, overtaking/ signalling = I MUST SPEED UP!!!","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9x86/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":52,"subreddit":"singapore","type":"comment","id":"l0u9x86","datetime":"2024-04-23 11:42:19"}
	{"user":"Jizzipient","content":"You see officer, I invited these young girls to my home to warn them about the dangers of what could have happened if you meet randos on the internet. It's educational!","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uahpl/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":32,"subreddit":"singapore","type":"comment","id":"l0uahpl","datetime":"2024-04-23 11:46:52"}
	{"user":"Party-Ring445","content":"Cannot lose mentality..","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucpiz/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":12,"subreddit":"singapore","type":"comment","id":"l0ucpiz","datetime":"2024-04-23 12:04:51"}
	{"user":"ihavenoidea90s","content":"This guy sucks at gaslighting as much as he does driving.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ualql/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":28,"subreddit":"singapore","type":"comment","id":"l0ualql","datetime":"2024-04-23 11:47:45"}
	{"user":"Critwice","content":"he think he TP lmao, can't see through his big fat ego.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u8622/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":23,"subreddit":"singapore","type":"comment","id":"l0u8622","datetime":"2024-04-23 11:28:35"}
	{"user":"kyrandia71","content":"Last time in driving school, the instructors teach you to slow down if someone is tailgating you and not speed up. Either slow down or just filter to left lane to let other vehicle overtake. Is like someone stare at you want to fight, instead of looking away or say sorry to de-escalate, one stares back and says, \"kua simi?\"","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uaf6t/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":23,"subreddit":"singapore","type":"comment","id":"l0uaf6t","datetime":"2024-04-23 11:46:17"}
	{"user":"Vedor","content":"Just goes to show how highly he thought of himself.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucnwd/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":13,"subreddit":"singapore","type":"comment","id":"l0ucnwd","datetime":"2024-04-23 12:04:28"}
	{"user":"Pitiful_Blackberry67","content":"Exactly, What was he thinking?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9ej5/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":10,"subreddit":"singapore","type":"comment","id":"l0u9ej5","datetime":"2024-04-23 11:38:16"}
	{"user":"thewind21","content":"Driver thinks he is a vilgante. Upholding the traffic rules.\n\nEarn too much. Head too big","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ueacq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":9,"subreddit":"singapore","type":"comment","id":"l0ueacq","datetime":"2024-04-23 12:18:04"}
	{"user":"splash8388","content":"Correct. Why need to tell him to slow down by chasing him? Just let him (Saab driver) be.  He (saab) is responsible for his own action ultimately. Why get involved as if you can tell the Saab driver to slow down ?  Saab driver may think you are trying to race with him. \nIt's a double edge sword for the merc driver with this action.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uicrs/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0uicrs","datetime":"2024-04-23 12:52:35"}
	{"user":"roksah","content":"I think need IQ test instead of driver test","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uiykm/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uiykm","datetime":"2024-04-23 12:58:11"}
	{"user":"RealityLongjumping13","content":"If you are reading this and is really your thought at that point of time, it's WRONG.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9vf3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0u9vf3","datetime":"2024-04-23 11:41:56"}
	{"user":"TOFU-area","content":"smartest sg driver","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ud4ez/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0ud4ez","datetime":"2024-04-23 12:08:17"}
	{"user":"MilkTeaRamen","content":"Breaking News: Singapore’s driver population halves overnight. Government to temporary scrap COE measures. CAT A cars below 50k.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udilq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":63,"subreddit":"singapore","type":"comment","id":"l0udilq","datetime":"2024-04-23 12:11:31"}
	{"user":"mako-lollipop","content":"First time drivers would have a higher BP when they drive though. Need to make driving license expire after 10 years so have to retake","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uea8d/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":7,"subreddit":"singapore","type":"comment","id":"l0uea8d","datetime":"2024-04-23 12:18:02"}
	{"user":"CisternOfADown","content":"It seems all the driving lessons go out the window once they get licence. Drivers need a 'refresher'. I suggest licences to be renewable every 2 years with a theory test at least. I'm sick of seeing drivers zooming past zebra crossings or not waiting for pedestrians to completely cross at traffic lights.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uljvo/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uljvo","datetime":"2024-04-23 13:23:03"}
	{"user":"Party-Ring445","content":"Brain is still in primary school","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udgx9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":59,"subreddit":"singapore","type":"comment","id":"l0udgx9","datetime":"2024-04-23 12:11:09"}
	{"user":"ilovenoodles06","content":"Guy is probably a rich kid i assume","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ud5o1/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":21,"subreddit":"singapore","type":"comment","id":"l0ud5o1","datetime":"2024-04-23 12:08:35"}
	{"user":"Lostwhispers05","content":"Perhaps because he created a throwaway account and r/sgexams has less strict posting requirements? Just speculating - I don't actually know r/sgexam's posting requirements.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uf3ll/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":17,"subreddit":"singapore","type":"comment","id":"l0uf3ll","datetime":"2024-04-23 12:25:04"}
	{"user":"captwaffles-cat","content":"Student driving daddy's car","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ueq9n/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0ueq9n","datetime":"2024-04-23 12:21:53"}
	{"user":"FalseAgent","content":"because this subreddit is awful so now there are splinter communities","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uejgj/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0uejgj","datetime":"2024-04-23 12:20:18"}
	{"user":"canceler80","content":"Troll account and is fake?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0um5fm/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0um5fm","datetime":"2024-04-23 13:29:03"}
	{"user":"drlqnr","content":"not enough karma to post here i think","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0umfvp/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0umfvp","datetime":"2024-04-23 13:31:59"}
	{"user":"KopiSiewSiewDai","content":"I don’t think the Merc driver is at fault for the accident at the cross junction. He is just a shitty driver who happened to be at the wrong place. \n\nThe bulk of the fault is still on the Saab driver","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9zv4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":82,"subreddit":"singapore","type":"comment","id":"l0u9zv4","datetime":"2024-04-23 11:42:53"}
	{"user":"Carbonaddictxd","content":"Precisely, why are all the comments blaming md just because he's not a nice driver?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9bfs/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":32,"subreddit":"singapore","type":"comment","id":"l0u9bfs","datetime":"2024-04-23 11:37:33"}
	{"user":"whimsicism","content":"This, it really feels like deflection to me when people blame the merc driver. He's a bad driver and annoying af but he didn't actually cause the incident.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ueie3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":15,"subreddit":"singapore","type":"comment","id":"l0ueie3","datetime":"2024-04-23 12:20:03"}
	{"user":"danorcs","content":"Unfortunately I think the Merc driver is partially responsible\n\nYou know exactly what the SB’s defence attorney will say:\n\n“your honour, my client was travelling along the straight road and was changing lanes in the process of overtaking a motorbike when a white merc unexpectedly aggressively accelerated, sideswiping my client’s car at high speed and nearly causing it to slide and mount the kerb \n\nThis was an accident that the merc driver could have prevented at anytime by not accelerating and allowing the SB to pass, despite his claims that he was potentially forced to mount a kerb\n\nMy client was shocked at being side swiped at high speed and in a lapse of judgement he panicked and accelerated away from the dangerous Merc driver\n\nMy client was so agitated by the aggressive Merc driver that he constantly kept checking for said driver on the rear wheel window, and most unfortunately failed to notice that he was reaching a red light, causing the tragic accident for which he is apologetically here today”\n\nIf I provoke someone into a frenzied state and he accidentally kills someone in a fit of panic, I may still be culpable in SG. It would require detailed legal examination at the very least","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ul678/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ul678","datetime":"2024-04-23 13:19:15"}
	{"user":"Alive-Confection3690","content":"Idk i kinda disagree here. md didnt cause the accident but he definitely aggravated the sd driver partially to do it. Majority of the fault is still on the sd of course but if md could swallow that petty pride of being overtaken, this accident might not have happened.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uf1ev/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uf1ev","datetime":"2024-04-23 12:24:31"}
	{"user":"Possible-Grocery6244","content":"i do think that the merc hold partial responsibility for the accident, or at least he was the catalyst for it. Assuming that SAAB was trying to do a hit and run after the sideswipe (not because of brake failure), merc not speeding up wouldnt have made SAAB driver to react the way he did. Now, im not saying that SAAB driver is absolved of any responsiblity, but the merc driver should be held accountable as well.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uea2p/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-9,"subreddit":"singapore","type":"comment","id":"l0uea2p","datetime":"2024-04-23 12:18:00"}
	{"user":"Party-Ring445","content":"Victim should sue him for all he's worth. And add to that criminal charge and canning","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ud91d/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ud91d","datetime":"2024-04-23 12:09:24"}
	{"user":"Geminispace","content":"As I mentioned in other Reddit post, these drivers will never admit their fault but the fault of everyone else. Probably the Saab driver out there blaming the vans and the car in front for turning despite green light","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uiju3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uiju3","datetime":"2024-04-23 12:54:23"}
	{"user":"velvetstigma","content":">I’m betting saab driver was distracted (likely eyes on rear view mirror),  checking merc reaction  before ploughing through the junction. Looking at the force of the Saab during the accident, No one sane will cheong thru a junction full of vehicles. Saab’s eyes were clearly off the road. \n\nThis is most definitely what happened here. He didn't expect the merc to speed up and thus clipping the merc. Once there was a hit, he panicked and only looked at the rear mirror whole hoping to speed through the junction, not realizing there was traffic ahead.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukbp3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukbp3","datetime":"2024-04-23 13:11:02"}
	{"user":"tens919382","content":"nah.  definitely on drugs.  who the fk cuts in like that","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uijq3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uijq3","datetime":"2024-04-23 12:54:21"}
	{"user":"Prov0st","content":"He would get outright nuked if he posted in this thread.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9z20/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":17,"subreddit":"singapore","type":"comment","id":"l0u9z20","datetime":"2024-04-23 11:42:43"}
	{"user":"darrenoloGy","content":"Exactly, single lane hog simi, not like the motorcycle was crawling at 10-20km/h. lame excuse.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0unxm5/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0unxm5","datetime":"2024-04-23 13:47:42"}
	{"user":"rafalim021","content":"100% spot on.\n\nPeople are just jumping on the Merc driver because:\n\n1. Its an opportunity for people to whack the everyday assholes on SG roads that speed up to stop someone from overtaking\n\n2. Its a Merc - and a more powerful engine model at that.\n\n\nGuy is definitely no saint though and his latest posts are abit of a clownfest and indicative of his fragile ego.\n\nI just hope this doesn't divert attention away from the SAAB driver who deserves everything he gets and more.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujjc1/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujjc1","datetime":"2024-04-23 13:03:32"}
	{"user":"Eseru","content":"That's the thing that really puzzles me. It's one thing if the junction was right after the bend and the Saab didn't have time to slow down. Or he was trying to beat the red light and the other cars had just started moving.  \n  \nBut the Saab had a lot of time and road to slow down after overtaking. It also looks like the light had already turned red. He just plowed straight into the traffic which was already moving across the junction without even attempting to brake? What the hell was he thinking?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0un3yx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0un3yx","datetime":"2024-04-23 13:38:58"}
	{"user":"wwabbbitt","content":"He's not the cause but he's the trigger","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0un02l/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0un02l","datetime":"2024-04-23 13:37:49"}
	{"user":"jlonso","content":">would have made no difference to the outcome.\n\nIt's really hard to measure the impact of benz's involvement in this.\n\nDoes SAAB have enough time and distance to stop at the junction? Yes. The reason for not stopping? Hit & Run?\n\nI wouldn't go as to say it made no difference to the outcome. The outcome could be different if that initial contact wasn't made.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0un85x/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0un85x","datetime":"2024-04-23 13:40:12"}
	{"user":"AdeptPlanktonk","content":"It's a trait of egomaniacs, narcissists.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufpgn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":11,"subreddit":"singapore","type":"comment","id":"l0ufpgn","datetime":"2024-04-23 12:30:18"}
	{"user":"tryingmydarnest","content":">address this culture.\n\nHow though? Enforcement will be an issue, unless we allow citizens to go bounty hunting.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukal6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukal6","datetime":"2024-04-23 13:10:44"}
	{"user":"jlonso","content":"Lawyer don't have to do shit. No way to verify account is his.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0umcup/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0umcup","datetime":"2024-04-23 13:31:10"}
	{"user":"NekoIdo","content":"Good point!\n\nu/SNH7Z good luck!","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uiii5/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0uiii5","datetime":"2024-04-23 12:54:03"}
	{"user":"Eshuon","content":"Uni students too, but yeah thats why it was removed","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubo25/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":6,"subreddit":"singapore","type":"comment","id":"l0ubo25","datetime":"2024-04-23 11:56:24"}
	{"user":"PhantomWolf83","content":"Don't forget about the BMWs.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh1yu/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":6,"subreddit":"singapore","type":"comment","id":"l0uh1yu","datetime":"2024-04-23 12:41:12"}
	{"user":"BearbearDarling","content":"> It would have happen with or without the Merc driver. \n\nYeah, we really don't know that. Yes, the saab was speeding to begin with. But to say a speeding driver will surely beat red light and cause a collision down the road is pure speculation. There are speeding vehicles on the road everyday who don't get into accidents. Moreover, the saab displayed some control by maneuvering and switching lanes trying to overtake. He didn't just crash into the rear of the merc. He didn't barrel over the motorbike. That shows some control on his part even if he was driving dangerously. Versus his conduct at the cross junction where he just plows straight through like a bullet. There is some contrast in his behavior.\n\nThe merc provoking the ssab by speeding up to in attempt to block the overtaking and the saab clipping the merc are not insignificant events. It's possible that they had an effect on the saab driver's state of mind. He could have become blinded with rage and tried to floor it to hit and run. Even so, I think that the merc cannot be legally liable for the collisions at the junction. But you can't tell me the merc is completely uninvolved as if he was a mere bystander. And the worst thing is two lives are lost. One cannot help but wonder if the merc had simply let the saab go on its way, perhaps what happened at the junction might have been different.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uny28/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uny28","datetime":"2024-04-23 13:47:50"}
	{"user":"[deleted]","content":"[removed]","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uorqc/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uorqc","datetime":"2024-04-23 13:56:41"}
	{"user":"MyNameIsOnce","content":"no info to dig for the saab driver. the most i personally found about the saab is a sgcarmart ad of the car before it was sold and it might not be the same car (but the colour and rims match + it’s a very rare car model, so it would be too much to be a coincidence)\n\ni wonder if there are any previous dashcam vids of the saab (SLD1505R) around.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucygd/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0ucygd","datetime":"2024-04-23 12:06:55"}
	{"user":"wirexyz","content":"I don’t understand also. SAAB speed and beat red light and killed 2. But somehow Benz is at fault because he never give way to the SAAB that is speeding like crazy.\n\nSo next time I see someone breaking the law I need to give way to them instead of preventing from committing a crime? I am glad there is court with fairness here cause I think public opinion is a bit wrong.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udc6h/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0udc6h","datetime":"2024-04-23 12:10:08"}
	{"user":"musicmonkay","content":"Did he really contribute tho? \n\nShitty driver that I will curse in my heart if I met on the road yes \n\nBut he didn’t force the Saab to speed, or do anything \n\nIf anything he was also sideswiped in the video right? Unless I saw wrongly which in any situation even though he did a dick move by speeding up, would be the merging driver (Saab)’s fault\n\nIt is 100% in the Saab drivers control whether to speed or drive recklessly, as much as a bad driver the merc was, he/she wasn’t even near the Saab when the accident happened","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uc4gl/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":13,"subreddit":"singapore","type":"comment","id":"l0uc4gl","datetime":"2024-04-23 12:00:05"}
	{"user":"Haunting-Owl","content":"methinks too","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhyfc/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uhyfc","datetime":"2024-04-23 12:49:01"}
	{"user":"KopiSiewSiewDai","content":"He cannot post anywhere else I think","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ua1k5/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0ua1k5","datetime":"2024-04-23 11:43:16"}
	{"user":"haaaaaairy1","content":"Probably cause one of the victims was a student so trying to pander to that demographic.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ua9xj/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":7,"subreddit":"singapore","type":"comment","id":"l0ua9xj","datetime":"2024-04-23 11:45:06"}
	{"user":"A_extra","content":"Because one of the victims killed was a jc student","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uadca/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uadca","datetime":"2024-04-23 11:45:52"}
	{"user":"Sacred-Turnip","content":"Because people are also sick and tired of such driving behaviour like the merc driver practice. Apparently, he was also caught on camera on another occasion driving aggressively, likely proving his bad actions are not limited to that incident. In addition, people are also not buying his reply in trying to make himself sound like a saint.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ub0lj/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":9,"subreddit":"singapore","type":"comment","id":"l0ub0lj","datetime":"2024-04-23 11:51:04"}
	{"user":"Alive_Ad7522","content":"Ppl are slamming him not for causing the eventual crash.  It is his driving posture and attitude that is undesirable.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u991l/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":9,"subreddit":"singapore","type":"comment","id":"l0u991l","datetime":"2024-04-23 11:37:00"}
	{"user":"lurkinglurkerwholurk","content":"Wrong place at the wrong time syndrome. That, and apparently there’s plenty of evidence his default driving is “a dick”, AND him also tries to taichi his (tiny) contribution to the Saab driver’s crash, a perceived “running away from responsibility” in an emotionally charged moment = a wild acceptable target appears. \n\nThrowing fuel into the fire, basically. He totally isn’t the cause of that fire, but…","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uffx3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uffx3","datetime":"2024-04-23 12:28:02"}
	{"user":"areunut1","content":"You can blame both at once. They are not mutually exclusive \n\n\nblame Saab driver for causing incident + blame Merc for driving like an idiot","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uiex4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0uiex4","datetime":"2024-04-23 12:53:07"}
	{"user":"Upper-Discount5109","content":"People always want someone to blame, always want to feel heard in the mob.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ub523/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ub523","datetime":"2024-04-23 11:52:06"}
	{"user":"Spiritual-Okra-7836","content":"if this idiot had let him overtake none of this might have happened either. So he is also partly to blame.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uahgt/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-2,"subreddit":"singapore","type":"comment","id":"l0uahgt","datetime":"2024-04-23 11:46:49"}
	{"user":"AutoModerator","content":"Facebook links are not allowed on this subreddit due to doxxing concerns. Please amend your submission to remove the link and write in to modmail for it to be manually approved again. Alternatively, you may wish to resubmit the post without the link.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/singapore) if you have any questions or concerns.*","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ue6a9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ue6a9","datetime":"2024-04-23 12:17:06"}
	{"user":"NekoIdo","content":"Hence the word “alleged” in the title","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u8f6n/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":7,"subreddit":"singapore","type":"comment","id":"l0u8f6n","datetime":"2024-04-23 11:30:35"}
	{"user":"GrammarNaziii","content":"This was my first reaction as well. Literally *anyone* could've created the account and wrote this shit up, but it seems like everyone here is just taking it at face value.\n\nWhat if it was the motorcycle driver who created this spiel to make the guy look even worse? Crazy sia.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufp25/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ufp25","datetime":"2024-04-23 12:30:13"}
	{"user":"ailes_d","content":"They know, they still dont want to let other people cut them","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufv18/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":29,"subreddit":"singapore","type":"comment","id":"l0ufv18","datetime":"2024-04-23 12:31:38"}
	{"user":"hawk_199","content":"Basic logic but not many people have it and not taught in driving school tbh...At least I don't recall.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhwf0/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0uhwf0","datetime":"2024-04-23 12:48:31"}
	{"user":"ivananiki","content":"But... he said he was a safe driver.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ulfse/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ulfse","datetime":"2024-04-23 13:21:54"}
	{"user":"Organic-Custard6243","content":"For some reason it is quite prevalent among Singaporean driver. When I drive overseas I find that they are more willing to give way. \n\nPerhaps, better legislation can help control this behaviour?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0unc75/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0unc75","datetime":"2024-04-23 13:41:22"}
	{"user":"Common-Metal8578","content":"There is no logic. The asshole is trying to rationalise his guilt and hope people agree with him. Even when you watch this guys own camera footage. He maintains the same distance from the motorcycle until the black car comes beside him. Suddenly he catches up with the motorcycle very quickly. Suddenly have an emergency? At that very moment? Anyway This kind of memory will eat him from the inside for the rest of his life and we don't need to help him with it.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhknu/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":12,"subreddit":"singapore","type":"comment","id":"l0uhknu","datetime":"2024-04-23 12:45:40"}
	{"user":"InfiniteDividends","content":"That's a poor reason to not signal, I still do. If the car behind speeds up, sure, I'll just lane change after you pass then. Life is too short to rage over idiots like that.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uj97g/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":36,"subreddit":"singapore","type":"comment","id":"l0uj97g","datetime":"2024-04-23 13:00:55"}
	{"user":"abgnerd","content":"I think this kind of logic is something we need to eradicate. I don't do XXX cos of YYY, when XXX is the correct thing to do.\n\nSignal your intentions. \n\nIf the other person speeds up to block you then they are the ass, you just wait for the next opportunity to switch lanes.\n\n\"I don't signal cos other drivers will speed up and block.\" \n\nThese are the same drivers who will speed up and block people who signal intentions. These are the drivers that never signal and cut into lanes.\n\nDownvote me if you feel slightly targeted by this post, but I think some people on Reddit are guilty of bad road manners and it shows.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ulogn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ulogn","datetime":"2024-04-23 13:24:18"}
	{"user":"splash8388","content":"Not really for me. I signal ahead as a form of respect to pre-alert that I intend to change lane. I will only change lane when no vehicles are very near my vehicle. This also deter motorcyclist who might speed up to squeeze between 2 cars  I do not think I am rude after I force my way into my desired lane once I give ample time. The reason is I already give ample notice my intention to change. U cannot blame me after that just because you don't like to give way. Ha ha.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujg5u/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujg5u","datetime":"2024-04-23 13:02:42"}
	{"user":"MojaMonkey","content":"The only thing the driver is signalling is weakness.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ueg0t/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0ueg0t","datetime":"2024-04-23 12:19:27"}
	{"user":"Fluffy-Nature-2087","content":"If they speed up when I signal, I will use my brains to overtake the car behind them and drive behind that fella who sped up. Then when that fella slip up on the road while infront of me, I would have it on dashcam and then later file a police report against them with it just to ruin their day or at least to let the police have a record of them slipping up.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0up6m2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0up6m2","datetime":"2024-04-23 14:01:13"}
	{"user":"szab999","content":"yeah probably his lawyer told him to take it down.. but the internet never forgets","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufa4j/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":28,"subreddit":"singapore","type":"comment","id":"l0ufa4j","datetime":"2024-04-23 12:26:40"}
	{"user":"Twrd4321","content":"Unless it is possible to verify the person who wrote the post as the person who drove the car, it cannot be administered as evidence. \n\nAnyone can claim to be anyone on Reddit. That’s why usually for ask me anything, mods usually ask for proof. It is not good enough for someone to claim in the post they drove the car.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ulawo/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ulawo","datetime":"2024-04-23 13:20:33"}
	{"user":"pureeyes","content":"Genuinely nice people would give way to others or at least be remorseful over their part in an accident that's killed two people 🤷🏻","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucmtv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":24,"subreddit":"singapore","type":"comment","id":"l0ucmtv","datetime":"2024-04-23 12:04:14"}
	{"user":"HungryEdward","content":"I went to see after reading your comment. Seems he doxxed himself by using his own Facebook account to post his dashcam footage.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh2s2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":15,"subreddit":"singapore","type":"comment","id":"l0uh2s2","datetime":"2024-04-23 12:41:23"}
	{"user":"PhantomWolf83","content":"It's been almost 20 years since I got my licence but I'm sure as hell they didn't teach that kind of defensive driving at BBDC...","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u75n0/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":87,"subreddit":"singapore","type":"comment","id":"l0u75n0","datetime":"2024-04-23 11:20:49"}
	{"user":"kumgongkia","content":"I will declare war on everyone to achieve world peace.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujnpr/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujnpr","datetime":"2024-04-23 13:04:41"}
	{"user":"may0_sandwich","content":"Lol, 90% of drivers behave this way, are you suggesting all of them may not have a drivers license?? I think the real issue here is the shitty attitude that's been normalised in Singapore.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhiu9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0uhiu9","datetime":"2024-04-23 12:45:12"}
	{"user":"erisestarrs","content":"I signal when I want to change lane not to get people to give way to me, but so the car will speed up and I can change into the lane BEHIND them.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ui4r2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":18,"subreddit":"singapore","type":"comment","id":"l0ui4r2","datetime":"2024-04-23 12:50:36"}
	{"user":"Ok-Recommendation925","content":"His gaslighting won't work, especially when two people are dead....\n\nIn fact those two deaths make this clown look and sound like the devil (and an ass-clown-hole)","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ue7co/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":11,"subreddit":"singapore","type":"comment","id":"l0ue7co","datetime":"2024-04-23 12:17:22"}
	{"user":"thewackykid","content":"yeah... to tell someone to slow down u should slow down yourself (NOT speed up).. if he don't listen and speed up then is not your fault... but at least he would have overtaken u safely and that will be the end of it...","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubw58/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":12,"subreddit":"singapore","type":"comment","id":"l0ubw58","datetime":"2024-04-23 11:58:11"}
	{"user":"Noobcakes19","content":"he's Bruised Wean.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uk3us/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uk3us","datetime":"2024-04-23 13:08:54"}
	{"user":"Anphant","content":"Nah, behavioural test is more important.  Stupid people can always be taught. Not so much for self-centred/obnoxious ones","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0unl96/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0unl96","datetime":"2024-04-23 13:44:03"}
	{"user":"Jay-ay","content":"Subscribed","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ug8m9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":17,"subreddit":"singapore","type":"comment","id":"l0ug8m9","datetime":"2024-04-23 12:34:43"}
	{"user":"XLStress","content":"Fuck yeah.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uocg7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uocg7","datetime":"2024-04-23 13:52:06"}
	{"user":"I_Miss_Every_Shot","content":"Maybe need to retake and reassess after every 5-10 years…. Check for bad habits","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufdye/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":6,"subreddit":"singapore","type":"comment","id":"l0ufdye","datetime":"2024-04-23 12:27:34"}
	{"user":"Brief_Worldliness162","content":"I thought he's 42 years old?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufttb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0ufttb","datetime":"2024-04-23 12:31:21"}
	{"user":"Stormagedd0nDarkLord","content":"Bulk? I'd say all of the fault is on the Saab driver for the accident. You can't be held responsible for someone doing a hit and run on you and zooming off. It's not like md floored it to chase after him. I think he reached the lights same time as the motobike.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udzcv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":49,"subreddit":"singapore","type":"comment","id":"l0udzcv","datetime":"2024-04-23 12:15:28"}
	{"user":"ssepaulette","content":"You guys should watch the video again. 5-6s mark frame by frame. Black car was already about to complete the overtaking and white car sped up. \n\nThis is different from asshole driver inching forward to prevent people from cutting which I believe is what you are talking about.\n\nPortion of the black car has already crossed the divider and white car sped up… hitting black car in the center. Any further in the rear and black car could have spun out of control and may have killed the motorcyclist.\nThis is reckless driving.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ue822/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":21,"subreddit":"singapore","type":"comment","id":"l0ue822","datetime":"2024-04-23 12:17:32"}
	{"user":"Ok-Recommendation925","content":"Basically MD is the guy that was on his way to join a riot. But because he was late to the rally point, and the rioting was started by SD happening (with public opinion against the rioters), he simply joined the onlooker crowds and joined them in being \"kaypoh/what-happened?\"\n\nLOL","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uerf9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uerf9","datetime":"2024-04-23 12:22:09"}
	{"user":"Background_Tax_1985","content":"Yeap agreed.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uadu9/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-1,"subreddit":"singapore","type":"comment","id":"l0uadu9","datetime":"2024-04-23 11:45:59"}
	{"user":"Background_Tax_1985","content":"Probably easier to blame him since he identified himself plus he was acting like a dick and i guess merc? \n\nI think once they announce who sd is everyone will be having a field day with him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9zkz/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":32,"subreddit":"singapore","type":"comment","id":"l0u9zkz","datetime":"2024-04-23 11:42:50"}
	{"user":"soulless33","content":"yeah same thing I told my friends.. the merc drive is an ass but he did not cause the accident..\n\nit's like everyone trying to explain why the driver plough thru the traffic and the merc driver is a scapegoat..\n\nend of the day its a terrible tragedy let us not assign blame, instead keep our prayers to those still in hospital..","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uaidk/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":18,"subreddit":"singapore","type":"comment","id":"l0uaidk","datetime":"2024-04-23 11:47:01"}
	{"user":"Background_Tax_1985","content":"Bro if im the dpp ah:\n\n1) your client is already speeding before he tried to overtake the white merc.\n\n2) why does your client need to overtake the 2 other vehicles at such high speed? That is not normal behaviour.\n\n3) a normal individual would have immediately stopped after the accident to check for damage, exchange info etc, not continue speeding\n\n4) it near impossible for a person to be that provoked in such a short span of time (say from bruce banner to hulk in 3s~5s)","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0up1d3/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0up1d3","datetime":"2024-04-23 13:59:38"}
	{"user":"Background_Tax_1985","content":"Nah. Sd was already speeding prior to the sideswipe and driving too aggresively, so it can't really be said that md aggravated him.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufbjx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":8,"subreddit":"singapore","type":"comment","id":"l0ufbjx","datetime":"2024-04-23 12:27:00"}
	{"user":"Background_Tax_1985","content":"I disagree. Sd was already speeding before the side swipe and carried on speeding after. Md's action is unrelated to the accident, even if is at fault for his behaviour.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uf45d/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uf45d","datetime":"2024-04-23 12:25:12"}
	{"user":"Vedor","content":"Such people confirm will just delete his account if he ever get nukes.\n\n\nIn the very first place, do you think he will post such thread if his licence plate wasn't made known to everyone?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ucuaq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0ucuaq","datetime":"2024-04-23 12:05:58"}
	{"user":"KneeGal","content":"I disagree. He is an asshole driver for sure but he is not the trigger because the SAAB was already speeding before the Merc noticed him (and started speeding up). Unless he has pre-cog, there is no reason for him to be going so fast in the first place.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0up2b7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0up2b7","datetime":"2024-04-23 13:59:55"}
	{"user":"KneeGal","content":"He was already speeding before he came into contact with the Merc. \n\nHe was still speeding after he came into contact.\n\nWhy do you think the outcome would have been different? It would be one thing if he was not speeding before and then sped after or vice versa. But he was constantly speeding.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uokul/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uokul","datetime":"2024-04-23 13:54:37"}
	{"user":"dibidi","content":"see how UK updated the highway code to include the hierarchy of road users requiring all road users to defer to the most vulnerable road users at all times. \n\nwith that rule in place any accident that happens the responsibility is automatically on the road user in the larger vehicle. insurance liability means de facto compliance. \n\nnow most everyone in the UK gives way.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uklbw/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uklbw","datetime":"2024-04-23 13:13:36"}
	{"user":"AutoModerator","content":"Facebook links are not allowed on this subreddit due to doxxing concerns. Please amend your submission to remove the link and write in to modmail for it to be manually approved again. Alternatively, you may wish to resubmit the post without the link.\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/singapore) if you have any questions or concerns.*","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uorr4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uorr4","datetime":"2024-04-23 13:56:41"}
	{"user":"Possible-Grocery6244","content":"because there is reasoning behind the speeding and the merc driver caused it. No one is saying SAAB is not at fault, but the merc driver definitely should share some responsibility because he was the one who sped up and induced the situation for the SAAB to sideswipe the merc, and in turn the SAAB driver panicked and wanted to hit and run hence speeding up after the sideswipe (assuming there is no brake failure). If the merc driver did not speed up, the subsequent chain of events wouldnt have occured","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uf975/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uf975","datetime":"2024-04-23 12:26:27"}
	{"user":"RiskDry6267","content":"Benz is being scolded for being an ego idiot.\nIf give way 100% no accident. Even though SAAB is probably 90% at fault.\nThe news story could have easily been the SAAB buah the Mercedes then crash him and both cars burning on the side of the road, and merc driver died, then what will we all say online? Asking for it right ? \nGive way from the start none of this shit will happen. Motorcycle bro in front also thank his lucky stars he wasn’t collateral damage","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uieem/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0uieem","datetime":"2024-04-23 12:52:59"}
	{"user":"smurflings","content":"Seeing, just in the replies to my post at least one blaming him for causing this eventually crash.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uc2zo/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uc2zo","datetime":"2024-04-23 11:59:45"}
	{"user":"Upper-Discount5109","content":"Multiple choices for saab driver after not being allowed to overtake. He could've waited, could've slowed down and overtake later, but he chose to chiong which eventually caused the accident. White merc = inconsiderate at most, didn't cause the accident. Saab driver = His decision, his consequences","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubdyp/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ubdyp","datetime":"2024-04-23 11:54:06"}
	{"user":"smurflings","content":"And if you had walked a different direction that day, none of this might have happened either! So you're also partly to blame.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubxw8/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-1,"subreddit":"singapore","type":"comment","id":"l0ubxw8","datetime":"2024-04-23 11:58:35"}
	{"user":"crispyprata","content":"Ok?  if Temasek JC didn’t organize an event at bedok reservoir the student would still be alive so TJC is also partly to blame /s","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uivg6/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0uivg6","datetime":"2024-04-23 12:57:22"}
	{"user":"meowinbox","content":"Bro, you and I both know that word doesn't make that much of a difference. \n\n\nI'm sure *you* are aware that he could just be trolling, I'm not singling *you* out in particular. My comment was also directed at the others already dragging him as though he's really going to be here reading these comments.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0u9i1q/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-21,"subreddit":"singapore","type":"comment","id":"l0u9i1q","datetime":"2024-04-23 11:39:00"}
	{"user":"alexand3rl","content":"It's taught in driving school. BTT literally has the question where if someone is trying to overtake you, you should slow down and assist the motorist to help overtake you","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0up6zn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0up6zn","datetime":"2024-04-23 14:01:19"}
	{"user":"zchew","content":"Yes, he teach others how to drive safely by speeding up to block them. Very safety. Huehuehuehue","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0unl7s/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0unl7s","datetime":"2024-04-23 13:44:03"}
	{"user":"theony","content":"> The asshole is trying to rationalise his guilt \n\nHe feels no guilt, he thinks he's a tragic hero who failed to achieve his goals even though his actions were heroic. That's why he posted his own video up, and made multiple reddit posts justifying his actions. \n\nThis kind will never, ever learn. The only thing he will take away from being slammed on the internet will be \"internet is stupid\". If the police catch him and charge him, the only thing he will take away is \"police are lazy and will only wayang in high profile cases to show off\".","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukchn/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukchn","datetime":"2024-04-23 13:11:15"}
	{"user":"taeng89","content":"Upvoted. Completely agree with this","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uo965/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uo965","datetime":"2024-04-23 13:51:08"}
	{"user":"Critwice","content":"he wanted to post here but not enough karma/age? r/sgexams mods deleted because irrelevant.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufsvh/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":22,"subreddit":"singapore","type":"comment","id":"l0ufsvh","datetime":"2024-04-23 12:31:07"}
	{"user":"timlim029","content":"Yeah. I'm surprised nobody is even questioning if the driver really wrote the post. I mean, it was posted on /r/SGExams. Could easily be a bored student.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uoyv5/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uoyv5","datetime":"2024-04-23 13:58:52"}
	{"user":"MilkTeaRamen","content":"Bro this is call offensive driving.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ud8xg/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":61,"subreddit":"singapore","type":"comment","id":"l0ud8xg","datetime":"2024-04-23 12:09:22"}
	{"user":"Prov0st","content":"Bro, that’s a genius cheat code.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uit3i/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":5,"subreddit":"singapore","type":"comment","id":"l0uit3i","datetime":"2024-04-23 12:56:45"}
	{"user":"Jackker","content":"Yeah, this is the way. If they slow down and an opportunity is there to change lane, do it.\n\nIf they speed up, wait for as long as necessary until the opportunity is there. Then change lane,  it really is that simple. 😊","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0unsy8/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0unsy8","datetime":"2024-04-23 13:46:19"}
	{"user":"foxbat2525","content":"Never too old to be a student driving daddy's car","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujyi4/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ujyi4","datetime":"2024-04-23 13:07:29"}
	{"user":"_WonderStruck_17","content":"Agreed, the Saab driver being a complete siaokia had no bearing on the Merc driver. Merc driver was a bully too but it was two separate incidents.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufwmx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":33,"subreddit":"singapore","type":"comment","id":"l0ufwmx","datetime":"2024-04-23 12:32:01"}
	{"user":"ailes_d","content":"Yeah the fault is totally on the saab driver because we all know hes speeding from very far away already. But in this case, the video not only provided evidence of saab driving recklessly, but also showcased the white merc cb mentality which lots of sg drivers have","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ugdjz/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":9,"subreddit":"singapore","type":"comment","id":"l0ugdjz","datetime":"2024-04-23 12:35:44"}
	{"user":"ABootSG","content":"Tbh looking at these two incidents separately, I'd say the merce is reckless, saab on drugs.\n\nBut the saab driver was clearly on some shit already because he didn't stop despite colliding with the merce. As for the merce not stopping after their collision it's probably because he saw the saab trying to hit and run so he kept up with him.\n\nAlso at the junction from the perpendicular POV of another car, it didnt look like the saab was braking at all when he was on the junction.\n\nI think we should wait for the toxicology report on the saab driver. Probably on some serious amps.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uf5wf/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":18,"subreddit":"singapore","type":"comment","id":"l0uf5wf","datetime":"2024-04-23 12:25:38"}
	{"user":"deathzz123","content":"TIL overtaking car can just chiong into lane no need check mirror or blind spot, if other driver dont yield they are reckless","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukqtq/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukqtq","datetime":"2024-04-23 13:15:03"}
	{"user":"infernoKings","content":"Exactly. Reckless driving by the SAAB driver.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0umdro/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0umdro","datetime":"2024-04-23 13:31:25"}
	{"user":"Possible-Grocery6244","content":"yes he was already speeding but precisely because of the sideswipe SAAB driver reacted the way he did. Or are you assuming that the incident would have occur either way without the sideswipe incident? im inclined to believe that this is a cause-and-effect situation","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ufwxr/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":0,"subreddit":"singapore","type":"comment","id":"l0ufwxr","datetime":"2024-04-23 12:32:06"}
	{"user":"tryingmydarnest","content":"TIL. Thanks for sharing.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukq89/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukq89","datetime":"2024-04-23 13:14:53"}
	{"user":"Emotional_Apricot591","content":"He literally made a bad situation worse, so yes he is to blame as well","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0udula/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":2,"subreddit":"singapore","type":"comment","id":"l0udula","datetime":"2024-04-23 12:14:19"}
	{"user":"Vedor","content":"Please tell that to journalists when they use the word \"alleged\" in their article.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uf43e/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0uf43e","datetime":"2024-04-23 12:25:11"}
	{"user":"NekoIdo","content":"Buddy, all these stuff is already out there, people are going to find it with or without me. I’m not making new shit up, I’m just reiterating what’s been said already. \n\nYou wanna defend this guy so bad knock yourself out man. But what’s it got to do with you being unable to take the word “alleged” at face value?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ua2tv/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":4,"subreddit":"singapore","type":"comment","id":"l0ua2tv","datetime":"2024-04-23 11:43:32"}
	{"user":"Jjzeng","content":"I was certainly offended by their driving","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uh6g8/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":15,"subreddit":"singapore","type":"comment","id":"l0uh6g8","datetime":"2024-04-23 12:42:14"}
	{"user":"hehetypo","content":"no this one is sinkie pwn sinkie driving end up pwn ownself so can sleep soundly at night","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ul3w7/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ul3w7","datetime":"2024-04-23 13:18:37"}
	{"user":"Stormagedd0nDarkLord","content":"Exactly. I really dunno why md is getting this level of hate and blame for this. It's illogical.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ug8vb/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":10,"subreddit":"singapore","type":"comment","id":"l0ug8vb","datetime":"2024-04-23 12:34:46"}
	{"user":"Additional-Ad-1644","content":"Life doesn’t have to be black and white, you know..\n\nWait till you are trying to overtake a lane hogger who somehow decides to speed up when you are passing him. Doesn’t matter if he’s right or wrong or legal or illegal. Still an asshole driver.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uoyq2/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uoyq2","datetime":"2024-04-23 13:58:49"}
	{"user":"Background_Tax_1985","content":"I believe it would have still occured without the sideswipe incident. \n\nThe sideswipe incident took place pretty far away from the junction and he had the option of also making a left turn (zebra crossing), a right turn (traffic light) to get away from the merc if that was the intention. Infact, i would even warrant to say that going left would have been much better route to \"escape\".\n\nClearly sb was not really bothered by the sideswipe, if at all.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uhd3z/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":3,"subreddit":"singapore","type":"comment","id":"l0uhd3z","datetime":"2024-04-23 12:43:49"}
	{"user":"kumgongkia","content":"Are u the Saab driver? How would u know how that guy thinks? After kena sideswiped the knee jerk reaction is to accelerate all the way? Huh?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ukfhh/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ukfhh","datetime":"2024-04-23 13:12:03"}
	{"user":"meowinbox","content":"I'm not defending the *driver*. Which part of what I said defended him? \n\n\nWhat I meant was, that post could have been written by some troll at home, and these netizens are wasting their energy giving this redditor all the rage and attention he wants, while the actual driver is somewhere else in his own world.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ubf3i/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-10,"subreddit":"singapore","type":"comment","id":"l0ubf3i","datetime":"2024-04-23 11:54:22"}
	{"user":"_WonderStruck_17","content":"To be fair that instance does exemplify the typical kiasi behaviour that most of our SG drivers have, not wanting to give way to others which infuriates the motoring community a lot. That clip of the Merc driver committing a full corner like as though he's racing for their F1 team to overtake a motorbike on a expressway exit is one example.\n\nBut of course this tragic incident shouldn't be attributed to him. Hopefully this is a hard lesson for all.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uinzd/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":13,"subreddit":"singapore","type":"comment","id":"l0uinzd","datetime":"2024-04-23 12:55:25"}
	{"user":"deathzz123","content":"I agree with you no black and white and yes he is an asshole driver but if I overtake someone who speeds back up I will go back to my lane. At most miss exit go 1 round lo","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0up7dj/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0up7dj","datetime":"2024-04-23 14:01:27"}
	{"user":"Possible-Grocery6244","content":"Him ramming through the traffic junction clearly shows that he is incapable of grasping the situation and brake accordingly when its a red light. But he took maneuvers to try to cut the behind cam car + merc driver and evade the motorcycle right before the sideswipe occurs. Isnt this just more evidence that his state of mind was different before and after the sideswipe, and that the sideswipe is the catalyst for this particular instance. My argument is he would have just rammed into the motorcycle or into the back of the merc without trying to cut both of them if his state of mind was the same before and after the sideswipe","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ujbfx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":-2,"subreddit":"singapore","type":"comment","id":"l0ujbfx","datetime":"2024-04-23 13:01:29"}
	{"user":"Possible-Grocery6244","content":"i did not say the knee jeck reaction is to accelerate all the way. Im merely stating what is observed from the video that the SAAB driver indeed accelerated all the way after the sideswipe and assumption is he was under the influenced and the sideswipe triggered a hit and run scenario","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0ulrqz/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0ulrqz","datetime":"2024-04-23 13:25:14"}
	{"user":"NekoIdo","content":"You’re absolutely right, it could be him, could not be him.\n\nIt’s a public forum though so good luck gate keeping all the content here. Have fun!","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uc1bx/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uc1bx","datetime":"2024-04-23 11:59:22"}
	{"user":"TimidHuman","content":"Justify so much but you also feeding the troll (if it was truly one). Same same la","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uck2s/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uck2s","datetime":"2024-04-23 12:03:36"}
	{"user":"happycanliao","content":"You can tell his state of mind just from that? Are you a psychiatrist?","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uk42j/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uk42j","datetime":"2024-04-23 13:08:58"}
	{"user":"Background_Tax_1985","content":"I can't really buy into your arguement. \n\nMy hypotesis is that if he was not intoxicated by drugs or alcohol, then he was trying to speed to somewhere (god knows where). \n\nThis is seen from the good control over the vehicle, even up to the accident. The vehicle barely wobbled, swerve around (except when overtaking, even after overtaking he was going as straight as an arrow) etc. Only issue is that sb was going way too fast. \n\nAnd in essence, there is no difference in his state of mind before or after. At least not so much that it would have caused the accident.","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uo6te/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uo6te","datetime":"2024-04-23 13:50:25"}
	{"user":"Possible-Grocery6244","content":"isnt it clear the difference between the state of mind of the driver if 1. you are trying to avoid the vehicles infront of you by maneuvering the car and 2. you just straight plow through a red light junction? I dont think you need to be a psych to reach this conclusion","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0uknwf/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0uknwf","datetime":"2024-04-23 13:14:16"}
	{"user":"happycanliao","content":"I don't think it's a state of mind issue? There could be other issues such as being distracted or the car may have been damaged by the accident causing loss of control (not likely but who knows).","url":"https://www.reddit.com/r/singapore/comments/1cau62k/screenshot_of_snh7z_drivers_alleged_self/l0um9zs/","context":"Screenshot of SNH7Z Driver's alleged \"Self Justification\"","score":1,"subreddit":"singapore","type":"comment","id":"l0um9zs","datetime":"2024-04-23 13:30:20"}
	24/04/24 11:02:02 INFO CodeGenerator: Code generated in 10.956579 ms
	24/04/24 11:02:02 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:02:02 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:02:02 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:02:02 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:02:02 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:02:02 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:02:02 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:02:02 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:02:02 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 6b0330d1bc18:46775 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:02:02 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:02:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:02:02 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/24 11:02:02 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 142983 bytes) 
	24/04/24 11:02:02 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/04/24 11:02:02 INFO CodeGenerator: Code generated in 44.306029 ms
	24/04/24 11:02:02 INFO CodeGenerator: Code generated in 23.329189 ms
	24/04/24 11:02:03 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/04/24 11:02:03 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/24 11:02:03 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/24 11:02:03 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/24 11:02:03 INFO AppInfoParser: Kafka startTimeMs: 1713927723099
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:04.137 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:02:03 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 6b0330d1bc18:46775 in memory (size: 12.3 KiB, free: 366.2 MiB)
	24/04/24 11:02:03 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 6b0330d1bc18:46775 in memory (size: 25.3 KiB, free: 366.3 MiB)
	24/04/24 11:02:04 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 1 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/24 11:02:04 INFO Metadata: [Producer clientId=producer-1] Cluster ID: L4eea4e_RqemEIkj6Jy59w
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:05.143 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:02:04 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 4 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/24 11:02:04 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
	24/04/24 11:02:04 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 5 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/24 11:02:04 WARN NetworkClient: [Producer clientId=producer-1] Error while fetching metadata with correlation id 6 : {reddit_preprocessed=LEADER_NOT_AVAILABLE}
	24/04/24 11:02:04 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to DgmOLtAzSgqCCNpqYUj5ng
	24/04/24 11:02:04 INFO PythonRunner: Times: total = 72, boot = -1440, init = 1511, finish = 1
	24/04/24 11:02:04 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1753 bytes result sent to driver
	24/04/24 11:02:04 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1929 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:02:04 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.945 s
	24/04/24 11:02:04 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:02:04 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/24 11:02:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/24 11:02:04 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 1.959797 s
	24/04/24 11:02:04 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/24 11:02:04 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/24 11:02:04 INFO SparkUI: Stopped Spark web UI at http://6b0330d1bc18:4040
	24/04/24 11:02:04 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/24 11:02:04 INFO MemoryStore: MemoryStore cleared
	24/04/24 11:02:04 INFO BlockManager: BlockManager stopped
	24/04/24 11:02:05 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/24 11:02:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/24 11:02:05 INFO SparkContext: Successfully stopped SparkContext
	24/04/24 11:02:05 INFO ShutdownHookManager: Shutdown hook called
	24/04/24 11:02:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4
	24/04/24 11:02:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-b6019d04-8cdf-450c-a51c-9c366492fdd4/pyspark-84e511f7-f940-4407-a216-92bff5618715
	24/04/24 11:02:05 INFO ShutdownHookManager: Deleting directory /tmp/spark-fbd5f110-b27a-4bb1-85d9-9e458eb9551a
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.146 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450, processId:1599 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.147 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.148 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.148 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.149 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.165 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.166 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.166 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.168 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/579/1450
[WI-579][TI-1450] - [INFO] 2024-04-24 11:02:06.169 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1450] - [INFO] 2024-04-24 11:02:07.051 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1450, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:21.411 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7147147147147147 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:02:24.450 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7088235294117646 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:24.325 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7168141592920354 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:28.348 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1451, taskName=search for intake JSON files, firstSubmitTime=1713928528300, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=580, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1451'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111528'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='580'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.349 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.350 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.351 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.351 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.351 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.351 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928528351
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.351 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 580_1451
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.351 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1451,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713928528300,
  "startTime" : 1713928528351,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1451.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 580,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1451"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111528"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "580"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "580_1451",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.352 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.355 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.386 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451 check successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.396 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.408 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.409 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.409 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.409 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.409 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.409 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451/580_1451.sh
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:28.447 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 1998
[WI-0][TI-1451] - [INFO] 2024-04-24 11:15:29.136 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1451, success=true)
[WI-0][TI-1451] - [INFO] 2024-04-24 11:15:29.142 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1451)
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:29.451 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.455 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451, processId:1998 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.456 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.456 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.463 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.466 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451
[WI-580][TI-1451] - [INFO] 2024-04-24 11:15:29.481 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1451] - [INFO] 2024-04-24 11:15:29.694 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1451, processInstanceId=580, status=7, startTime=1713928528351, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1451.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451, endTime=1713928529456, processId=1998, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1451] - [INFO] 2024-04-24 11:15:29.710 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1451, processInstanceId=580, status=7, startTime=1713928528351, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1451.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1451, endTime=1713928529456, processId=1998, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], eventCreateTime=0, eventSendTime=1713928529693)
[WI-0][TI-1451] - [INFO] 2024-04-24 11:15:30.146 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1451, success=true)
[WI-0][TI-1451] - [INFO] 2024-04-24 11:15:30.157 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1451, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:31.285 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1453, taskName=move to processing, firstSubmitTime=1713928531275, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=580, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1453'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111531'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='580'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.286 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.287 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.289 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928531289
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 580_1453
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.289 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1453,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713928531275,
  "startTime" : 1713928531289,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1453.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 580,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1453"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111531"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "580"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "580_1453",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.290 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.315 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453 check successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.316 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.317 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.318 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.318 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.319 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.319 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.320 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.320 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.320 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.327 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.327 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.336 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.336 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.336 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453/580_1453.sh
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:31.353 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2012
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:31.714 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1453, processInstanceId=580, startTime=1713928531289, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1453.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:31.722 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1453, processInstanceId=580, startTime=1713928531289, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1453.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1713928531714)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:31.722 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1453, processInstanceId=580, startTime=1713928531289, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:31.724 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=1453, processInstanceId=580, startTime=1713928531289, workflowInstanceHost=172.18.0.9:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1713928531714)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:32.132 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1453, success=true)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:32.142 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1453)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:32.150 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1453, success=true)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:32.157 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1453)
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:32.355 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:32.364 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7897727272727273 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.387 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453, processId:2012 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.387 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453
[WI-580][TI-1453] - [INFO] 2024-04-24 11:15:32.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:32.726 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1453, processInstanceId=580, status=7, startTime=1713928531289, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1453.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453, endTime=1713928532388, processId=2012, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:32.739 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1453, processInstanceId=580, status=7, startTime=1713928531289, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1453.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1453, endTime=1713928532388, processId=2012, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], eventCreateTime=0, eventSendTime=1713928532726)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:33.131 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1453, success=true)
[WI-0][TI-1453] - [INFO] 2024-04-24 11:15:33.135 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1453, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:33.197 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1454, taskName=spark preprocessing, firstSubmitTime=1713928533191, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=580, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null},{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1454'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111533'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='580'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.200 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.200 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.202 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.204 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.204 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.204 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928533204
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.204 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 580_1454
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.207 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1454,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713928533191,
  "startTime" : 1713928533204,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1454.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 580,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null},{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1454"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111533"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "580"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "580_1454",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.208 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.210 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.211 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.214 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.215 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.217 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1454 check successfully
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.217 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-580][TI-1454] - [ERROR] 2024-04-24 11:15:33.226 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[181] - Task execute failed, due to meet an exception
org.apache.dolphinscheduler.plugin.task.api.TaskException: Download resource file: file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar error
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.downloadResourcesIfNeeded(TaskExecutionContextUtils.java:149)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:229)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: File file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:668)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:658)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:458)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:483)
	at org.apache.dolphinscheduler.plugin.storage.hdfs.HdfsStorageOperator.copyHdfsToLocal(HdfsStorageOperator.java:371)
	at org.apache.dolphinscheduler.plugin.storage.hdfs.HdfsStorageOperator.download(HdfsStorageOperator.java:291)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.downloadResourcesIfNeeded(TaskExecutionContextUtils.java:137)
	... 5 common frames omitted
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.227 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[118] - Cancel the task successfully
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[125] - Get a exception when execute the task, will send the task status: FAILURE to master: 172.18.1.1:1234
[WI-580][TI-1454] - [INFO] 2024-04-24 11:15:33.231 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:33.409 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8284960422163589 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:33.740 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1454, processInstanceId=580, startTime=1713928533204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1454.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:33.743 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=1454, processInstanceId=580, startTime=1713928533204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1454.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1713928533740)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:33.743 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1454, processInstanceId=580, status=6, startTime=1713928533204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1454.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1454, endTime=1713928533227, processId=0, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], eventCreateTime=0, eventSendTime=0)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:33.745 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionFinishEvent(taskInstanceId=1454, processInstanceId=580, status=6, startTime=1713928533204, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.9:5678, logPath=/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1454.log, executePath=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1454, endTime=1713928533227, processId=0, appIds=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], eventCreateTime=0, eventSendTime=1713928533740)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:34.133 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1454, success=true)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:34.140 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1454, success=true)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:34.143 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1454, success=true)
[WI-0][TI-1454] - [INFO] 2024-04-24 11:15:34.147 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1454, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:34.410 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7368421052631579 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:37.424 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7653958944281525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:53.496 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7256637168141594 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:15:54.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7865168539325842 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:05.531 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1456, taskName=search for intake JSON files, firstSubmitTime=1713928565525, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=580, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1456'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111605'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='580'}, raw_file_dir=Property{prop='raw_file_dir', direct=OUT, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.532 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.533 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.536 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.536 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928565537
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 580_1456
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1456,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713928565525,
  "startTime" : 1713928565537,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1456.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 580,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1456"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111605"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "580"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "OUT",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "580_1456",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.537 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.538 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.538 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.542 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.542 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1456 check successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.543 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.544 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.544 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.544 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.563 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.563 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.563 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}"
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.564 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.564 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1456/580_1456.sh
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:05.575 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2031
[WI-0][TI-1456] - [INFO] 2024-04-24 11:16:06.236 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1456, success=true)
[WI-0][TI-1456] - [INFO] 2024-04-24 11:16:06.245 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1456)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:06.582 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.596 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1456, processId:2031 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.597 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.598 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.599 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.600 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.604 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.605 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.605 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1456
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.605 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1456
[WI-580][TI-1456] - [INFO] 2024-04-24 11:16:06.606 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1456] - [INFO] 2024-04-24 11:16:07.260 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1456, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:08.281 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1458, taskName=move to processing, firstSubmitTime=1713928568272, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=580, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1458'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111608'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='580'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.282 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.282 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.285 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.285 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928568285
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 580_1458
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1458,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713928568272,
  "startTime" : 1713928568285,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1458.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 580,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1458"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111608"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "580"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "580_1458",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.286 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.296 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.297 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1458 check successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.298 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.299 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.299 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.299 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.301 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.301 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.302 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.302 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.302 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1458/580_1458.sh
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:08.306 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2044
[WI-0][TI-1458] - [INFO] 2024-04-24 11:16:09.252 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1458, success=true)
[WI-0][TI-1458] - [INFO] 2024-04-24 11:16:09.266 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1458)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:09.308 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.337 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1458, processId:2044 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.339 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.340 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.340 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.341 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.345 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.345 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.346 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1458
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.347 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1458
[WI-580][TI-1458] - [INFO] 2024-04-24 11:16:09.347 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1458] - [INFO] 2024-04-24 11:16:10.239 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1458, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:10.322 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1459, taskName=spark preprocessing, firstSubmitTime=1713928570315, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=580, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null},{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1459'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111610'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='580'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.324 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.324 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.326 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.326 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928570326
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 580_1459
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.327 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1459,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713928570315,
  "startTime" : 1713928570326,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/580/1459.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 580,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null},{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1459"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111610"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "580"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "580_1459",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.328 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.328 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.329 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/580/1459 check successfully
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.333 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-580][TI-1459] - [ERROR] 2024-04-24 11:16:10.334 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[181] - Task execute failed, due to meet an exception
org.apache.dolphinscheduler.plugin.task.api.TaskException: Download resource file: file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar error
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.downloadResourcesIfNeeded(TaskExecutionContextUtils.java:149)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:229)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: File file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:668)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:658)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:458)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:483)
	at org.apache.dolphinscheduler.plugin.storage.hdfs.HdfsStorageOperator.copyHdfsToLocal(HdfsStorageOperator.java:371)
	at org.apache.dolphinscheduler.plugin.storage.hdfs.HdfsStorageOperator.download(HdfsStorageOperator.java:291)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.downloadResourcesIfNeeded(TaskExecutionContextUtils.java:137)
	... 5 common frames omitted
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.334 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[118] - Cancel the task successfully
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[125] - Get a exception when execute the task, will send the task status: FAILURE to master: 172.18.1.1:1234
[WI-580][TI-1459] - [INFO] 2024-04-24 11:16:10.338 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1459] - [INFO] 2024-04-24 11:16:11.247 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1459, success=true)
[WI-0][TI-1459] - [INFO] 2024-04-24 11:16:11.258 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1459, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:13.664 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1460, taskName=search for intake JSON files, firstSubmitTime=1713928573640, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=581, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1460'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111613'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='581'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.666 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.668 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.669 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.675 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928573675
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 581_1460
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.675 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1460,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713928573640,
  "startTime" : 1713928573675,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/581/1460.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 581,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1460"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111613"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "581"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "581_1460",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.690 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.692 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.703 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.703 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1460 check successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.704 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.705 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.705 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.705 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.705 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.705 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.705 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.706 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.706 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.706 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.707 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.707 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.707 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1460/581_1460.sh
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:13.717 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2057
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:13.718 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1460] - [INFO] 2024-04-24 11:16:14.269 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1460, success=true)
[WI-0][TI-1460] - [INFO] 2024-04-24 11:16:14.275 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1460)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:14.721 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.721 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1460, processId:2057 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.722 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.722 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.730 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1460
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1460
[WI-581][TI-1460] - [INFO] 2024-04-24 11:16:14.731 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1460] - [INFO] 2024-04-24 11:16:15.274 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1460, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:16.397 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1462, taskName=move to processing, firstSubmitTime=1713928576383, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=581, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1462'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111616'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='581'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.402 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.419 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928576420
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 581_1462
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1462,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713928576383,
  "startTime" : 1713928576420,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/581/1462.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 581,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1462"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111616"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "581"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "581_1462",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.420 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.423 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.424 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.425 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1462 check successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.426 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.427 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.427 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.427 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.427 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.427 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.427 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.428 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1462/581_1462.sh
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:16.432 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2070
[WI-0][TI-1462] - [INFO] 2024-04-24 11:16:17.293 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1462, success=true)
[WI-0][TI-1462] - [INFO] 2024-04-24 11:16:17.302 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1462)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:17.440 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.442 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1462, processId:2070 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.443 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.443 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.443 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.443 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.446 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1462
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1462
[WI-581][TI-1462] - [INFO] 2024-04-24 11:16:17.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1462] - [INFO] 2024-04-24 11:16:18.271 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1462, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:18.350 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1463, taskName=spark preprocessing, firstSubmitTime=1713928578339, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=111, appIds=null, processInstanceId=581, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null},{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1463'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111618'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='581'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.352 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.352 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928578356
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 581_1463
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.356 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1463,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713928578339,
  "startTime" : 1713928578356,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/111/581/1463.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 111,
  "processInstanceId" : 581,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null},{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1463"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111618"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "581"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "581_1463",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.357 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.363 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_111/581/1463 check successfully
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.364 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-581][TI-1463] - [ERROR] 2024-04-24 11:16:18.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[181] - Task execute failed, due to meet an exception
org.apache.dolphinscheduler.plugin.task.api.TaskException: Download resource file: file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar error
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.downloadResourcesIfNeeded(TaskExecutionContextUtils.java:149)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.beforeExecute(WorkerTaskExecutor.java:229)
	at org.apache.dolphinscheduler.server.worker.runner.WorkerTaskExecutor.run(WorkerTaskExecutor.java:167)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
Caused by: java.io.FileNotFoundException: File file:/dolphinscheduler/default/resources/spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:668)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:989)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:658)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:458)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:483)
	at org.apache.dolphinscheduler.plugin.storage.hdfs.HdfsStorageOperator.copyHdfsToLocal(HdfsStorageOperator.java:371)
	at org.apache.dolphinscheduler.plugin.storage.hdfs.HdfsStorageOperator.download(HdfsStorageOperator.java:291)
	at org.apache.dolphinscheduler.server.worker.utils.TaskExecutionContextUtils.downloadResourcesIfNeeded(TaskExecutionContextUtils.java:137)
	... 5 common frames omitted
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[118] - Cancel the task successfully
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.372 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[125] - Get a exception when execute the task, will send the task status: FAILURE to master: 172.18.1.1:1234
[WI-581][TI-1463] - [INFO] 2024-04-24 11:16:18.373 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1463] - [INFO] 2024-04-24 11:16:19.280 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1463, success=true)
[WI-0][TI-1463] - [INFO] 2024-04-24 11:16:19.287 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1463, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:53.818 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.739766081871345 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:16:54.869 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.771505376344086 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:17:47.109 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7520891364902507 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:17:48.127 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.788135593220339 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:13.256 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7181008902077152 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:16.230 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1464, taskName=search for intake JSON files, firstSubmitTime=1713928696220, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=112, appIds=null, processInstanceId=582, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1464'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111816'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='582'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.240 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.244 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928696245
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 582_1464
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1464,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713928696220,
  "startTime" : 1713928696245,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/112/582/1464.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 112,
  "processInstanceId" : 582,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1464"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111816"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "582"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "582_1464",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.240 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1464 check successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.253 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.254 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.255 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.255 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.256 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.256 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1464/582_1464.sh
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:16.260 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2107
[WI-0][TI-1464] - [INFO] 2024-04-24 11:18:16.517 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1464, success=true)
[WI-0][TI-1464] - [INFO] 2024-04-24 11:18:16.525 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1464)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:17.260 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.262 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1464, processId:2107 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.262 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.262 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.262 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.262 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.279 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.283 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1464
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1464
[WI-582][TI-1464] - [INFO] 2024-04-24 11:18:17.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1464] - [INFO] 2024-04-24 11:18:17.505 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1464, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:18.667 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1466, taskName=move to processing, firstSubmitTime=1713928698634, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=112, appIds=null, processInstanceId=582, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1466'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111818'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='582'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.671 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.671 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.686 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.687 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928698687
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 582_1466
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.687 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1466,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713928698634,
  "startTime" : 1713928698687,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/112/582/1466.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 112,
  "processInstanceId" : 582,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1466"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111818"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "582"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "582_1466",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.688 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.688 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.688 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.709 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.728 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.737 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1466 check successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.738 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.738 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.739 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.750 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.751 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.751 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.761 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.761 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1466/582_1466.sh
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:18.774 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2121
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:19.299 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7717717717717718 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1466] - [INFO] 2024-04-24 11:18:19.530 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1466, success=true)
[WI-0][TI-1466] - [INFO] 2024-04-24 11:18:19.538 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1466)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:19.780 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.783 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1466, processId:2121 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.783 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.783 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.783 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.784 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.790 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.798 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.798 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1466
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.799 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1466
[WI-582][TI-1466] - [INFO] 2024-04-24 11:18:19.799 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1466] - [INFO] 2024-04-24 11:18:20.526 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1466, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:20.597 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1467, taskName=spark preprocessing, firstSubmitTime=1713928700583, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=112, appIds=null, processInstanceId=582, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1467'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424111820'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='582'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.598 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.599 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.601 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.601 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928700602
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 582_1467
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.602 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1467,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713928700583,
  "startTime" : 1713928700602,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/112/582/1467.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 112,
  "processInstanceId" : 582,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1467"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424111820"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "582"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "582_1467",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.603 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.610 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.611 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.613 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467 check successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.614 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.618 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark_reddit_preprocessing.py)})
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.624 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.625 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.625 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.626 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.626 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.629 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.630 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.632 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/500-20240423.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.642 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.643 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/582_1467.sh
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:20.648 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2133
[WI-0][TI-1467] - [INFO] 2024-04-24 11:18:21.522 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1467, success=true)
[WI-0][TI-1467] - [INFO] 2024-04-24 11:18:21.533 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1467)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:21.648 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:22.655 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-8f5c0a91-f29b-4ce4-9b09-8ca8c015bba1;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:23.373 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.707641196013289 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:24.004 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 470ms :: artifacts dl 12ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-8f5c0a91-f29b-4ce4-9b09-8ca8c015bba1
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/8ms)
	24/04/24 11:18:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:25.007 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:24 INFO SparkContext: Running Spark version 3.5.1
	24/04/24 11:18:24 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:18:24 INFO SparkContext: Java version 1.8.0_402
	24/04/24 11:18:24 INFO ResourceUtils: ==============================================================
	24/04/24 11:18:24 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/24 11:18:24 INFO ResourceUtils: ==============================================================
	24/04/24 11:18:24 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/24 11:18:24 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/24 11:18:24 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/24 11:18:24 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/24 11:18:24 INFO SecurityManager: Changing view acls to: default
	24/04/24 11:18:24 INFO SecurityManager: Changing modify acls to: default
	24/04/24 11:18:24 INFO SecurityManager: Changing view acls groups to: 
	24/04/24 11:18:24 INFO SecurityManager: Changing modify acls groups to: 
	24/04/24 11:18:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/24 11:18:24 INFO Utils: Successfully started service 'sparkDriver' on port 43103.
	24/04/24 11:18:24 INFO SparkEnv: Registering MapOutputTracker
	24/04/24 11:18:24 INFO SparkEnv: Registering BlockManagerMaster
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:26.020 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/24 11:18:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/24 11:18:25 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/24 11:18:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b3ae9a57-3e80-476d-8cfd-e46c04445abc
	24/04/24 11:18:25 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/24 11:18:25 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/24 11:18:25 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/24 11:18:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:43103/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:43103/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://6b0330d1bc18:43103/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://6b0330d1bc18:43103/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://6b0330d1bc18:43103/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://6b0330d1bc18:43103/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://6b0330d1bc18:43103/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://6b0330d1bc18:43103/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://6b0330d1bc18:43103/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://6b0330d1bc18:43103/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://6b0330d1bc18:43103/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://6b0330d1bc18:43103/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO SparkContext: Added file file:///local_storage/reddit/processing/500-20240423.json at file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /local_storage/reddit/processing/500-20240423.json to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/500-20240423.json
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:18:25 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:18:25 INFO Executor: Starting executor ID driver on host 6b0330d1bc18
	24/04/24 11:18:25 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:18:25 INFO Executor: Java version 1.8.0_402
	24/04/24 11:18:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/24 11:18:25 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@7ae93a9f for default.
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /local_storage/reddit/processing/500-20240423.json has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/500-20240423.json
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:18:25 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:25 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:18:25 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928704248
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:27.024 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:26 INFO TransportClientFactory: Successfully created connection to 6b0330d1bc18/172.18.1.1:43103 after 112 ms (0 ms spent in bootstraps)
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp9099341527030874328.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp9099341527030874328.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6426344620753080889.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6426344620753080889.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp8501023243265949569.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp8501023243265949569.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp2270790176528914935.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp2270790176528914935.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp2072132105398630895.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp2072132105398630895.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp3701696071701728242.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp3701696071701728242.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6929182070340622646.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6929182070340622646.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6931457231060372409.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6931457231060372409.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp2410552560615527437.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp2410552560615527437.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp8320594195885303725.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp8320594195885303725.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp1098632241735211629.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp1098632241735211629.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/24 11:18:26 INFO Executor: Fetching spark://6b0330d1bc18:43103/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928704248
	24/04/24 11:18:26 INFO Utils: Fetching spark://6b0330d1bc18:43103/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6186017103934082585.tmp
	24/04/24 11:18:26 INFO Utils: /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/fetchFileTemp6186017103934082585.tmp has been previously copied to /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:18:26 INFO Executor: Adding file:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/24 11:18:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37313.
	24/04/24 11:18:26 INFO NettyBlockTransferService: Server created on 6b0330d1bc18:37313
	24/04/24 11:18:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/24 11:18:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6b0330d1bc18, 37313, None)
	24/04/24 11:18:26 INFO BlockManagerMasterEndpoint: Registering block manager 6b0330d1bc18:37313 with 366.3 MiB RAM, BlockManagerId(driver, 6b0330d1bc18, 37313, None)
	24/04/24 11:18:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6b0330d1bc18, 37313, None)
	24/04/24 11:18:26 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6b0330d1bc18, 37313, None)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:27.420 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7733333333333333 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:28.037 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/500-20240423.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:28.445 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8836363636363637 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:29.045 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:28 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.6 KiB, free 366.0 MiB)
	24/04/24 11:18:28 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/04/24 11:18:28 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6b0330d1bc18:37313 (size: 32.7 KiB, free: 366.3 MiB)
	24/04/24 11:18:28 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/24 11:18:28 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:29.446 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7703488372093024 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:30.455 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8936781609195402 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:31.457 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.717579250720461 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:32.467 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.821529745042493 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:35.066 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:34 INFO CodeGenerator: Code generated in 411.900794 ms
	24/04/24 11:18:34 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:18:34 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:18:34 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:34 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:18:34 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/24 11:18:34 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:34 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.8 KiB, free 365.9 MiB)
	24/04/24 11:18:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.9 MiB)
	24/04/24 11:18:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 6b0330d1bc18:37313 (size: 12.3 KiB, free: 366.3 MiB)
	24/04/24 11:18:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/24 11:18:35 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:36.087 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:35 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/04/24 11:18:35 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/500-20240423.json:0+147333
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:37.094 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:36 INFO CodeGenerator: Code generated in 115.565513 ms
	24/04/24 11:18:36 INFO PythonRunner: Times: total = 728, boot = 557, init = 158, finish = 13
	24/04/24 11:18:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2947 bytes result sent to driver
	24/04/24 11:18:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1263 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/24 11:18:36 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59663
	24/04/24 11:18:36 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.520 s
	24/04/24 11:18:36 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/24 11:18:36 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.614030 s
	
	
	
	 1 
	
	
	
	
	
	
	 2 
	
	
	
	24/04/24 11:18:36 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 6b0330d1bc18:37313 in memory (size: 12.3 KiB, free: 366.3 MiB)
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:38.182 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:37 INFO CodeGenerator: Code generated in 63.554761 ms
	24/04/24 11:18:37 INFO CodeGenerator: Code generated in 23.036673 ms
	24/04/24 11:18:37 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark_reddit_preprocessing.py:84
	24/04/24 11:18:37 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/24 11:18:37 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark_reddit_preprocessing.py:84)
	24/04/24 11:18:37 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:37 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:37 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:37 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 68.1 KiB, free 365.9 MiB)
	24/04/24 11:18:37 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 365.8 MiB)
	24/04/24 11:18:37 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 6b0330d1bc18:37313 (size: 25.4 KiB, free: 366.2 MiB)
	24/04/24 11:18:37 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:37 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/24 11:18:37 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:18:37 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/04/24 11:18:37 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/userFiles-9943c0b5-445a-4c99-bb57-5f05b3a9aec4/500-20240423.json:0+147333
	24/04/24 11:18:37 INFO CodeGenerator: Code generated in 14.925648 ms
	24/04/24 11:18:37 INFO CodeGenerator: Code generated in 20.794436 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:39.184 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:38 INFO CodeGenerator: Code generated in 36.901629 ms
	24/04/24 11:18:38 INFO CodeGenerator: Code generated in 32.010802 ms
	24/04/24 11:18:38 INFO PythonRunner: Times: total = 126, boot = -1408, init = 1520, finish = 14
	24/04/24 11:18:38 INFO CodeGenerator: Code generated in 23.710789 ms
	24/04/24 11:18:38 INFO CodeGenerator: Code generated in 55.695475 ms
	24/04/24 11:18:38 INFO PythonUDFRunner: Times: total = 606, boot = 469, init = 136, finish = 1
	24/04/24 11:18:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 135926 bytes result sent to driver
	24/04/24 11:18:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1129 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/24 11:18:38 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark_reddit_preprocessing.py:84) finished in 1.158 s
	24/04/24 11:18:38 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/24 11:18:38 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467/spark_reddit_preprocessing.py:84, took 1.183878 s
	24/04/24 11:18:38 INFO CodeGenerator: Code generated in 7.110504 ms
	24/04/24 11:18:38 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:38 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:18:38 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:18:38 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:38 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:38 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:18:38 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:18:38 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 6b0330d1bc18:37313 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:18:38 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:38 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/24 11:18:38 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 13432 bytes) 
	24/04/24 11:18:38 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/04/24 11:18:39 INFO CodeGenerator: Code generated in 6.832929 ms
	24/04/24 11:18:39 INFO CodeGenerator: Code generated in 17.113049 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:40.213 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:39 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/04/24 11:18:39 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/24 11:18:39 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/24 11:18:39 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/24 11:18:39 INFO AppInfoParser: Kafka startTimeMs: 1713928719313
	24/04/24 11:18:39 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 6b0330d1bc18:37313 in memory (size: 25.4 KiB, free: 366.3 MiB)
	24/04/24 11:18:39 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to DgmOLtAzSgqCCNpqYUj5ng
	24/04/24 11:18:39 INFO Metadata: [Producer clientId=producer-1] Cluster ID: L4eea4e_RqemEIkj6Jy59w
	24/04/24 11:18:39 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 1 with epoch 0
	24/04/24 11:18:39 INFO PythonRunner: Times: total = 113, boot = -1314, init = 1427, finish = 0
	24/04/24 11:18:40 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1753 bytes result sent to driver
	24/04/24 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1094 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/24 11:18:40 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.105 s
	24/04/24 11:18:40 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/24 11:18:40 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 1.120990 s
	24/04/24 11:18:40 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:40 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:18:40 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:18:40 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:40 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:40 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.7 KiB, free 365.9 MiB)
	24/04/24 11:18:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.9 MiB)
	24/04/24 11:18:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 6b0330d1bc18:37313 (size: 9.2 KiB, free: 366.3 MiB)
	24/04/24 11:18:40 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/04/24 11:18:40 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 13432 bytes) 
	24/04/24 11:18:40 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:40.523 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7137404580152671 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:41.220 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:40 INFO PythonRunner: Times: total = 139, boot = -1024, init = 1162, finish = 1
	24/04/24 11:18:40 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1710 bytes result sent to driver
	24/04/24 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 172 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/04/24 11:18:40 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.186 s
	24/04/24 11:18:40 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/04/24 11:18:40 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 0.202786 s
	24/04/24 11:18:40 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:40 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:18:40 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:18:40 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:40 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:40 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:40 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:18:40 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:18:40 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 6b0330d1bc18:37313 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:18:40 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
	24/04/24 11:18:40 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 13432 bytes) 
	24/04/24 11:18:40 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
	24/04/24 11:18:40 INFO PythonRunner: Times: total = 120, boot = -206, init = 326, finish = 0
	24/04/24 11:18:40 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1710 bytes result sent to driver
	24/04/24 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 145 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
	24/04/24 11:18:40 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.191 s
	24/04/24 11:18:40 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
	24/04/24 11:18:40 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 0.213530 s
	24/04/24 11:18:40 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:40 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:18:40 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:18:40 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:40 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:40 INFO DAGScheduler: Submitting ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:40 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:18:40 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:18:40 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 6b0330d1bc18:37313 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:18:40 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/04/24 11:18:40 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 13432 bytes) 
	24/04/24 11:18:40 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
	24/04/24 11:18:40 INFO PythonRunner: Times: total = 87, boot = -135, init = 216, finish = 6
	24/04/24 11:18:40 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1710 bytes result sent to driver
	24/04/24 11:18:40 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 107 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/04/24 11:18:40 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.126 s
	24/04/24 11:18:40 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/04/24 11:18:40 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 0.132691 s
	24/04/24 11:18:41 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:18:41 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:18:41 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:18:41 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:18:41 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:18:41 INFO DAGScheduler: Submitting ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:18:41 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:18:41 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:18:41 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 6b0330d1bc18:37313 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:18:41 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:18:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:18:41 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/04/24 11:18:41 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 13432 bytes) 
	24/04/24 11:18:41 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:41.537 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9304635761589404 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:18:42.233 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:18:41 INFO PythonRunner: Times: total = 161, boot = -320, init = 481, finish = 0
	24/04/24 11:18:41 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1710 bytes result sent to driver
	24/04/24 11:18:41 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 236 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:18:41 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/04/24 11:18:41 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 0.263 s
	24/04/24 11:18:41 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:18:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
	24/04/24 11:18:41 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 0.353383 s
	24/04/24 11:18:41 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/24 11:18:41 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/24 11:18:41 INFO SparkUI: Stopped Spark web UI at http://6b0330d1bc18:4040
	24/04/24 11:18:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/24 11:18:41 INFO MemoryStore: MemoryStore cleared
	24/04/24 11:18:41 INFO BlockManager: BlockManager stopped
	24/04/24 11:18:41 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/24 11:18:41 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/24 11:18:41 INFO SparkContext: Successfully stopped SparkContext
	24/04/24 11:18:41 INFO ShutdownHookManager: Shutdown hook called
	24/04/24 11:18:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c/pyspark-017f0ca0-3e3c-4dee-a085-d377247607d6
	24/04/24 11:18:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-be4d51b9-f97d-4bc8-a19b-cf1e079d424c
	24/04/24 11:18:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a5d12c17-ba90-4733-a24c-57f5c8e5a854
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.237 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467, processId:2133 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.238 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.238 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.242 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.243 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.244 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1467
[WI-582][TI-1467] - [INFO] 2024-04-24 11:18:43.245 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1467] - [INFO] 2024-04-24 11:18:43.588 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1467, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:19:01.664 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7264437689969605 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:19:02.717 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8112676056338028 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:21:25.657 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7058823529411764 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:06.824 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7017045454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:11.822 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1468, taskName=search for intake JSON files, firstSubmitTime=1713928931797, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=112, appIds=null, processInstanceId=582, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1468'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112211'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='582'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.825 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.825 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.827 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.828 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.828 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.828 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928931828
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.828 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 582_1468
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.830 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1468,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713928931797,
  "startTime" : 1713928931828,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/112/582/1468.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 112,
  "processInstanceId" : 582,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1468"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112211"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "582"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "582_1468",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.836 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.836 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.836 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.845 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.846 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1468 check successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.847 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.848 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.848 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.849 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.849 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.849 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.849 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.850 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.850 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.850 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}"
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.851 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.851 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1468/582_1468.sh
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:11.861 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2418
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:11.915 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7553516819571865 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-1468] - [INFO] 2024-04-24 11:22:12.675 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1468, success=true)
[WI-0][TI-1468] - [INFO] 2024-04-24 11:22:12.684 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1468)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:12.882 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.915 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1468, processId:2418 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.922 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.933 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.937 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.941 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.960 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1468
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.962 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1468
[WI-582][TI-1468] - [INFO] 2024-04-24 11:22:12.964 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1468] - [INFO] 2024-04-24 11:22:14.142 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1468, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:15.210 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1470, taskName=move to processing, firstSubmitTime=1713928935203, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=112, appIds=null, processInstanceId=582, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1470'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112215'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='582'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.211 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.211 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.212 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.212 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928935212
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.212 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 582_1470
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.213 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1470,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713928935203,
  "startTime" : 1713928935212,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/112/582/1470.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 112,
  "processInstanceId" : 582,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1470"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112215"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "582"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "582_1470",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.213 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.234 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1470 check successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.235 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.236 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.236 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.236 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.236 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.236 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.237 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1470/582_1470.sh
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:15.247 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2432
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:15.248 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1470] - [INFO] 2024-04-24 11:22:16.161 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1470, success=true)
[WI-0][TI-1470] - [INFO] 2024-04-24 11:22:16.169 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1470)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:16.254 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.255 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1470, processId:2432 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.272 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.273 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.274 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.277 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.278 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1470
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.278 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1470
[WI-582][TI-1470] - [INFO] 2024-04-24 11:22:16.279 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1470] - [INFO] 2024-04-24 11:22:17.157 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1470, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:17.243 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1471, taskName=spark preprocessing, firstSubmitTime=1713928937234, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=112, appIds=null, processInstanceId=582, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='spark preprocessing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1471'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112217'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='582'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.244 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: spark preprocessing to wait queue success
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.244 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.245 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713928937246
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 582_1471
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1471,
  "taskName" : "spark preprocessing",
  "firstSubmitTime" : 1713928937234,
  "startTime" : 1713928937246,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/112/582/1471.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 112,
  "processInstanceId" : 582,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "spark preprocessing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1471"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112217"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "582"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "582_1471",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.246 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.251 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471 check successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.252 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.253 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark_reddit_preprocessing.py)})
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.254 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.255 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/500-20240423.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.255 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.256 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/582_1471.sh
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:17.259 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2444
[WI-0][TI-1471] - [INFO] 2024-04-24 11:22:18.166 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1471, success=true)
[WI-0][TI-1471] - [INFO] 2024-04-24 11:22:18.177 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1471)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:18.266 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:19.299 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-f7234c02-3c73-4d0c-a5ed-21297a859337;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:20.302 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 455ms :: artifacts dl 11ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-f7234c02-3c73-4d0c-a5ed-21297a859337
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/9ms)
	24/04/24 11:22:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:21.304 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:20 INFO SparkContext: Running Spark version 3.5.1
	24/04/24 11:22:20 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:22:20 INFO SparkContext: Java version 1.8.0_402
	24/04/24 11:22:20 INFO ResourceUtils: ==============================================================
	24/04/24 11:22:20 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/24 11:22:20 INFO ResourceUtils: ==============================================================
	24/04/24 11:22:20 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/24 11:22:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/24 11:22:20 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/24 11:22:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/24 11:22:20 INFO SecurityManager: Changing view acls to: default
	24/04/24 11:22:20 INFO SecurityManager: Changing modify acls to: default
	24/04/24 11:22:20 INFO SecurityManager: Changing view acls groups to: 
	24/04/24 11:22:20 INFO SecurityManager: Changing modify acls groups to: 
	24/04/24 11:22:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/04/24 11:22:21 INFO Utils: Successfully started service 'sparkDriver' on port 45279.
	24/04/24 11:22:21 INFO SparkEnv: Registering MapOutputTracker
	24/04/24 11:22:21 INFO SparkEnv: Registering BlockManagerMaster
	24/04/24 11:22:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/24 11:22:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/24 11:22:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/24 11:22:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-272958a4-74cf-4f89-90b0-1587048a460f
	24/04/24 11:22:21 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/24 11:22:21 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:22.305 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/24 11:22:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:45279/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:45279/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://6b0330d1bc18:45279/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://6b0330d1bc18:45279/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://6b0330d1bc18:45279/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://6b0330d1bc18:45279/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://6b0330d1bc18:45279/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://6b0330d1bc18:45279/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://6b0330d1bc18:45279/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://6b0330d1bc18:45279/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://6b0330d1bc18:45279/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://6b0330d1bc18:45279/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO SparkContext: Added file file:///local_storage/reddit/processing/500-20240423.json at file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /local_storage/reddit/processing/500-20240423.json to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/500-20240423.json
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:22:21 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928940528
	24/04/24 11:22:21 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:22:21 INFO Executor: Starting executor ID driver on host 6b0330d1bc18
	24/04/24 11:22:21 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:22:21 INFO Executor: Java version 1.8.0_402
	24/04/24 11:22:21 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/24 11:22:22 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@691a3205 for default.
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /local_storage/reddit/processing/500-20240423.json has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/500-20240423.json
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:22:22 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713928940528
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:23.309 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:22 INFO TransportClientFactory: Successfully created connection to 6b0330d1bc18/172.18.1.1:45279 after 47 ms (0 ms spent in bootstraps)
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp3084777035401344384.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp3084777035401344384.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5844261751086705910.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5844261751086705910.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp6249255321184945455.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp6249255321184945455.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp8212563107257851223.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp8212563107257851223.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp1013666881610745032.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp1013666881610745032.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp687876804897391243.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp687876804897391243.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp7419034297515833353.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp7419034297515833353.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5334567033579919829.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5334567033579919829.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp4363376977383408342.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp4363376977383408342.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5166195557819455955.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5166195557819455955.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5220441463740045151.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5220441463740045151.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/24 11:22:22 INFO Executor: Fetching spark://6b0330d1bc18:45279/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713928940528
	24/04/24 11:22:22 INFO Utils: Fetching spark://6b0330d1bc18:45279/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5203858571016590313.tmp
	24/04/24 11:22:22 INFO Utils: /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/fetchFileTemp5203858571016590313.tmp has been previously copied to /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:22:22 INFO Executor: Adding file:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:22:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36037.
	24/04/24 11:22:23 INFO NettyBlockTransferService: Server created on 6b0330d1bc18:36037
	24/04/24 11:22:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/24 11:22:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6b0330d1bc18, 36037, None)
	24/04/24 11:22:23 INFO BlockManagerMasterEndpoint: Registering block manager 6b0330d1bc18:36037 with 366.3 MiB RAM, BlockManagerId(driver, 6b0330d1bc18, 36037, None)
	24/04/24 11:22:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6b0330d1bc18, 36037, None)
	24/04/24 11:22:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6b0330d1bc18, 36037, None)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:24.312 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/500-20240423.json 
	
	
	
	24/04/24 11:22:24 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.6 KiB, free 366.0 MiB)
	24/04/24 11:22:24 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/04/24 11:22:24 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6b0330d1bc18:36037 (size: 32.7 KiB, free: 366.3 MiB)
	24/04/24 11:22:24 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:24 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/24 11:22:24 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:29.326 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:28 INFO CodeGenerator: Code generated in 341.666952 ms
	24/04/24 11:22:29 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:22:29 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:22:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:22:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/24 11:22:29 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:29 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.8 KiB, free 365.9 MiB)
	24/04/24 11:22:29 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.9 MiB)
	24/04/24 11:22:29 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 6b0330d1bc18:36037 (size: 12.3 KiB, free: 366.3 MiB)
	24/04/24 11:22:29 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:29 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:30.329 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:22:29 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/04/24 11:22:29 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/500-20240423.json:0+147333
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:31.228 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7113702623906706 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:31.340 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:30 INFO CodeGenerator: Code generated in 62.795531 ms
	24/04/24 11:22:30 INFO PythonRunner: Times: total = 644, boot = 447, init = 183, finish = 14
	24/04/24 11:22:30 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2947 bytes result sent to driver
	24/04/24 11:22:30 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1113 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:30 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/24 11:22:30 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 50759
	24/04/24 11:22:30 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.363 s
	24/04/24 11:22:30 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/24 11:22:30 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.479358 s
	
	
	
	 1 
	
	
	
	
	
	
	 2 
	
	
	
	24/04/24 11:22:30 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 6b0330d1bc18:36037 in memory (size: 12.3 KiB, free: 366.3 MiB)
	
	
	
	 3 
	
	
	
	
	
	
	 4 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:32.262 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8768768768768769 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:32.343 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:31 INFO CodeGenerator: Code generated in 41.7212 ms
	24/04/24 11:22:31 INFO CodeGenerator: Code generated in 14.742538 ms
	24/04/24 11:22:32 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark_reddit_preprocessing.py:84
	24/04/24 11:22:32 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/24 11:22:32 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark_reddit_preprocessing.py:84)
	24/04/24 11:22:32 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:32 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 68.1 KiB, free 365.9 MiB)
	24/04/24 11:22:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.3 KiB, free 365.8 MiB)
	24/04/24 11:22:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 6b0330d1bc18:36037 (size: 25.3 KiB, free: 366.2 MiB)
	24/04/24 11:22:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/24 11:22:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:22:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:33.264 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9945054945054945 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:33.351 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:32 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/userFiles-2844ae54-ed38-43f9-8e4c-aaca279cf2c4/500-20240423.json:0+147333
	24/04/24 11:22:32 INFO CodeGenerator: Code generated in 59.907165 ms
	24/04/24 11:22:33 INFO CodeGenerator: Code generated in 86.199904 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:34.267 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8380681818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:34.362 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:34 INFO CodeGenerator: Code generated in 20.225838 ms
	24/04/24 11:22:34 INFO CodeGenerator: Code generated in 39.47987 ms
	24/04/24 11:22:34 INFO PythonRunner: Times: total = 427, boot = -2203, init = 2562, finish = 68
	24/04/24 11:22:34 INFO CodeGenerator: Code generated in 21.134869 ms
	24/04/24 11:22:34 INFO CodeGenerator: Code generated in 50.459464 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:35.364 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:34 INFO PythonUDFRunner: Times: total = 1010, boot = 874, init = 135, finish = 1
	24/04/24 11:22:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 135926 bytes result sent to driver
	24/04/24 11:22:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2096 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/24 11:22:34 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark_reddit_preprocessing.py:84) finished in 2.225 s
	24/04/24 11:22:34 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/24 11:22:34 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471/spark_reddit_preprocessing.py:84, took 2.271928 s
	24/04/24 11:22:34 INFO CodeGenerator: Code generated in 7.200507 ms
	24/04/24 11:22:34 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:34 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:22:34 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:22:34 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:34 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:34 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:22:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:22:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 6b0330d1bc18:36037 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:22:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/24 11:22:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 11577 bytes) 
	24/04/24 11:22:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/04/24 11:22:34 INFO CodeGenerator: Code generated in 5.524715 ms
	24/04/24 11:22:35 INFO CodeGenerator: Code generated in 9.615466 ms
	24/04/24 11:22:35 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/04/24 11:22:35 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/24 11:22:35 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/24 11:22:35 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/24 11:22:35 INFO AppInfoParser: Kafka startTimeMs: 1713928955160
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:36.366 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 6b0330d1bc18:36037 in memory (size: 25.3 KiB, free: 366.3 MiB)
	24/04/24 11:22:35 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to DgmOLtAzSgqCCNpqYUj5ng
	24/04/24 11:22:35 INFO Metadata: [Producer clientId=producer-1] Cluster ID: L4eea4e_RqemEIkj6Jy59w
	24/04/24 11:22:35 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 2 with epoch 0
	24/04/24 11:22:35 INFO PythonRunner: Times: total = 80, boot = -1926, init = 2006, finish = 0
	24/04/24 11:22:35 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1796 bytes result sent to driver
	24/04/24 11:22:35 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1034 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:35 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/24 11:22:35 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.052 s
	24/04/24 11:22:35 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/24 11:22:35 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 1.068847 s
	24/04/24 11:22:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:36 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:22:36 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:22:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:36 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.7 KiB, free 365.9 MiB)
	24/04/24 11:22:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.9 MiB)
	24/04/24 11:22:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 6b0330d1bc18:36037 (size: 9.2 KiB, free: 366.3 MiB)
	24/04/24 11:22:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/04/24 11:22:36 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10868 bytes) 
	24/04/24 11:22:36 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/04/24 11:22:36 INFO PythonRunner: Times: total = 121, boot = -1064, init = 1185, finish = 0
	24/04/24 11:22:36 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1710 bytes result sent to driver
	24/04/24 11:22:36 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 161 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/04/24 11:22:36 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.182 s
	24/04/24 11:22:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/04/24 11:22:36 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 0.203913 s
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:37.297 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7521865889212828 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:37.369 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:36 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:22:36 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:22:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:36 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:22:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:22:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 6b0330d1bc18:36037 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:22:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
	24/04/24 11:22:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10770 bytes) 
	24/04/24 11:22:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
	24/04/24 11:22:36 INFO PythonRunner: Times: total = 81, boot = -192, init = 273, finish = 0
	24/04/24 11:22:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1710 bytes result sent to driver
	24/04/24 11:22:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 117 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
	24/04/24 11:22:36 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.145 s
	24/04/24 11:22:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
	24/04/24 11:22:36 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 0.180117 s
	24/04/24 11:22:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:36 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:22:36 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:22:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:36 INFO DAGScheduler: Submitting ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:22:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:22:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 6b0330d1bc18:36037 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:22:36 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/04/24 11:22:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10847 bytes) 
	24/04/24 11:22:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
	24/04/24 11:22:36 INFO PythonRunner: Times: total = 147, boot = -223, init = 370, finish = 0
	24/04/24 11:22:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1710 bytes result sent to driver
	24/04/24 11:22:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 174 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/04/24 11:22:36 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.222 s
	24/04/24 11:22:36 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/04/24 11:22:36 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 0.238274 s
	24/04/24 11:22:37 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:22:37 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:22:37 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:22:37 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:22:37 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:22:37 INFO DAGScheduler: Submitting ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:22:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:22:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:22:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 6b0330d1bc18:36037 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:22:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:22:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:22:37 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/04/24 11:22:37 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 11034 bytes) 
	24/04/24 11:22:37 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
	24/04/24 11:22:37 INFO PythonRunner: Times: total = 86, boot = -134, init = 219, finish = 1
	24/04/24 11:22:37 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1710 bytes result sent to driver
	24/04/24 11:22:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 113 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:22:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/04/24 11:22:37 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 0.127 s
	24/04/24 11:22:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:22:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
	24/04/24 11:22:37 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 0.145170 s
	24/04/24 11:22:37 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/24 11:22:37 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/24 11:22:37 INFO SparkUI: Stopped Spark web UI at http://6b0330d1bc18:4040
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:38.388 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:22:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/24 11:22:37 INFO MemoryStore: MemoryStore cleared
	24/04/24 11:22:37 INFO BlockManager: BlockManager stopped
	24/04/24 11:22:37 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/24 11:22:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/24 11:22:37 INFO SparkContext: Successfully stopped SparkContext
	24/04/24 11:22:37 INFO ShutdownHookManager: Shutdown hook called
	24/04/24 11:22:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a
	24/04/24 11:22:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-bc2685de-016a-420a-907b-6bb13e247a8a/pyspark-02f4ad9c-8ee2-4a17-bf9e-6298f723c7b8
	24/04/24 11:22:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-08b5dd39-e11e-4aac-9cb3-7465e653b137
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.389 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471, processId:2444 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.392 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.396 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_112/582/1471
[WI-582][TI-1471] - [INFO] 2024-04-24 11:22:38.400 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1471] - [INFO] 2024-04-24 11:22:39.222 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1471, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:58.425 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9538043478260869 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:22:59.531 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.863905325443787 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:24:57.144 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7976539589442815 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:25:08.200 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7002967359050445 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:25:50.366 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7583892617449663 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:13.274 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1472, taskName=search for intake JSON files, firstSubmitTime=1713929233261, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=583, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1472'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340113777664'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112713'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='583'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.277 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake JSON files to wait queue success
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.277 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.279 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.279 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.279 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.279 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713929233279
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.279 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 583_1472
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.280 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1472,
  "taskName" : "search for intake JSON files",
  "firstSubmitTime" : 1713929233261,
  "startTime" : 1713929233279,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/583/1472.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 583,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set raw_file_dir to null\\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1472"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340113777664"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112713"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "583"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "583_1472",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.280 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.281 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.281 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.284 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.285 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1472 check successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.286 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${REDDIT_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set raw_file_dir to null\nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"",
  "resourceList" : [ ]
}
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.287 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.288 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.288 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/reddit/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set raw_file_dir to null
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.289 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.289 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1472/583_1472.sh
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:13.307 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2742
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:13.309 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-1472] - [INFO] 2024-04-24 11:27:13.843 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1472, success=true)
[WI-0][TI-1472] - [INFO] 2024-04-24 11:27:13.851 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1472)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:14.320 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.321 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1472, processId:2742 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.323 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.326 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1472
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.327 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1472
[WI-583][TI-1472] - [INFO] 2024-04-24 11:27:14.327 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1472] - [INFO] 2024-04-24 11:27:14.819 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1472, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:15.979 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1474, taskName=move to processing, firstSubmitTime=1713929235953, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=583, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1474'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13340390094208'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112715'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='583'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.989 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.990 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.992 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.992 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713929235992
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 583_1474
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.992 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1474,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1713929235953,
  "startTime" : 1713929235992,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/583/1474.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 583,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${REDDIT_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1474"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13340390094208"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112715"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "583"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "583_1474",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.993 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1474 check successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.998 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.999 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${REDDIT_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${REDDIT_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${REDDIT_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.999 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:15.999 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:16.001 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:16.001 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:16.001 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is reddit/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/reddit/processing/500-20240423.json")
if ! grep -q "/local_storage/reddit/processing" <<< "/local_storage/reddit/processing/500-20240423.json"; then
    mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/reddit/processing/${filename})}"
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:16.002 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:16.002 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1474/583_1474.sh
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:16.006 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2756
[WI-0][TI-1474] - [INFO] 2024-04-24 11:27:16.823 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1474, success=true)
[WI-0][TI-1474] - [INFO] 2024-04-24 11:27:16.830 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1474)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:17.007 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/reddit/processing/500-20240423.json)}
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.012 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1474, processId:2756 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.014 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.015 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.018 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1474
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.019 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1474
[WI-583][TI-1474] - [INFO] 2024-04-24 11:27:17.020 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1474] - [INFO] 2024-04-24 11:27:17.819 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1474, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:17.915 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1475, taskName=preprocessing and kafka, firstSubmitTime=1713929237892, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=583, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1475'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13210058002752'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112717'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='583'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.916 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.916 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.918 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.921 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.921 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.921 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713929237921
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.921 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 583_1475
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.922 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1475,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1713929237892,
  "startTime" : 1713929237921,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/583/1475.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 583,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name reddit_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_reddit_preprocessing.py\\n\\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\\\\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1475"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13210058002752"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112717"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "583"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "583_1475",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.922 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.922 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.922 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.925 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475 check successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.928 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.930 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py, resourceRelativePath=spark_reddit_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark_reddit_preprocessing.py)})
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.930 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.930 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.931 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name reddit_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_reddit_preprocessing.py\n\n#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \\",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_reddit_preprocessing.py",
    "res" : null
  } ]
}
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.931 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.931 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.931 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.931 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.931 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.932 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.932 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.932 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3-scala2.13
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name reddit_preprocessing \
    --files /local_storage/reddit/processing/500-20240423.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.13:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_reddit_preprocessing.py

#     --jars spark_packages/spark-sql-kafka-0-10_2.13-3.5.1.jar \
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.932 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.932 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/583_1475.sh
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:17.935 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2768
[WI-0][TI-1475] - [INFO] 2024-04-24 11:27:18.854 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1475, success=true)
[WI-0][TI-1475] - [INFO] 2024-04-24 11:27:18.863 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1475)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:18.937 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:19.941 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3-scala2.13/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:20.942 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.13 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-6cac665f-4866-4f4b-84e6-1e421587f595;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 471ms :: artifacts dl 11ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.13;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.13;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.scala-lang.modules#scala-parallel-collections_2.13;1.0.4 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-6cac665f-4866-4f4b-84e6-1e421587f595
		confs: [default]
		0 artifacts copied, 12 already retrieved (0kB/9ms)
	24/04/24 11:27:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:21.944 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:21 INFO SparkContext: Running Spark version 3.5.1
	24/04/24 11:27:21 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:27:21 INFO SparkContext: Java version 1.8.0_402
	24/04/24 11:27:21 INFO ResourceUtils: ==============================================================
	24/04/24 11:27:21 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/04/24 11:27:21 INFO ResourceUtils: ==============================================================
	24/04/24 11:27:21 INFO SparkContext: Submitted application: reddit_preprocessing
	24/04/24 11:27:21 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/04/24 11:27:21 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/04/24 11:27:21 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/04/24 11:27:21 INFO SecurityManager: Changing view acls to: default
	24/04/24 11:27:21 INFO SecurityManager: Changing modify acls to: default
	24/04/24 11:27:21 INFO SecurityManager: Changing view acls groups to: 
	24/04/24 11:27:21 INFO SecurityManager: Changing modify acls groups to: 
	24/04/24 11:27:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:22.973 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:22 INFO Utils: Successfully started service 'sparkDriver' on port 33939.
	24/04/24 11:27:22 INFO SparkEnv: Registering MapOutputTracker
	24/04/24 11:27:22 INFO SparkEnv: Registering BlockManagerMaster
	24/04/24 11:27:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/04/24 11:27:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/04/24 11:27:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/04/24 11:27:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-00b97b93-2070-4768-9862-d52515994d17
	24/04/24 11:27:22 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/04/24 11:27:22 INFO SparkEnv: Registering OutputCommitCoordinator
	24/04/24 11:27:22 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/04/24 11:27:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:33939/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at spark://6b0330d1bc18:33939/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at spark://6b0330d1bc18:33939/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://6b0330d1bc18:33939/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://6b0330d1bc18:33939/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://6b0330d1bc18:33939/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://6b0330d1bc18:33939/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://6b0330d1bc18:33939/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://6b0330d1bc18:33939/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://6b0330d1bc18:33939/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://6b0330d1bc18:33939/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://6b0330d1bc18:33939/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO SparkContext: Added file file:///local_storage/reddit/processing/500-20240423.json at file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /local_storage/reddit/processing/500-20240423.json to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/500-20240423.json
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar at file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:27:22 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713929241545
	24/04/24 11:27:22 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/commons-logging_commons-logging-1.1.3.jar
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:23.981 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:23 INFO Executor: Starting executor ID driver on host 6b0330d1bc18
	24/04/24 11:27:23 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/04/24 11:27:23 INFO Executor: Java version 1.8.0_402
	24/04/24 11:27:23 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/04/24 11:27:23 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@691a3205 for default.
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///local_storage/reddit/processing/500-20240423.json with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /local_storage/reddit/processing/500-20240423.json has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/500-20240423.json
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:27:23 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO TransportClientFactory: Successfully created connection to 6b0330d1bc18/172.18.1.1:33939 after 67 ms (0 ms spent in bootstraps)
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp568268413341837214.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp568268413341837214.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.scala-lang.modules_scala-parallel-collections_2.13-1.0.4.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp6943663642611202955.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp6943663642611202955.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/commons-logging_commons-logging-1.1.3.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp2515544327628795240.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp2515544327628795240.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp2422080907171086793.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp2422080907171086793.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.lz4_lz4-java-1.8.0.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp8999762954531569300.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp8999762954531569300.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.slf4j_slf4j-api-2.0.7.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp1689925330982610887.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp1689925330982610887.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-token-provider-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp1240916773792875952.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp1240916773792875952.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp6606861249540860272.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp6606861249540860272.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.kafka_kafka-clients-3.4.1.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp4554319326283556154.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp4554319326283556154.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.commons_commons-pool2-2.11.1.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp690391117174804862.tmp
	24/04/24 11:27:23 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp690391117174804862.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar
	24/04/24 11:27:23 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.spark_spark-sql-kafka-0-10_2.13-3.5.1.jar to class loader default
	24/04/24 11:27:23 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1713929241545
	24/04/24 11:27:23 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp3432880209580799780.tmp
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:24.984 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:24 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp3432880209580799780.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/04/24 11:27:24 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/04/24 11:27:24 INFO Executor: Fetching spark://6b0330d1bc18:33939/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1713929241545
	24/04/24 11:27:24 INFO Utils: Fetching spark://6b0330d1bc18:33939/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp5082220143503605865.tmp
	24/04/24 11:27:24 INFO Utils: /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/fetchFileTemp5082220143503605865.tmp has been previously copied to /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/com.google.code.findbugs_jsr305-3.0.0.jar
	24/04/24 11:27:24 INFO Executor: Adding file:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/04/24 11:27:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35747.
	24/04/24 11:27:24 INFO NettyBlockTransferService: Server created on 6b0330d1bc18:35747
	24/04/24 11:27:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/04/24 11:27:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 6b0330d1bc18, 35747, None)
	24/04/24 11:27:24 INFO BlockManagerMasterEndpoint: Registering block manager 6b0330d1bc18:35747 with 366.3 MiB RAM, BlockManagerId(driver, 6b0330d1bc18, 35747, None)
	24/04/24 11:27:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 6b0330d1bc18, 35747, None)
	24/04/24 11:27:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 6b0330d1bc18, 35747, None)
	
	
	
	 /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/500-20240423.json 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:25.986 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.6 KiB, free 366.0 MiB)
	24/04/24 11:27:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.7 KiB, free 365.9 MiB)
	24/04/24 11:27:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 6b0330d1bc18:35747 (size: 32.7 KiB, free: 366.3 MiB)
	24/04/24 11:27:25 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/04/24 11:27:25 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:30.016 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:29 INFO CodeGenerator: Code generated in 355.610649 ms
	24/04/24 11:27:29 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:27:29 INFO FileInputFormat: Total input files to process : 1
	24/04/24 11:27:29 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:29 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:27:29 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/04/24 11:27:29 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:29 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:29 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:29 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.8 KiB, free 365.9 MiB)
	24/04/24 11:27:30 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.3 KiB, free 365.9 MiB)
	24/04/24 11:27:30 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 6b0330d1bc18:35747 (size: 12.3 KiB, free: 366.3 MiB)
	24/04/24 11:27:30 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:31.018 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:30 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/04/24 11:27:30 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:27:30 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/04/24 11:27:30 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/500-20240423.json:0+147333
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:32.033 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:31 INFO CodeGenerator: Code generated in 65.767612 ms
	24/04/24 11:27:31 INFO PythonRunner: Times: total = 694, boot = 565, init = 115, finish = 14
	24/04/24 11:27:31 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2947 bytes result sent to driver
	24/04/24 11:27:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1255 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:31 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 59415
	24/04/24 11:27:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/04/24 11:27:31 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.568 s
	24/04/24 11:27:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/04/24 11:27:31 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.654996 s
	
	
	
	 1 
	
	
	
	
	
	
	 2 
	
	
	
	24/04/24 11:27:31 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 6b0330d1bc18:35747 in memory (size: 12.3 KiB, free: 366.3 MiB)
	
	
	
	 3 
	
	
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:33.003 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7134328358208956 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:33.038 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	
	
	
	 4 
	
	
	
	24/04/24 11:27:32 INFO CodeGenerator: Code generated in 29.29278 ms
	24/04/24 11:27:32 INFO CodeGenerator: Code generated in 17.952813 ms
	24/04/24 11:27:32 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark_reddit_preprocessing.py:84
	24/04/24 11:27:32 INFO DAGScheduler: Got job 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark_reddit_preprocessing.py:84) with 1 output partitions
	24/04/24 11:27:32 INFO DAGScheduler: Final stage: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark_reddit_preprocessing.py:84)
	24/04/24 11:27:32 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:32 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:32 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:32 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 68.1 KiB, free 365.9 MiB)
	24/04/24 11:27:32 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.4 KiB, free 365.8 MiB)
	24/04/24 11:27:32 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 6b0330d1bc18:35747 (size: 25.4 KiB, free: 366.2 MiB)
	24/04/24 11:27:32 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:27:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[19] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:32 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/04/24 11:27:32 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10558 bytes) 
	24/04/24 11:27:32 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/04/24 11:27:32 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/userFiles-557aa059-bd45-4259-9ae4-580856037a09/500-20240423.json:0+147333
	24/04/24 11:27:32 INFO CodeGenerator: Code generated in 15.036573 ms
	24/04/24 11:27:32 INFO CodeGenerator: Code generated in 10.4259 ms
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:34.029 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7436708860759493 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:34.063 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:33 INFO CodeGenerator: Code generated in 38.609543 ms
	24/04/24 11:27:33 INFO CodeGenerator: Code generated in 46.684563 ms
	24/04/24 11:27:33 INFO CodeGenerator: Code generated in 35.499577 ms
	24/04/24 11:27:33 INFO PythonRunner: Times: total = 130, boot = -1418, init = 1526, finish = 22
	24/04/24 11:27:33 INFO CodeGenerator: Code generated in 74.923647 ms
	24/04/24 11:27:33 INFO PythonUDFRunner: Times: total = 798, boot = 559, init = 238, finish = 1
	24/04/24 11:27:33 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 135926 bytes result sent to driver
	24/04/24 11:27:33 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1262 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:33 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/04/24 11:27:33 INFO DAGScheduler: ResultStage 1 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark_reddit_preprocessing.py:84) finished in 1.281 s
	24/04/24 11:27:33 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/04/24 11:27:33 INFO DAGScheduler: Job 1 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475/spark_reddit_preprocessing.py:84, took 1.294777 s
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:35.030 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7038626609442059 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:35.064 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:34 INFO CodeGenerator: Code generated in 8.428109 ms
	24/04/24 11:27:34 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:34 INFO DAGScheduler: Got job 2 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:27:34 INFO DAGScheduler: Final stage: ResultStage 2 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:27:34 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:34 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:34 INFO DAGScheduler: Submitting ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:34 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:27:34 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:27:34 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 6b0330d1bc18:35747 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:27:34 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:27:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (SQLExecutionRDD[26] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:34 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/04/24 11:27:34 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 11577 bytes) 
	24/04/24 11:27:34 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/04/24 11:27:34 INFO CodeGenerator: Code generated in 26.7418 ms
	24/04/24 11:27:34 INFO CodeGenerator: Code generated in 22.699863 ms
	24/04/24 11:27:35 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:36.065 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:35 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/04/24 11:27:35 INFO AppInfoParser: Kafka version: 3.4.1
	24/04/24 11:27:35 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/04/24 11:27:35 INFO AppInfoParser: Kafka startTimeMs: 1713929255193
	24/04/24 11:27:35 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 6b0330d1bc18:35747 in memory (size: 25.4 KiB, free: 366.3 MiB)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:37.066 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:36 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition reddit_preprocessed-0 to 0 since the associated topicId changed from null to DgmOLtAzSgqCCNpqYUj5ng
	24/04/24 11:27:36 INFO Metadata: [Producer clientId=producer-1] Cluster ID: L4eea4e_RqemEIkj6Jy59w
	24/04/24 11:27:36 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 3 with epoch 0
	24/04/24 11:27:36 INFO PythonRunner: Times: total = 164, boot = -1973, init = 2136, finish = 1
	24/04/24 11:27:36 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1753 bytes result sent to driver
	24/04/24 11:27:36 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1466 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/04/24 11:27:36 INFO DAGScheduler: ResultStage 2 (save at NativeMethodAccessorImpl.java:0) finished in 1.482 s
	24/04/24 11:27:36 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/04/24 11:27:36 INFO DAGScheduler: Job 2 finished: save at NativeMethodAccessorImpl.java:0, took 1.495443 s
	24/04/24 11:27:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:36 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:27:36 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:27:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:36 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 21.7 KiB, free 365.9 MiB)
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.9 MiB)
	24/04/24 11:27:36 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 6b0330d1bc18:35747 (size: 9.2 KiB, free: 366.3 MiB)
	24/04/24 11:27:36 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:27:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[33] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/04/24 11:27:36 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10868 bytes) 
	24/04/24 11:27:36 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/04/24 11:27:36 INFO PythonRunner: Times: total = 63, boot = -1363, init = 1426, finish = 0
	24/04/24 11:27:36 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1710 bytes result sent to driver
	24/04/24 11:27:36 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 84 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/04/24 11:27:36 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 0.113 s
	24/04/24 11:27:36 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/04/24 11:27:36 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 0.128994 s
	24/04/24 11:27:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:36 INFO DAGScheduler: Got job 4 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:27:36 INFO DAGScheduler: Final stage: ResultStage 4 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:27:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:36 INFO DAGScheduler: Submitting ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:27:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 6b0330d1bc18:35747 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:27:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:27:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (SQLExecutionRDD[40] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
	24/04/24 11:27:36 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10770 bytes) 
	24/04/24 11:27:36 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
	24/04/24 11:27:36 INFO PythonRunner: Times: total = 58, boot = -84, init = 142, finish = 0
	24/04/24 11:27:36 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1710 bytes result sent to driver
	24/04/24 11:27:36 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 76 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
	24/04/24 11:27:36 INFO DAGScheduler: ResultStage 4 (save at NativeMethodAccessorImpl.java:0) finished in 0.086 s
	24/04/24 11:27:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
	24/04/24 11:27:36 INFO DAGScheduler: Job 4 finished: save at NativeMethodAccessorImpl.java:0, took 0.098448 s
	24/04/24 11:27:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:36 INFO DAGScheduler: Got job 5 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:27:36 INFO DAGScheduler: Final stage: ResultStage 5 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:27:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:36 INFO DAGScheduler: Submitting ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:27:36 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 6b0330d1bc18:35747 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:27:36 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:27:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (SQLExecutionRDD[47] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
	24/04/24 11:27:36 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 10847 bytes) 
	24/04/24 11:27:36 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
	24/04/24 11:27:36 INFO PythonRunner: Times: total = 81, boot = -93, init = 174, finish = 0
	24/04/24 11:27:36 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1710 bytes result sent to driver
	24/04/24 11:27:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 114 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
	24/04/24 11:27:36 INFO DAGScheduler: ResultStage 5 (save at NativeMethodAccessorImpl.java:0) finished in 0.127 s
	24/04/24 11:27:36 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
	24/04/24 11:27:36 INFO DAGScheduler: Job 5 finished: save at NativeMethodAccessorImpl.java:0, took 0.143400 s
	24/04/24 11:27:36 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/04/24 11:27:36 INFO DAGScheduler: Got job 6 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/04/24 11:27:36 INFO DAGScheduler: Final stage: ResultStage 6 (save at NativeMethodAccessorImpl.java:0)
	24/04/24 11:27:36 INFO DAGScheduler: Parents of final stage: List()
	24/04/24 11:27:36 INFO DAGScheduler: Missing parents: List()
	24/04/24 11:27:36 INFO DAGScheduler: Submitting ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 21.7 KiB, free 365.8 MiB)
	24/04/24 11:27:36 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 365.8 MiB)
	24/04/24 11:27:36 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 6b0330d1bc18:35747 (size: 9.2 KiB, free: 366.2 MiB)
	24/04/24 11:27:36 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
	24/04/24 11:27:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (SQLExecutionRDD[54] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
	24/04/24 11:27:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (6b0330d1bc18, executor driver, partition 0, PROCESS_LOCAL, 11034 bytes) 
	24/04/24 11:27:36 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
	24/04/24 11:27:36 INFO PythonRunner: Times: total = 60, boot = -117, init = 177, finish = 0
	24/04/24 11:27:36 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1710 bytes result sent to driver
	24/04/24 11:27:36 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 77 ms on 6b0330d1bc18 (executor driver) (1/1)
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
	24/04/24 11:27:36 INFO DAGScheduler: ResultStage 6 (save at NativeMethodAccessorImpl.java:0) finished in 0.087 s
	24/04/24 11:27:36 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
	24/04/24 11:27:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
	24/04/24 11:27:36 INFO DAGScheduler: Job 6 finished: save at NativeMethodAccessorImpl.java:0, took 0.096792 s
	24/04/24 11:27:36 INFO SparkContext: Invoking stop() from shutdown hook
	24/04/24 11:27:36 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/04/24 11:27:36 INFO SparkUI: Stopped Spark web UI at http://6b0330d1bc18:4040
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:38.069 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/04/24 11:27:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/04/24 11:27:37 INFO MemoryStore: MemoryStore cleared
	24/04/24 11:27:37 INFO BlockManager: BlockManager stopped
	24/04/24 11:27:37 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/04/24 11:27:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/04/24 11:27:37 INFO SparkContext: Successfully stopped SparkContext
	24/04/24 11:27:37 INFO ShutdownHookManager: Shutdown hook called
	24/04/24 11:27:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702/pyspark-d5b9a38c-110f-4d7f-91ed-b05cf8fe31dc
	24/04/24 11:27:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-8237ab04-06d1-49d8-885b-0be125ca6cbb
	24/04/24 11:27:37 INFO ShutdownHookManager: Deleting directory /tmp/spark-4e7c5bfb-885f-410c-9114-625c9d1f6702
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.069 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475, processId:2768 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.080 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.080 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.080 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.080 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.085 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.085 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.085 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1475
[WI-583][TI-1475] - [INFO] 2024-04-24 11:27:38.086 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1475] - [INFO] 2024-04-24 11:27:38.923 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1475, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:38.964 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=1476, taskName=move to archives, firstSubmitTime=1713929258953, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.9:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13201021801792, processDefineVersion=113, appIds=null, processInstanceId=583, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit archive folder\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to archives'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240424'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='1476'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13375839065312'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240424112738'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='583'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/reddit/processing/500-20240423.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240423'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_post'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13201021801792'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.967 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to archives to wait queue success
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.978 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.979 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.980 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1713929258981
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 583_1476
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.987 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 1476,
  "taskName" : "move to archives",
  "firstSubmitTime" : 1713929258953,
  "startTime" : 1713929258981,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.9:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240424/13201021801792/113/583/1476.log",
  "processId" : 0,
  "processDefineCode" : 13201021801792,
  "processDefineVersion" : 113,
  "processInstanceId" : 583,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit archive folder\\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to archives"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1476"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13375839065312"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240424112738"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "583"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit/processing/500-20240423.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240423"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_post"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13201021801792"
    }
  },
  "taskAppId" : "583_1476",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/reddit/processing/500-20240423.json\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.989 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.990 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.990 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.994 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.995 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1476 check successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.996 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.996 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit archive folder\nmv ${raw_file_dir} ${REDDIT_HOME}/archive\n",
  "resourceList" : [ ]
}
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.997 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/reddit/processing/500-20240423.json"}] successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.997 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.997 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.997 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.999 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.999 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.999 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit archive folder
mv /local_storage/reddit/processing/500-20240423.json /local_storage/reddit/archive

[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.999 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:38.999 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1476/583_1476.sh
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:39.008 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3012
[WI-0][TI-1476] - [INFO] 2024-04-24 11:27:39.929 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=1476, success=true)
[WI-0][TI-1476] - [INFO] 2024-04-24 11:27:39.936 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=1476)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:40.005 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.007 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1476, processId:3012 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.009 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.010 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.010 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.010 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.013 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1476
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.014 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13201021801792_113/583/1476
[WI-583][TI-1476] - [INFO] 2024-04-24 11:27:40.014 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-1476] - [INFO] 2024-04-24 11:27:40.928 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=1476, success=true)
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:43.059 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8598382749326146 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:27:44.068 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8265582655826559 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:28:00.135 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7749287749287749 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:28:03.205 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9196675900277007 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:29:14.518 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8093922651933702 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:31:40.248 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7366863905325444 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:31:45.279 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.711484593837535 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:31:46.304 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7457627118644067 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:31:48.314 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7341040462427746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:31:53.356 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8666666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:32:15.496 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7360703812316716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:32:19.540 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7329376854599406 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:32:20.551 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7971830985915493 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:32:49.699 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507082152974505 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:32:52.749 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7633136094674556 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:33:28.006 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7492957746478873 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:41:54.633 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7723342939481268 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:41:55.645 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8406593406593406 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:41:56.647 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8370165745856354 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:41:58.674 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.871584699453552 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:41:59.691 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8149171270718232 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:42:00.692 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7542857142857142 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:42:01.693 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.78125 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-04-24 11:42:03.702 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7094017094017094 is over then the MaxCpuUsagePercentageThresholds 0.7
