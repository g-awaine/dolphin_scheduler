[WI-0][TI-0] - [INFO] 2024-05-05 07:17:35.055 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7257142857142858 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:17:36.088 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9293193717277487 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:17:37.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7486033519553073 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:17:40.109 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7093023255813954 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:18:30.390 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7378917378917378 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:18:31.404 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7194029850746269 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:19:14.685 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7262247838616716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:20:24.106 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.78733162072259 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:20:25.126 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8716720907227137 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:20:26.127 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7000000000000001 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:23:26.975 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7385057471264368 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:25:15.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7068493150684931 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:25:47.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7438692098092643 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:25:48.911 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7308781869688384 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:25:56.930 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7681564245810056 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:01.964 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7787356321839081 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:02.978 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7335164835164835 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:04.990 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7574931880108993 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:06.007 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.743801652892562 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:11.019 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7154696132596685 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:13.110 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7087912087912087 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:20.146 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7017045454545454 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:24.176 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7407407407407407 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:28.235 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8111111111111111 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:43.671 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7206703910614525 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:44.799 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9142091152815013 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:45.800 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7130434782608696 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:57.089 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3638, taskName=search for intake article JSON files, firstSubmitTime=1714865217078, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""},{"prop":"is_exists","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake article JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3638'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524226'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505072657'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.092 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake article JSON files to wait queue success
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.094 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.099 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.102 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.103 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.103 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865217103
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.105 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3638
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.110 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3638,
  "taskName" : "search for intake article JSON files",
  "firstSubmitTime" : 1714865217078,
  "startTime" : 1714865217103,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3638.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"is_exists\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set does_file_exist to 1 \\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\n    is_exists=0\\nelse\\n    is_exists=1\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\\necho \\\"#{setValue(is_exists=${is_exists})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake article JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3638"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524226"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505072657"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    }
  },
  "taskAppId" : "991_3638",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.110 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.111 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.111 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.114 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.116 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3638 check successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.118 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.119 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.120 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "is_exists",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"",
  "resourceList" : [ ]
}
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.120 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.121 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.121 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.122 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.122 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.122 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/google_news/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set does_file_exist to 1 
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
    is_exists=0
else
    is_exists=1
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
echo "#{setValue(is_exists=${is_exists})}"
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.122 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.122 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3638/991_3638.sh
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:57.134 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2476
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:57.135 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3638] - [INFO] 2024-05-05 07:26:57.974 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3638, success=true)
[WI-0][TI-3638] - [INFO] 2024-05-05 07:26:57.979 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3638)
[WI-0][TI-0] - [INFO] 2024-05-05 07:26:58.138 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/google_news/processing/988-20240505062610.json)}
	#{setValue(is_exists=1)}
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.139 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3638, processId:2476 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.141 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.142 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.142 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.144 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.146 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3638
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.148 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3638
[WI-991][TI-3638] - [INFO] 2024-05-05 07:26:58.148 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3638] - [INFO] 2024-05-05 07:26:58.980 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3638, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:00.126 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3640, taskName=move to processing, firstSubmitTime=1714865220099, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3640'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505072700'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/988-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.128 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.128 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.129 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.130 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865220130
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.130 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3640
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.131 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3640,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714865220099,
  "startTime" : 1714865220130,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3640.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3640"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505072700"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/988-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "991_3640",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/988-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.131 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.135 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.139 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.139 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3640 check successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.140 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.142 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.145 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.145 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.145 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.146 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.147 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.153 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.157 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.158 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.163 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.165 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/processing/988-20240505062610.json")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/processing/988-20240505062610.json"; then
    mv /local_storage/google_news/processing/988-20240505062610.json /local_storage/google_news/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/google_news/processing/${filename})}"
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.165 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.166 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3640/991_3640.sh
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:00.176 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2490
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:00.177 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3640] - [INFO] 2024-05-05 07:27:01.019 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3640, success=true)
[WI-0][TI-3640] - [INFO] 2024-05-05 07:27:01.025 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3640)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:01.178 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/google_news/processing/988-20240505062610.json)}
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.179 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3640, processId:2490 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.185 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.185 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.185 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.185 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.188 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.188 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.188 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3640
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.188 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3640
[WI-991][TI-3640] - [INFO] 2024-05-05 07:27:01.189 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3640] - [INFO] 2024-05-05 07:27:02.012 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3640, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:02.133 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3641, taskName=preprocessing and kafka, firstSubmitTime=1714865222126, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3641'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505072702'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/988-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.133 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.139 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.140 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.140 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865222141
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3641
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.141 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3641,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714865222126,
  "startTime" : 1714865222141,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3641.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3641"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505072702"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/988-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "991_3641",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/988-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.142 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.142 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.142 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.144 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.145 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641 check successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.147 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.170 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark_gnews_preprocessing.py)})
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.171 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.171 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.172 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.172 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.172 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.172 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.173 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.173 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.173 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.174 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.174 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/988-20240505062610.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.174 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.175 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/991_3641.sh
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:02.178 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2502
[WI-0][TI-3641] - [INFO] 2024-05-05 07:27:03.019 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3641, success=true)
[WI-0][TI-3641] - [INFO] 2024-05-05 07:27:03.041 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3641)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:03.182 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:04.864 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7009063444108761 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:05.205 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-7d3953fa-747a-40a7-a672-67ec46b32f8b;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:05.900 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7421120528555755 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:06.528 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: resolution report :: resolve 727ms :: artifacts dl 80ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-7d3953fa-747a-40a7-a672-67ec46b32f8b
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/13ms)
	24/05/05 07:27:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:07.536 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:06 INFO SparkContext: Running Spark version 3.5.1
	24/05/05 07:27:06 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/05 07:27:06 INFO SparkContext: Java version 1.8.0_402
	24/05/05 07:27:06 INFO ResourceUtils: ==============================================================
	24/05/05 07:27:06 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/05 07:27:06 INFO ResourceUtils: ==============================================================
	24/05/05 07:27:06 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/05 07:27:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/05 07:27:06 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/05 07:27:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
	24/05/05 07:27:07 INFO SecurityManager: Changing view acls to: default
	24/05/05 07:27:07 INFO SecurityManager: Changing modify acls to: default
	24/05/05 07:27:07 INFO SecurityManager: Changing view acls groups to: 
	24/05/05 07:27:07 INFO SecurityManager: Changing modify acls groups to: 
	24/05/05 07:27:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:08.538 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:07 INFO Utils: Successfully started service 'sparkDriver' on port 35393.
	24/05/05 07:27:07 INFO SparkEnv: Registering MapOutputTracker
	24/05/05 07:27:07 INFO SparkEnv: Registering BlockManagerMaster
	24/05/05 07:27:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/05 07:27:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/05 07:27:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/05 07:27:08 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6e3ff291-101b-41d0-9a1c-280d00e39bc6
	24/05/05 07:27:08 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/05 07:27:08 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:08.930 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7992957746478873 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:09.540 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:08 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/05 07:27:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://02dc7f71660d:35393/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://02dc7f71660d:35393/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://02dc7f71660d:35393/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://02dc7f71660d:35393/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://02dc7f71660d:35393/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://02dc7f71660d:35393/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://02dc7f71660d:35393/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://02dc7f71660d:35393/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://02dc7f71660d:35393/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://02dc7f71660d:35393/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://02dc7f71660d:35393/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO SparkContext: Added file file:///local_storage/google_news/processing/988-20240505062610.json at file:///local_storage/google_news/processing/988-20240505062610.json with timestamp 1714865226744
	24/05/05 07:27:08 INFO Utils: Copying /local_storage/google_news/processing/988-20240505062610.json to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/988-20240505062610.json
	24/05/05 07:27:08 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:27:08 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:27:08 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865226744
	24/05/05 07:27:08 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:27:09 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865226744
	24/05/05 07:27:09 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:27:09 INFO Executor: Starting executor ID driver on host 02dc7f71660d
	24/05/05 07:27:09 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/05 07:27:09 INFO Executor: Java version 1.8.0_402
	24/05/05 07:27:09 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/05 07:27:09 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@2bec9e37 for default.
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:09.942 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.796657381615599 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:10.545 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:09 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///local_storage/google_news/processing/988-20240505062610.json with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /local_storage/google_news/processing/988-20240505062610.json has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/988-20240505062610.json
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:27:10 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO TransportClientFactory: Successfully created connection to 02dc7f71660d/172.18.1.1:35393 after 70 ms (0 ms spent in bootstraps)
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp3937236189696378093.tmp
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:10.943 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8138888888888889 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:11.548 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp3937236189696378093.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2703781729005527375.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2703781729005527375.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2551024168480648706.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2551024168480648706.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp882000142228018281.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp882000142228018281.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp5689643707921820413.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp5689643707921820413.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp1905587348591458796.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp1905587348591458796.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp208621989302524057.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp208621989302524057.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:27:10 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/05 07:27:10 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865226744
	24/05/05 07:27:10 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp1704578229814508265.tmp
	24/05/05 07:27:10 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp1704578229814508265.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:27:11 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/05 07:27:11 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865226744
	24/05/05 07:27:11 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2833928385352269607.tmp
	24/05/05 07:27:11 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2833928385352269607.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:27:11 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/05 07:27:11 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865226744
	24/05/05 07:27:11 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp4171218816381614264.tmp
	24/05/05 07:27:11 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp4171218816381614264.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:27:11 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/05 07:27:11 INFO Executor: Fetching spark://02dc7f71660d:35393/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865226744
	24/05/05 07:27:11 INFO Utils: Fetching spark://02dc7f71660d:35393/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2587845184638581263.tmp
	24/05/05 07:27:11 INFO Utils: /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/fetchFileTemp2587845184638581263.tmp has been previously copied to /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:27:11 INFO Executor: Adding file:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:11.951 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.79375 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:12.565 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45037.
	24/05/05 07:27:11 INFO NettyBlockTransferService: Server created on 02dc7f71660d:45037
	24/05/05 07:27:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/05 07:27:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 02dc7f71660d, 45037, None)
	24/05/05 07:27:11 INFO BlockManagerMasterEndpoint: Registering block manager 02dc7f71660d:45037 with 366.3 MiB RAM, BlockManagerId(driver, 02dc7f71660d, 45037, None)
	24/05/05 07:27:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 02dc7f71660d, 45037, None)
	24/05/05 07:27:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 02dc7f71660d, 45037, None)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:12.958 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8083067092651758 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:13.960 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8049535603715171 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:14.567 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
	24/05/05 07:27:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/05/05 07:27:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 02dc7f71660d:45037 (size: 32.6 KiB, free: 366.3 MiB)
	24/05/05 07:27:13 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/05 07:27:13 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/05 07:27:13 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:14.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8535911602209945 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:16.004 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7158915501591593 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:17.006 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8666666666666667 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:18.012 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9382022471910112 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:19.015 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.936 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:20.086 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9233576642335767 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:21.089 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9161490683229814 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:22.091 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9238095238095237 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:22.595 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:22 INFO CodeGenerator: Code generated in 651.381913 ms
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:23.093 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9273255813953487 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:23.600 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:22 INFO FileInputFormat: Total input files to process : 1
	24/05/05 07:27:22 INFO FileInputFormat: Total input files to process : 1
	24/05/05 07:27:22 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/05/05 07:27:22 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:27:22 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/05/05 07:27:22 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:27:22 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:27:22 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:27:23 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.2 KiB, free 365.9 MiB)
	24/05/05 07:27:23 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 365.9 MiB)
	24/05/05 07:27:23 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 02dc7f71660d:45037 (size: 12.5 KiB, free: 366.3 MiB)
	24/05/05 07:27:23 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:27:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:27:23 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/05 07:27:23 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:24.111 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9487951807228915 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:24.609 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:23 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
	24/05/05 07:27:24 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/988-20240505062610.json:0+3160
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:25.121 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9041666666666666 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:26.129 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.956386292834891 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:26.639 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:26 INFO CodeGenerator: Code generated in 154.871071 ms
	24/05/05 07:27:26 INFO PythonRunner: Times: total = 1625, boot = 1354, init = 271, finish = 0
	24/05/05 07:27:26 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2718 bytes result sent to driver
	24/05/05 07:27:26 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2731 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:27:26 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/05 07:27:26 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 55071
	24/05/05 07:27:26 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 3.244 s
	24/05/05 07:27:26 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:27:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/05/05 07:27:26 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 3.519334 s
	24/05/05 07:27:26 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 02dc7f71660d:45037 in memory (size: 12.5 KiB, free: 366.3 MiB)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:27.132 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7952522255192878 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:28.142 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9232954545454545 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:28.644 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:27 INFO CodeGenerator: Code generated in 90.118376 ms
	24/05/05 07:27:27 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
	24/05/05 07:27:27 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:27:27 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
	24/05/05 07:27:27 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:27:27 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:27:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:27:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 30.5 KiB, free 365.9 MiB)
	24/05/05 07:27:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 365.9 MiB)
	24/05/05 07:27:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 02dc7f71660d:45037 (size: 13.4 KiB, free: 366.3 MiB)
	24/05/05 07:27:27 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:27:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:27:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/05/05 07:27:28 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:27:28 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/05/05 07:27:28 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/988-20240505062610.json:0+3160
	24/05/05 07:27:28 INFO CodeGenerator: Code generated in 41.968282 ms
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:29.147 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.882716049382716 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:29.653 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:28 INFO PythonRunner: Times: total = 195, boot = -2155, init = 2350, finish = 0
	24/05/05 07:27:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4228 bytes result sent to driver
	24/05/05 07:27:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 722 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:27:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/05/05 07:27:28 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.998 s
	24/05/05 07:27:28 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:27:28 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/05/05 07:27:28 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 1.108223 s
	24/05/05 07:27:28 INFO CodeGenerator: Code generated in 40.383216 ms
	+-------------------+--------------------+--------------------+--------------------+-------+-----------------+
	|            authors|             content|                 url|             context|   type|         datetime|
	+-------------------+--------------------+--------------------+--------------------+-------+-----------------+
	|[Daphne Psaledakis]|By Daphne Psaleda...|https://news.goog...|Top US Treasury o...|article|24-05-03 18:06:00|
	+-------------------+--------------------+--------------------+--------------------+-------+-----------------+
	
	24/05/05 07:27:29 INFO CodeGenerator: Code generated in 23.289636 ms
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:30.197 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9588235294117646 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:30.656 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:29 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark_gnews_preprocessing.py:67
	24/05/05 07:27:29 INFO DAGScheduler: Got job 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark_gnews_preprocessing.py:67) with 1 output partitions
	24/05/05 07:27:29 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark_gnews_preprocessing.py:67)
	24/05/05 07:27:29 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:27:29 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:27:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:27:29 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 57.5 KiB, free 365.8 MiB)
	24/05/05 07:27:29 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 365.8 MiB)
	24/05/05 07:27:29 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 02dc7f71660d:45037 (size: 23.4 KiB, free: 366.2 MiB)
	24/05/05 07:27:29 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:27:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:27:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/05 07:27:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:27:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/05/05 07:27:29 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/userFiles-351de3a8-ff5e-4706-8db2-74be3e236470/988-20240505062610.json:0+3160
	24/05/05 07:27:29 INFO CodeGenerator: Code generated in 34.664512 ms
	24/05/05 07:27:30 INFO CodeGenerator: Code generated in 81.442513 ms
	24/05/05 07:27:30 INFO CodeGenerator: Code generated in 87.310183 ms
	24/05/05 07:27:30 INFO PythonRunner: Times: total = 138, boot = -1416, init = 1554, finish = 0
	24/05/05 07:27:30 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4781 bytes result sent to driver
	24/05/05 07:27:30 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 640 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:27:30 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/05 07:27:30 INFO DAGScheduler: ResultStage 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark_gnews_preprocessing.py:67) finished in 0.713 s
	24/05/05 07:27:30 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:27:30 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/05/05 07:27:30 INFO DAGScheduler: Job 2 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641/spark_gnews_preprocessing.py:67, took 0.726514 s
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:31.201 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9043715846994536 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:31.658 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:31 INFO CodeGenerator: Code generated in 22.359408 ms
	24/05/05 07:27:31 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/05/05 07:27:31 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:27:31 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/05/05 07:27:31 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:27:31 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:27:31 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[25] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:27:31 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.7 KiB, free 365.8 MiB)
	24/05/05 07:27:31 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/05/05 07:27:31 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 02dc7f71660d:45037 (size: 9.0 KiB, free: 366.2 MiB)
	24/05/05 07:27:31 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:27:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[25] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:27:31 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/05/05 07:27:31 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 12644 bytes) 
	24/05/05 07:27:31 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/05/05 07:27:31 INFO CodeGenerator: Code generated in 62.650313 ms
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:32.206 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.899665551839465 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:32.664 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:31 INFO CodeGenerator: Code generated in 87.134012 ms
	24/05/05 07:27:31 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/05/05 07:27:31 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/05/05 07:27:32 INFO AppInfoParser: Kafka version: 3.4.1
	24/05/05 07:27:32 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/05/05 07:27:32 INFO AppInfoParser: Kafka startTimeMs: 1714865252144
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:33.365 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.847972972972973 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:33.668 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:32 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 02dc7f71660d:45037 in memory (size: 13.4 KiB, free: 366.2 MiB)
	24/05/05 07:27:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 02dc7f71660d:45037 in memory (size: 23.4 KiB, free: 366.3 MiB)
	24/05/05 07:27:33 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition gnews_preprocessed-0 to 0 since the associated topicId changed from null to vBpmtWa3QRWNv4qz8CVDGw
	24/05/05 07:27:33 INFO Metadata: [Producer clientId=producer-1] Cluster ID: xO1AMuOnS8uX6jJWW9t4bg
	24/05/05 07:27:33 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
	24/05/05 07:27:33 INFO PythonRunner: Times: total = 152, boot = -1403, init = 1555, finish = 0
	24/05/05 07:27:33 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1664 bytes result sent to driver
	24/05/05 07:27:33 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 2291 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:27:33 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/05/05 07:27:33 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 2.313 s
	24/05/05 07:27:33 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:27:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/05/05 07:27:33 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 2.334597 s
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:34.376 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.878203110497728 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:34.681 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:27:33 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/05 07:27:33 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/05 07:27:33 INFO SparkUI: Stopped Spark web UI at http://02dc7f71660d:4040
	24/05/05 07:27:33 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/05 07:27:33 INFO MemoryStore: MemoryStore cleared
	24/05/05 07:27:34 INFO BlockManager: BlockManager stopped
	24/05/05 07:27:34 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/05 07:27:34 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/05 07:27:34 INFO SparkContext: Successfully stopped SparkContext
	24/05/05 07:27:34 INFO ShutdownHookManager: Shutdown hook called
	24/05/05 07:27:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-99bbd88b-2c33-42a2-bb16-c615a7535dbe
	24/05/05 07:27:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e/pyspark-28d637d8-0204-4662-a65e-48960ab3d3f4
	24/05/05 07:27:34 INFO ShutdownHookManager: Deleting directory /tmp/spark-37a758da-1931-48fe-b33c-8c0eac7aaf0e
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.691 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641, processId:2502 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.693 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.693 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.694 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.694 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.712 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.712 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.713 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3641
[WI-991][TI-3641] - [INFO] 2024-05-05 07:27:35.713 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3641] - [INFO] 2024-05-05 07:27:36.149 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3641, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:47.426 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.863768115942029 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:48.484 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7595628415300547 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:27:49.492 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7513513513513513 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:04.587 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7529761904761905 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:16.675 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7304347826086955 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:17.688 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7215909090909091 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:18.693 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7091412742382271 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:23.720 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.786144578313253 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:28.819 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7621776504297995 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:28:31.857 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7098591549295774 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:14.525 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7135135135135136 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:21.552 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7275270886797833 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:27.583 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8504398826979471 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:29.662 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7884057971014492 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:31.695 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7348703170028819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:32.733 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7122507122507122 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:29:43.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7648809523809523 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:30:00.861 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7781065088757396 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:30:01.903 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7196765498652292 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:30:02.918 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.719298245614035 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:30:03.925 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7485549132947977 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:31:12.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7728531855955678 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:31:23.275 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7434402332361516 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:26.559 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7886904761904762 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:28.673 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7265193370165746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:51.869 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3646, taskName=search for intake article JSON files, firstSubmitTime=1714865571851, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""},{"prop":"is_exists","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake article JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3646'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524226'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073251'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/988-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.873 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake article JSON files to wait queue success
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.873 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.880 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865571880
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.880 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3646
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.881 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3646,
  "taskName" : "search for intake article JSON files",
  "firstSubmitTime" : 1714865571851,
  "startTime" : 1714865571880,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3646.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"is_exists\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set does_file_exist to 1 \\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\n    is_exists=0\\nelse\\n    is_exists=1\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\\necho \\\"#{setValue(is_exists=${is_exists})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake article JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3646"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524226"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073251"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/988-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "991_3646",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/988-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.882 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.882 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.882 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.886 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.887 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3646 check successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.888 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.889 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "is_exists",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"",
  "resourceList" : [ ]
}
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.889 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.889 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.889 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.889 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.889 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.890 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.890 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.890 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/google_news/processing -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set does_file_exist to 1 
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
    is_exists=0
else
    is_exists=1
fi

echo "#{setValue(raw_file_dir=/local_storage/google_news/processing/988-20240505062610.json)}"
echo "#{setValue(is_exists=1)}"
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.890 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.891 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3646/991_3646.sh
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:51.922 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2788
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:51.937 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-3646] - [INFO] 2024-05-05 07:32:52.261 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3646, success=true)
[WI-0][TI-3646] - [INFO] 2024-05-05 07:32:52.266 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3646)
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:52.962 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	#{setValue(raw_file_dir=/local_storage/google_news/processing/988-20240505062610.json)}
	#{setValue(is_exists=1)}
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.963 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3646, processId:2788 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.979 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.980 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.980 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.981 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.985 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.985 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.985 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3646
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.986 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3646
[WI-991][TI-3646] - [INFO] 2024-05-05 07:32:52.986 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3646] - [INFO] 2024-05-05 07:32:53.248 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3646, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:54.297 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3648, taskName=move to processing, firstSubmitTime=1714865574291, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3648'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073254'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/988-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.298 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.299 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.300 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865574301
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3648
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.301 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3648,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714865574291,
  "startTime" : 1714865574301,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3648.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3648"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073254"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/988-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "991_3648",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/988-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.302 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.305 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3648 check successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.306 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.307 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.308 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.308 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.308 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.308 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.309 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.309 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.309 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.309 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/processing/988-20240505062610.json")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/processing/988-20240505062610.json"; then
    mv /local_storage/google_news/processing/988-20240505062610.json /local_storage/google_news/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/google_news/processing/${filename})}"
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.309 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.310 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3648/991_3648.sh
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:54.313 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2802
[WI-0][TI-3648] - [INFO] 2024-05-05 07:32:55.258 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3648, success=true)
[WI-0][TI-3648] - [INFO] 2024-05-05 07:32:55.268 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3648)
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:55.316 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	JSON is already in processing
	#{setValue(raw_file_dir=/local_storage/google_news/processing/988-20240505062610.json)}
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.319 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3648, processId:2802 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.319 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.321 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.322 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.324 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.331 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3648
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3648
[WI-991][TI-3648] - [INFO] 2024-05-05 07:32:55.332 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3648] - [INFO] 2024-05-05 07:32:56.269 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3648, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:56.351 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3649, taskName=preprocessing and kafka, firstSubmitTime=1714865576334, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3649'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073256'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/988-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.352 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.352 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.354 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.354 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865576354
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3649
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.354 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3649,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714865576334,
  "startTime" : 1714865576354,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3649.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3649"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073256"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/988-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "991_3649",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/988-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.356 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.360 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649 check successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.361 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.379 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark_gnews_preprocessing.py)})
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.390 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.391 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.392 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.392 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/988-20240505062610.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.392 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.392 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/991_3649.sh
[WI-991][TI-3649] - [INFO] 2024-05-05 07:32:56.411 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 2814
[WI-0][TI-3649] - [INFO] 2024-05-05 07:32:57.267 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3649, success=true)
[WI-0][TI-3649] - [INFO] 2024-05-05 07:32:57.280 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3649)
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:57.429 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:57.811 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.868945868945869 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:32:58.827 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8065476190476191 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:00.512 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-b4a6ba88-a0ac-4c40-8af0-2fb66ee81519;1.0
		confs: [default]
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:00.870 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8151260504201681 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:01.512 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:01.937 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9712041884816754 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:02.514 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 1313ms :: artifacts dl 30ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-b4a6ba88-a0ac-4c40-8af0-2fb66ee81519
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/27ms)
	24/05/05 07:33:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:02.938 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9075144508670521 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:03.941 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8123249299719888 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:04.521 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:04 INFO SparkContext: Running Spark version 3.5.1
	24/05/05 07:33:04 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/05 07:33:04 INFO SparkContext: Java version 1.8.0_402
	24/05/05 07:33:04 INFO ResourceUtils: ==============================================================
	24/05/05 07:33:04 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/05 07:33:04 INFO ResourceUtils: ==============================================================
	24/05/05 07:33:04 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/05 07:33:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/05 07:33:04 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/05 07:33:04 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:05.525 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:04 INFO SecurityManager: Changing view acls to: default
	24/05/05 07:33:04 INFO SecurityManager: Changing modify acls to: default
	24/05/05 07:33:04 INFO SecurityManager: Changing view acls groups to: 
	24/05/05 07:33:04 INFO SecurityManager: Changing modify acls groups to: 
	24/05/05 07:33:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/05 07:33:05 INFO Utils: Successfully started service 'sparkDriver' on port 39587.
	24/05/05 07:33:05 INFO SparkEnv: Registering MapOutputTracker
	24/05/05 07:33:05 INFO SparkEnv: Registering BlockManagerMaster
	24/05/05 07:33:05 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/05 07:33:05 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/05 07:33:05 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/05 07:33:05 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-950a04d1-4b98-42f2-b029-560d4c27f38e
	24/05/05 07:33:05 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/05 07:33:05 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:06.529 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:05 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/05 07:33:05 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://02dc7f71660d:39587/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://02dc7f71660d:39587/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://02dc7f71660d:39587/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://02dc7f71660d:39587/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://02dc7f71660d:39587/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://02dc7f71660d:39587/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://02dc7f71660d:39587/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://02dc7f71660d:39587/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://02dc7f71660d:39587/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://02dc7f71660d:39587/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://02dc7f71660d:39587/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO SparkContext: Added file file:///local_storage/google_news/processing/988-20240505062610.json at file:///local_storage/google_news/processing/988-20240505062610.json with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /local_storage/google_news/processing/988-20240505062610.json to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/988-20240505062610.json
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:06.966 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7521613832853027 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:07.536 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:06 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:33:06 INFO Executor: Starting executor ID driver on host 02dc7f71660d
	24/05/05 07:33:06 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/05 07:33:06 INFO Executor: Java version 1.8.0_402
	24/05/05 07:33:06 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/05 07:33:06 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@9bdf67 for default.
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///local_storage/google_news/processing/988-20240505062610.json with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /local_storage/google_news/processing/988-20240505062610.json has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/988-20240505062610.json
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865584043
	24/05/05 07:33:06 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:33:06 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:33:07 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:33:07 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO TransportClientFactory: Successfully created connection to 02dc7f71660d/172.18.1.1:39587 after 81 ms (0 ms spent in bootstraps)
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp3278529228507265060.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp3278529228507265060.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp5915563620130283905.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp5915563620130283905.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp4691126663295265777.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp4691126663295265777.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp2388728841572171717.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp2388728841572171717.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp7712822552439982691.tmp
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:08.548 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp7712822552439982691.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp6418484281920008170.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp6418484281920008170.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp425060721151299351.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp425060721151299351.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp8409737702360107406.tmp
	24/05/05 07:33:07 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp8409737702360107406.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:33:07 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/05 07:33:07 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865584043
	24/05/05 07:33:07 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp6950362147931250064.tmp
	24/05/05 07:33:08 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp6950362147931250064.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:33:08 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/05 07:33:08 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865584043
	24/05/05 07:33:08 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp2517442314821485650.tmp
	24/05/05 07:33:08 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp2517442314821485650.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:33:08 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/05 07:33:08 INFO Executor: Fetching spark://02dc7f71660d:39587/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865584043
	24/05/05 07:33:08 INFO Utils: Fetching spark://02dc7f71660d:39587/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp2435420256987125129.tmp
	24/05/05 07:33:08 INFO Utils: /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/fetchFileTemp2435420256987125129.tmp has been previously copied to /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:33:08 INFO Executor: Adding file:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/05 07:33:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43505.
	24/05/05 07:33:08 INFO NettyBlockTransferService: Server created on 02dc7f71660d:43505
	24/05/05 07:33:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/05 07:33:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 02dc7f71660d, 43505, None)
	24/05/05 07:33:08 INFO BlockManagerMasterEndpoint: Registering block manager 02dc7f71660d:43505 with 366.3 MiB RAM, BlockManagerId(driver, 02dc7f71660d, 43505, None)
	24/05/05 07:33:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 02dc7f71660d, 43505, None)
	24/05/05 07:33:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 02dc7f71660d, 43505, None)
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:10.550 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
	24/05/05 07:33:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/05/05 07:33:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 02dc7f71660d:43505 (size: 32.6 KiB, free: 366.3 MiB)
	24/05/05 07:33:10 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/05 07:33:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/05 07:33:10 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:16.590 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:16 INFO CodeGenerator: Code generated in 704.55958 ms
	24/05/05 07:33:16 INFO FileInputFormat: Total input files to process : 1
	24/05/05 07:33:16 INFO FileInputFormat: Total input files to process : 1
	24/05/05 07:33:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:17.607 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:16 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:33:16 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/05/05 07:33:16 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:33:16 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:33:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:33:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.2 KiB, free 365.9 MiB)
	24/05/05 07:33:16 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 365.9 MiB)
	24/05/05 07:33:16 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 02dc7f71660d:43505 (size: 12.5 KiB, free: 366.3 MiB)
	24/05/05 07:33:16 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:33:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:33:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/05 07:33:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:33:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:18.624 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:17 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/988-20240505062610.json:0+3160
	24/05/05 07:33:18 INFO CodeGenerator: Code generated in 104.513434 ms
	24/05/05 07:33:18 INFO PythonRunner: Times: total = 601, boot = 490, init = 109, finish = 2
	24/05/05 07:33:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2718 bytes result sent to driver
	24/05/05 07:33:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1303 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:33:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/05 07:33:18 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 49041
	24/05/05 07:33:18 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.744 s
	24/05/05 07:33:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:33:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/05/05 07:33:18 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.908173 s
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:19.634 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:18 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 02dc7f71660d:43505 in memory (size: 12.5 KiB, free: 366.3 MiB)
	24/05/05 07:33:19 INFO CodeGenerator: Code generated in 74.024724 ms
	24/05/05 07:33:19 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
	24/05/05 07:33:19 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:33:19 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
	24/05/05 07:33:19 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:33:19 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:33:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:33:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 30.5 KiB, free 365.9 MiB)
	24/05/05 07:33:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 365.9 MiB)
	24/05/05 07:33:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 02dc7f71660d:43505 (size: 13.4 KiB, free: 366.3 MiB)
	24/05/05 07:33:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:33:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:33:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/05/05 07:33:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:33:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:20.216 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7799352750809062 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:20.641 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:19 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/988-20240505062610.json:0+3160
	24/05/05 07:33:19 INFO CodeGenerator: Code generated in 74.265948 ms
	24/05/05 07:33:19 INFO PythonRunner: Times: total = 130, boot = -1307, init = 1437, finish = 0
	24/05/05 07:33:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 4228 bytes result sent to driver
	24/05/05 07:33:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 484 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:33:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/05/05 07:33:20 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.515 s
	24/05/05 07:33:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:33:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/05/05 07:33:20 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.559068 s
	24/05/05 07:33:20 INFO CodeGenerator: Code generated in 153.492436 ms
	+-------------------+--------------------+--------------------+--------------------+-------+-----------------+
	|            authors|             content|                 url|             context|   type|         datetime|
	+-------------------+--------------------+--------------------+--------------------+-------+-----------------+
	|[Daphne Psaledakis]|By Daphne Psaleda...|https://news.goog...|Top US Treasury o...|article|24-05-03 18:06:00|
	+-------------------+--------------------+--------------------+--------------------+-------+-----------------+
	
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:21.250 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8533724340175953 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:21.690 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:20 INFO CodeGenerator: Code generated in 31.720021 ms
	24/05/05 07:33:20 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark_gnews_preprocessing.py:67
	24/05/05 07:33:20 INFO DAGScheduler: Got job 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark_gnews_preprocessing.py:67) with 1 output partitions
	24/05/05 07:33:20 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark_gnews_preprocessing.py:67)
	24/05/05 07:33:20 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:33:20 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:33:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:33:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 57.5 KiB, free 365.8 MiB)
	24/05/05 07:33:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 365.8 MiB)
	24/05/05 07:33:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 02dc7f71660d:43505 (size: 23.4 KiB, free: 366.2 MiB)
	24/05/05 07:33:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:33:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:33:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/05 07:33:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:33:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/05/05 07:33:21 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/userFiles-bc6b6807-e454-4a12-b0a4-80ee8cd86131/988-20240505062610.json:0+3160
	24/05/05 07:33:21 INFO CodeGenerator: Code generated in 48.200319 ms
	24/05/05 07:33:21 INFO CodeGenerator: Code generated in 32.158693 ms
	24/05/05 07:33:21 INFO CodeGenerator: Code generated in 86.499377 ms
	24/05/05 07:33:21 INFO PythonRunner: Times: total = 205, boot = -1300, init = 1504, finish = 1
	24/05/05 07:33:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 4781 bytes result sent to driver
	24/05/05 07:33:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 457 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:33:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/05 07:33:21 INFO DAGScheduler: ResultStage 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark_gnews_preprocessing.py:67) finished in 0.500 s
	24/05/05 07:33:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:33:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/05/05 07:33:21 INFO DAGScheduler: Job 2 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649/spark_gnews_preprocessing.py:67, took 0.525116 s
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:22.700 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:22 INFO CodeGenerator: Code generated in 21.887806 ms
	24/05/05 07:33:22 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/05/05 07:33:22 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:33:22 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/05/05 07:33:22 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:33:22 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:33:22 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[25] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:33:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.7 KiB, free 365.8 MiB)
	24/05/05 07:33:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/05/05 07:33:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 02dc7f71660d:43505 (size: 9.0 KiB, free: 366.2 MiB)
	24/05/05 07:33:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:33:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[25] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:33:22 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/05/05 07:33:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 12644 bytes) 
	24/05/05 07:33:22 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
	24/05/05 07:33:22 INFO CodeGenerator: Code generated in 27.43314 ms
	24/05/05 07:33:22 INFO CodeGenerator: Code generated in 20.505794 ms
	24/05/05 07:33:22 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/05/05 07:33:22 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/05/05 07:33:22 INFO AppInfoParser: Kafka version: 3.4.1
	24/05/05 07:33:22 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/05/05 07:33:22 INFO AppInfoParser: Kafka startTimeMs: 1714865602582
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:23.704 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:23 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 02dc7f71660d:43505 in memory (size: 23.4 KiB, free: 366.2 MiB)
	24/05/05 07:33:23 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 02dc7f71660d:43505 in memory (size: 13.4 KiB, free: 366.3 MiB)
	24/05/05 07:33:23 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition gnews_preprocessed-0 to 0 since the associated topicId changed from null to vBpmtWa3QRWNv4qz8CVDGw
	24/05/05 07:33:23 INFO Metadata: [Producer clientId=producer-1] Cluster ID: xO1AMuOnS8uX6jJWW9t4bg
	24/05/05 07:33:23 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 1 with epoch 0
	24/05/05 07:33:23 INFO PythonRunner: Times: total = 144, boot = -870, init = 1014, finish = 0
	24/05/05 07:33:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1664 bytes result sent to driver
	24/05/05 07:33:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1374 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:33:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/05/05 07:33:23 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 1.390 s
	24/05/05 07:33:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:33:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/05/05 07:33:23 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 1.403378 s
	24/05/05 07:33:23 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/05 07:33:23 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/05 07:33:23 INFO SparkUI: Stopped Spark web UI at http://02dc7f71660d:4040
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:24.705 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:33:23 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/05 07:33:23 INFO MemoryStore: MemoryStore cleared
	24/05/05 07:33:23 INFO BlockManager: BlockManager stopped
	24/05/05 07:33:23 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/05 07:33:23 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/05 07:33:23 INFO SparkContext: Successfully stopped SparkContext
	24/05/05 07:33:23 INFO ShutdownHookManager: Shutdown hook called
	24/05/05 07:33:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d
	24/05/05 07:33:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-c5789ed1-d7cc-47f0-813b-f1b9ef2d635d/pyspark-5104881c-0ecb-446b-91fe-c2bc9f7f6a37
	24/05/05 07:33:23 INFO ShutdownHookManager: Deleting directory /tmp/spark-9459b193-e4d7-421b-9a7e-31a90b6ab692
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.707 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649, processId:2814 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.707 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.708 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.710 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.711 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.715 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.715 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.715 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.716 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3649
[WI-991][TI-3649] - [INFO] 2024-05-05 07:33:24.717 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3649] - [INFO] 2024-05-05 07:33:25.421 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3649, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:25.537 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3650, taskName=move to archives, firstSubmitTime=1714865605530, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=991, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"processing"}], executorId=1, cmdTypeIfComplement=7, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit archive folder\nmv ${raw_file_dir} ${GNEWS_HOME}/archive\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to archives'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3650'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524228'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073325'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='processing'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='991'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/988-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.538 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to archives to wait queue success
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.539 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.540 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.541 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865605541
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 991_3650
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.541 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3650,
  "taskName" : "move to archives",
  "firstSubmitTime" : 1714865605530,
  "startTime" : 1714865605541,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/991/3650.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 991,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"processing\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 7,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit archive folder\\nmv ${raw_file_dir} ${GNEWS_HOME}/archive\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to archives"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3650"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524228"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073325"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "processing"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "991"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/988-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "991_3650",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/988-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.542 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.545 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.546 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3650 check successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.547 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.547 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.548 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.548 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.548 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit archive folder\nmv ${raw_file_dir} ${GNEWS_HOME}/archive\n",
  "resourceList" : [ ]
}
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.548 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.551 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/988-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.551 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.551 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.551 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.551 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.551 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.552 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit archive folder
mv /local_storage/google_news/processing/988-20240505062610.json /local_storage/google_news/archive

[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.552 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.552 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3650/991_3650.sh
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:25.558 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3040
[WI-0][TI-3650] - [INFO] 2024-05-05 07:33:26.421 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3650, success=true)
[WI-0][TI-3650] - [INFO] 2024-05-05 07:33:26.429 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3650)
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:26.560 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.567 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3650, processId:3040 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.568 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.570 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.573 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3650
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/991/3650
[WI-991][TI-3650] - [INFO] 2024-05-05 07:33:26.574 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3650] - [INFO] 2024-05-05 07:33:27.413 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3650, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:33.355 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7657142857142857 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:33:59.440 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7795690403411449 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:37:22.138 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7130681818181819 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:38:56.396 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3651, taskName=search for intake article JSON files, firstSubmitTime=1714865936388, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=992, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""},{"prop":"is_exists","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"","resourceList":[]}, environmentConfig=null, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='search for intake article JSON files'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3651'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524226'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073856'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='992'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=null, dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.397 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: search for intake article JSON files to wait queue success
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.400 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.401 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865936401
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 992_3651
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3651,
  "taskName" : "search for intake article JSON files",
  "firstSubmitTime" : 1714865936388,
  "startTime" : 1714865936401,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/992/3651.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 992,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"},{\"prop\":\"is_exists\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# find 1 raw file in the folder\\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\\n\\n# check if raw_file_dir is empty, then set does_file_exist to 1 \\nif [ -z \\\"$raw_file_dir\\\" ]; then\\n    raw_file_dir=null\\n    is_exists=0\\nelse\\n    is_exists=1\\nfi\\n\\necho \\\"#{setValue(raw_file_dir=${raw_file_dir})}\\\"\\necho \\\"#{setValue(is_exists=${is_exists})}\\\"\",\"resourceList\":[]}",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "search for intake article JSON files"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3651"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524226"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073856"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "992"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    }
  },
  "taskAppId" : "992_3651",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.405 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.406 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.406 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3651 check successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  }, {
    "prop" : "is_exists",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# find 1 raw file in the folder\nraw_file_dir=$(find ${GNEWS_HOME}/${folder} -type f -name '*.json'| head -n 1)\n\n# check if raw_file_dir is empty, then set does_file_exist to 1 \nif [ -z \"$raw_file_dir\" ]; then\n    raw_file_dir=null\n    is_exists=0\nelse\n    is_exists=1\nfi\n\necho \"#{setValue(raw_file_dir=${raw_file_dir})}\"\necho \"#{setValue(is_exists=${is_exists})}\"",
  "resourceList" : [ ]
}
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.407 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: null successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.408 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.411 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.411 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.411 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
# find 1 raw file in the folder
raw_file_dir=$(find /local_storage/google_news/in -type f -name '*.json'| head -n 1)

# check if raw_file_dir is empty, then set does_file_exist to 1 
if [ -z "$raw_file_dir" ]; then
    raw_file_dir=null
    is_exists=0
else
    is_exists=1
fi

echo "#{setValue(raw_file_dir=${raw_file_dir})}"
echo "#{setValue(is_exists=${is_exists})}"
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.411 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.411 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3651/992_3651.sh
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:56.415 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3119
[WI-0][TI-3651] - [INFO] 2024-05-05 07:38:57.281 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3651, success=true)
[WI-0][TI-3651] - [INFO] 2024-05-05 07:38:57.288 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3651)
[WI-0][TI-0] - [INFO] 2024-05-05 07:38:57.416 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/google_news/in/987-20240505062610.json)}
	#{setValue(is_exists=1)}
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.437 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3651, processId:3119 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.452 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.452 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.452 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.453 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.457 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3651
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3651
[WI-992][TI-3651] - [INFO] 2024-05-05 07:38:57.458 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3651] - [INFO] 2024-05-05 07:38:58.278 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3651, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:38:59.374 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3653, taskName=move to processing, firstSubmitTime=1714865939364, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=992, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to processing'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3653'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524227'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073859'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='992'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/in/987-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/987-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.377 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to processing to wait queue success
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.377 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.379 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.380 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.380 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.380 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865939380
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.380 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 992_3653
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.380 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3653,
  "taskName" : "move to processing",
  "firstSubmitTime" : 1714865939364,
  "startTime" : 1714865939380,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/992/3653.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 992,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the google news processing folder\\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\\nfilename=$(basename \\\"${raw_file_dir}\\\")\\nif ! grep -q \\\"${GNEWS_HOME}/processing\\\" <<< \\\"${raw_file_dir}\\\"; then\\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\\nelse\\n    echo JSON is already in processing\\nfi\\n\\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\\necho \\\"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\\\"\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to processing"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3653"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524227"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073859"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "992"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/in/987-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "992_3653",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/in/987-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.381 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.381 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.381 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.390 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3653 check successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.394 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.395 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.397 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.398 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the google news processing folder\n# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error\nfilename=$(basename \"${raw_file_dir}\")\nif ! grep -q \"${GNEWS_HOME}/processing\" <<< \"${raw_file_dir}\"; then\n    mv ${raw_file_dir} ${GNEWS_HOME}/processing\nelse\n    echo JSON is already in processing\nfi\n\n# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing\necho \"#{setValue(raw_file_dir=${GNEWS_HOME}/processing/${filename})}\"",
  "resourceList" : [ ]
}
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/in/987-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.398 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.399 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.406 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.407 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.407 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.407 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the google news processing folder
# but dont move it, if it already exists in the folder, for example when the selected folder to search is google_news/processing, the file may already be inside the processing folder, which will lead to an error
filename=$(basename "/local_storage/google_news/in/987-20240505062610.json")
if ! grep -q "/local_storage/google_news/processing" <<< "/local_storage/google_news/in/987-20240505062610.json"; then
    mv /local_storage/google_news/in/987-20240505062610.json /local_storage/google_news/processing
else
    echo JSON is already in processing
fi

# update the raw_file_dir parameters in the scenario that the raw file gets moved to processing
echo "#{setValue(raw_file_dir=/local_storage/google_news/processing/${filename})}"
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.407 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.407 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3653/992_3653.sh
[WI-992][TI-3653] - [INFO] 2024-05-05 07:38:59.431 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3133
[WI-0][TI-3653] - [INFO] 2024-05-05 07:39:00.284 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3653, success=true)
[WI-0][TI-3653] - [INFO] 2024-05-05 07:39:00.293 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3653)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:00.441 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
	#{setValue(raw_file_dir=/local_storage/google_news/processing/987-20240505062610.json)}
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.444 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3653, processId:3133 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.445 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.446 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.447 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.449 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.450 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3653
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3653
[WI-992][TI-3653] - [INFO] 2024-05-05 07:39:00.452 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3653] - [INFO] 2024-05-05 07:39:01.282 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3653, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:01.388 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3654, taskName=preprocessing and kafka, firstSubmitTime=1714865941380, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=992, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[],"rawScript":"$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n","resourceList":[{"id":null,"resourceName":"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py","res":null}]}, environmentConfig=export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='preprocessing and kafka'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3654'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524229'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073901'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='992'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/987-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/987-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.389 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: preprocessing and kafka to wait queue success
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.389 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.390 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.391 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865941391
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 992_3654
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.391 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3654,
  "taskName" : "preprocessing and kafka",
  "firstSubmitTime" : 1714865941380,
  "startTime" : 1714865941391,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/992/3654.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 992,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"$SPARK_HOME/bin/spark-submit \\\\\\n    --master local \\\\\\n    --deploy-mode client \\\\\\n    --conf spark.driver.cores=1 \\\\\\n    --conf spark.driver.memory=1G \\\\\\n    --conf spark.executor.instances=1 \\\\\\n    --conf spark.executor.cores=1 \\\\\\n    --conf spark.executor.memory=1G \\\\\\n    --name gnews_preprocessing \\\\\\n    --files ${raw_file_dir} \\\\\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\\\\n    --conf spark.driver.extraJavaOptions=\\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\" \\\\\\n    spark_gnews_preprocessing.py\\n\",\"resourceList\":[{\"id\":null,\"resourceName\":\"file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py\",\"res\":null}]}",
  "environmentConfig" : "export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYSPARK_PYTHON=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocessing and kafka"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3654"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524229"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073901"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "992"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/987-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "992_3654",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/987-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.396 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.396 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.396 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.398 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654 check successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.399 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py=ResourceContext.ResourceItem(resourceAbsolutePathInStorage=file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py, resourceRelativePath=spark_gnews_preprocessing.py, resourceAbsolutePathInLocal=/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark_gnews_preprocessing.py)})
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.401 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "$SPARK_HOME/bin/spark-submit \\\n    --master local \\\n    --deploy-mode client \\\n    --conf spark.driver.cores=1 \\\n    --conf spark.driver.memory=1G \\\n    --conf spark.executor.instances=1 \\\n    --conf spark.executor.cores=1 \\\n    --conf spark.executor.memory=1G \\\n    --name gnews_preprocessing \\\n    --files ${raw_file_dir} \\\n    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \\\n    --conf spark.driver.extraJavaOptions=\"-Divy.cache.dir=/tmp -Divy.home=/tmp\" \\\n    spark_gnews_preprocessing.py\n",
  "resourceList" : [ {
    "id" : null,
    "resourceName" : "file:/dolphinscheduler/default/resources/spark_gnews_preprocessing.py",
    "res" : null
  } ]
}
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.401 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.402 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/987-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.402 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.402 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.403 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYSPARK_PYTHON=/bin/python3.11
$SPARK_HOME/bin/spark-submit \
    --master local \
    --deploy-mode client \
    --conf spark.driver.cores=1 \
    --conf spark.driver.memory=1G \
    --conf spark.executor.instances=1 \
    --conf spark.executor.cores=1 \
    --conf spark.executor.memory=1G \
    --name gnews_preprocessing \
    --files /local_storage/google_news/processing/987-20240505062610.json \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1 \
    --conf spark.driver.extraJavaOptions="-Divy.cache.dir=/tmp -Divy.home=/tmp" \
    spark_gnews_preprocessing.py

[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.403 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.403 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/992_3654.sh
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:01.406 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3152
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.085 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3654, processInstanceId=992, startTime=1714865941391, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.12:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13475238524230/33/992/3654.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.088 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionRunningEvent(taskInstanceId=3654, processInstanceId=992, startTime=1714865941391, taskInstanceHost=172.18.1.1:1234, workflowInstanceHost=172.18.0.12:5678, status=TaskExecutionStatus{code=1, desc='running'}, logPath=/opt/dolphinscheduler/logs/20240505/13475238524230/33/992/3654.log, executePath=null, processId=0, appIds=null, eventCreateTime=0, eventSendTime=1714865942084)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.089 +0800 o.a.d.s.w.m.MessageRetryRunner:[132] - Begin retry send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3654, processInstanceId=992, startTime=1714865941391, workflowInstanceHost=172.18.0.12:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=0)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.090 +0800 o.a.d.s.w.m.MessageRetryRunner:[135] - Success send message to master, event: TaskInstanceExecutionInfoEvent(taskInstanceId=3654, processInstanceId=992, startTime=1714865941391, workflowInstanceHost=172.18.0.12:5678, taskInstanceHost=172.18.1.1:1234, logPath=null, processId=0, eventCreateTime=0, eventSendTime=1714865942084)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.299 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3654, success=true)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.305 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3654)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.320 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3654, success=true)
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:02.344 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3654)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:02.408 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:04.445 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-173ccbc6-5f11-423b-99a5-52f83ec04a49;1.0
		confs: [default]
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
		found org.slf4j#slf4j-api;2.0.7 in central
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:04.679 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8671723005836576 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:05.467 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
		found org.apache.commons#commons-pool2;2.11.1 in central
	:: resolution report :: resolve 877ms :: artifacts dl 98ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-173ccbc6-5f11-423b-99a5-52f83ec04a49
		confs: [default]
		0 artifacts copied, 11 already retrieved (0kB/11ms)
	24/05/05 07:39:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:05.698 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8000107851596203 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:06.472 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:06 INFO SparkContext: Running Spark version 3.5.1
	24/05/05 07:39:06 INFO SparkContext: OS info Linux, 6.5.0-28-generic, amd64
	24/05/05 07:39:06 INFO SparkContext: Java version 1.8.0_402
	24/05/05 07:39:06 INFO ResourceUtils: ==============================================================
	24/05/05 07:39:06 INFO ResourceUtils: No custom resources configured for spark.driver.
	24/05/05 07:39:06 INFO ResourceUtils: ==============================================================
	24/05/05 07:39:06 INFO SparkContext: Submitted application: gnews_preprocessing
	24/05/05 07:39:06 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
	24/05/05 07:39:06 INFO ResourceProfile: Limiting resource is cpus at 1 tasks per executor
	24/05/05 07:39:06 INFO ResourceProfileManager: Added ResourceProfile id: 0
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:06.701 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7522388059701492 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:07.473 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:06 INFO SecurityManager: Changing view acls to: default
	24/05/05 07:39:06 INFO SecurityManager: Changing modify acls to: default
	24/05/05 07:39:06 INFO SecurityManager: Changing view acls groups to: 
	24/05/05 07:39:06 INFO SecurityManager: Changing modify acls groups to: 
	24/05/05 07:39:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: default; groups with view permissions: EMPTY; users with modify permissions: default; groups with modify permissions: EMPTY
	24/05/05 07:39:06 INFO Utils: Successfully started service 'sparkDriver' on port 35589.
	24/05/05 07:39:06 INFO SparkEnv: Registering MapOutputTracker
	24/05/05 07:39:07 INFO SparkEnv: Registering BlockManagerMaster
	24/05/05 07:39:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
	24/05/05 07:39:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
	24/05/05 07:39:07 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
	24/05/05 07:39:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-bc8ca97c-22dc-4e36-8c76-23cd870bed34
	24/05/05 07:39:07 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
	24/05/05 07:39:07 INFO SparkEnv: Registering OutputCommitCoordinator
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:07.710 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8280254777070064 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:08.477 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:07 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
	24/05/05 07:39:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at spark://02dc7f71660d:35589/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at spark://02dc7f71660d:35589/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://02dc7f71660d:35589/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://02dc7f71660d:35589/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://02dc7f71660d:35589/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://02dc7f71660d:35589/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at spark://02dc7f71660d:35589/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://02dc7f71660d:35589/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://02dc7f71660d:35589/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://02dc7f71660d:35589/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added JAR file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at spark://02dc7f71660d:35589/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO SparkContext: Added file file:///local_storage/google_news/processing/987-20240505062610.json at file:///local_storage/google_news/processing/987-20240505062610.json with timestamp 1714865946215
	24/05/05 07:39:07 INFO Utils: Copying /local_storage/google_news/processing/987-20240505062610.json to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/987-20240505062610.json
	24/05/05 07:39:07 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:39:07 INFO SparkContext: Added file file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar at file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO Utils: Copying /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:39:07 INFO SparkContext: Added file file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865946215
	24/05/05 07:39:07 INFO Utils: Copying /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar at file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:39:08 INFO SparkContext: Added file file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar at file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Copying /tmp/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:39:08 INFO Executor: Starting executor ID driver on host 02dc7f71660d
	24/05/05 07:39:08 INFO Executor: OS info Linux, 6.5.0-28-generic, amd64
	24/05/05 07:39:08 INFO Executor: Java version 1.8.0_402
	24/05/05 07:39:08 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
	24/05/05 07:39:08 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@aef7c01 for default.
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865946215
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:09.481 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///local_storage/google_news/processing/987-20240505062610.json with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /local_storage/google_news/processing/987-20240505062610.json has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/987-20240505062610.json
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:39:08 INFO Executor: Fetching file:///tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: /tmp/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO TransportClientFactory: Successfully created connection to 02dc7f71660d/172.18.1.1:35589 after 47 ms (0 ms spent in bootstraps)
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp8450785425512384000.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp8450785425512384000.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp5103265354139214561.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp5103265354139214561.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/commons-logging_commons-logging-1.1.3.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/commons-logging_commons-logging-1.1.3.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp5144156750257504468.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp5144156750257504468.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.kafka_kafka-clients-3.4.1.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp9045966320236268688.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp9045966320236268688.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/com.google.code.findbugs_jsr305-3.0.0.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp2941337309730080124.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp2941337309730080124.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.commons_commons-pool2-2.11.1.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp2507012535663936836.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp2507012535663936836.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.slf4j_slf4j-api-2.0.7.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.slf4j_slf4j-api-2.0.7.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp7871621984408701758.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp7871621984408701758.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.1.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp3942013230879196469.tmp
	24/05/05 07:39:08 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp3942013230879196469.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.lz4_lz4-java-1.8.0.jar
	24/05/05 07:39:08 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.lz4_lz4-java-1.8.0.jar to class loader default
	24/05/05 07:39:08 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:08 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp7155146301603143090.tmp
	24/05/05 07:39:09 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp7155146301603143090.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
	24/05/05 07:39:09 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
	24/05/05 07:39:09 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1714865946215
	24/05/05 07:39:09 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp6905994301257861919.tmp
	24/05/05 07:39:09 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp6905994301257861919.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.xerial.snappy_snappy-java-1.1.10.3.jar
	24/05/05 07:39:09 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
	24/05/05 07:39:09 INFO Executor: Fetching spark://02dc7f71660d:35589/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1714865946215
	24/05/05 07:39:09 INFO Utils: Fetching spark://02dc7f71660d:35589/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp8030451533860938450.tmp
	24/05/05 07:39:09 INFO Utils: /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/fetchFileTemp8030451533860938450.tmp has been previously copied to /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-api-3.3.4.jar
	24/05/05 07:39:09 INFO Executor: Adding file:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
	24/05/05 07:39:09 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40955.
	24/05/05 07:39:09 INFO NettyBlockTransferService: Server created on 02dc7f71660d:40955
	24/05/05 07:39:09 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
	24/05/05 07:39:09 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 02dc7f71660d, 40955, None)
	24/05/05 07:39:09 INFO BlockManagerMasterEndpoint: Registering block manager 02dc7f71660d:40955 with 366.3 MiB RAM, BlockManagerId(driver, 02dc7f71660d, 40955, None)
	24/05/05 07:39:09 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 02dc7f71660d, 40955, None)
	24/05/05 07:39:09 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 02dc7f71660d, 40955, None)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:11.485 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:10 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 345.7 KiB, free 366.0 MiB)
	24/05/05 07:39:10 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 32.6 KiB, free 365.9 MiB)
	24/05/05 07:39:10 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 02dc7f71660d:40955 (size: 32.6 KiB, free: 366.3 MiB)
	24/05/05 07:39:10 INFO SparkContext: Created broadcast 0 from wholeTextFiles at NativeMethodAccessorImpl.java:0
	24/05/05 07:39:10 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
	24/05/05 07:39:10 INFO SharedState: Warehouse path is 'file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark-warehouse'.
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:13.756 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8287292817679558 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:14.774 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8474576271186441 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:15.776 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8491620111731844 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:17.498 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:16 INFO CodeGenerator: Code generated in 472.979791 ms
	24/05/05 07:39:16 INFO FileInputFormat: Total input files to process : 1
	24/05/05 07:39:16 INFO FileInputFormat: Total input files to process : 1
	24/05/05 07:39:16 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0
	24/05/05 07:39:17 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:39:17 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)
	24/05/05 07:39:17 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:39:17 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:39:17 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:39:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 28.2 KiB, free 365.9 MiB)
	24/05/05 07:39:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 12.5 KiB, free 365.9 MiB)
	24/05/05 07:39:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 02dc7f71660d:40955 (size: 12.5 KiB, free: 366.3 MiB)
	24/05/05 07:39:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:39:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[7] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:39:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
	24/05/05 07:39:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:39:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:18.501 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:17 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/987-20240505062610.json:0+4304
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:19.508 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:18 INFO CodeGenerator: Code generated in 98.021634 ms
	24/05/05 07:39:18 INFO PythonRunner: Times: total = 718, boot = 588, init = 128, finish = 2
	24/05/05 07:39:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2761 bytes result sent to driver
	24/05/05 07:39:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1318 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:39:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
	24/05/05 07:39:18 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 56929
	24/05/05 07:39:18 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 1.660 s
	24/05/05 07:39:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:39:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
	24/05/05 07:39:18 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 1.770546 s
	24/05/05 07:39:19 INFO CodeGenerator: Code generated in 36.859472 ms
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:20.519 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:19 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
	24/05/05 07:39:19 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:39:19 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
	24/05/05 07:39:19 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:39:19 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:39:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:39:19 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 30.5 KiB, free 365.9 MiB)
	24/05/05 07:39:19 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 13.4 KiB, free 365.8 MiB)
	24/05/05 07:39:19 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 02dc7f71660d:40955 (size: 13.4 KiB, free: 366.2 MiB)
	24/05/05 07:39:19 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:39:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:39:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
	24/05/05 07:39:19 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:39:19 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
	24/05/05 07:39:19 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/987-20240505062610.json:0+4304
	24/05/05 07:39:19 INFO CodeGenerator: Code generated in 42.053571 ms
	24/05/05 07:39:20 INFO PythonRunner: Times: total = 201, boot = -1060, init = 1261, finish = 0
	24/05/05 07:39:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 5157 bytes result sent to driver
	24/05/05 07:39:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 706 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:39:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
	24/05/05 07:39:20 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.731 s
	24/05/05 07:39:20 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:39:20 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
	24/05/05 07:39:20 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.750079 s
	24/05/05 07:39:20 INFO CodeGenerator: Code generated in 21.325589 ms
	+-----------+--------------------+--------------------+--------------------+-------+-----------------+
	|    authors|             content|                 url|             context|   type|         datetime|
	+-----------+--------------------+--------------------+--------------------+-------+-----------------+
	|[Erin Hale]|Company-wide memo...|https://news.goog...|Wall Street Journ...|article|24-05-03 13:45:22|
	+-----------+--------------------+--------------------+--------------------+-------+-----------------+
	
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:20.814 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8881789137380192 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:21.520 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:20 INFO CodeGenerator: Code generated in 53.195813 ms
	24/05/05 07:39:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 02dc7f71660d:40955 in memory (size: 12.5 KiB, free: 366.3 MiB)
	24/05/05 07:39:21 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 02dc7f71660d:40955 in memory (size: 13.4 KiB, free: 366.3 MiB)
	24/05/05 07:39:21 INFO SparkContext: Starting job: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark_gnews_preprocessing.py:67
	24/05/05 07:39:21 INFO DAGScheduler: Got job 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark_gnews_preprocessing.py:67) with 1 output partitions
	24/05/05 07:39:21 INFO DAGScheduler: Final stage: ResultStage 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark_gnews_preprocessing.py:67)
	24/05/05 07:39:21 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:39:21 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:39:21 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at toJavaRDD at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:39:21 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 57.5 KiB, free 365.9 MiB)
	24/05/05 07:39:21 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.4 KiB, free 365.9 MiB)
	24/05/05 07:39:21 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 02dc7f71660d:40955 (size: 23.4 KiB, free: 366.2 MiB)
	24/05/05 07:39:21 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:39:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at toJavaRDD at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:39:21 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
	24/05/05 07:39:21 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 9890 bytes) 
	24/05/05 07:39:21 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
	24/05/05 07:39:21 INFO WholeTextFileRDD: Input split: Paths:/tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/userFiles-a551f484-7ca8-4362-ad97-28d5e5a46fd3/987-20240505062610.json:0+4304
	24/05/05 07:39:21 INFO CodeGenerator: Code generated in 12.846682 ms
	24/05/05 07:39:21 INFO CodeGenerator: Code generated in 16.408956 ms
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:21.948 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.9015151515151515 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:22.521 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:21 INFO CodeGenerator: Code generated in 107.363107 ms
	24/05/05 07:39:21 INFO PythonRunner: Times: total = 71, boot = -1474, init = 1545, finish = 0
	24/05/05 07:39:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 5805 bytes result sent to driver
	24/05/05 07:39:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 380 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:39:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
	24/05/05 07:39:21 INFO DAGScheduler: ResultStage 2 (collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark_gnews_preprocessing.py:67) finished in 0.467 s
	24/05/05 07:39:21 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:39:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
	24/05/05 07:39:21 INFO DAGScheduler: Job 2 finished: collect at /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654/spark_gnews_preprocessing.py:67, took 0.479724 s
	24/05/05 07:39:22 INFO CodeGenerator: Code generated in 17.641777 ms
	24/05/05 07:39:22 INFO SparkContext: Starting job: save at NativeMethodAccessorImpl.java:0
	24/05/05 07:39:22 INFO DAGScheduler: Got job 3 (save at NativeMethodAccessorImpl.java:0) with 1 output partitions
	24/05/05 07:39:22 INFO DAGScheduler: Final stage: ResultStage 3 (save at NativeMethodAccessorImpl.java:0)
	24/05/05 07:39:22 INFO DAGScheduler: Parents of final stage: List()
	24/05/05 07:39:22 INFO DAGScheduler: Missing parents: List()
	24/05/05 07:39:22 INFO DAGScheduler: Submitting ResultStage 3 (SQLExecutionRDD[25] at save at NativeMethodAccessorImpl.java:0), which has no missing parents
	24/05/05 07:39:22 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 20.7 KiB, free 365.8 MiB)
	24/05/05 07:39:22 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 9.0 KiB, free 365.8 MiB)
	24/05/05 07:39:22 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 02dc7f71660d:40955 (size: 9.0 KiB, free: 366.2 MiB)
	24/05/05 07:39:22 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585
	24/05/05 07:39:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (SQLExecutionRDD[25] at save at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
	24/05/05 07:39:22 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
	24/05/05 07:39:22 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3) (02dc7f71660d, executor driver, partition 0, PROCESS_LOCAL, 13706 bytes) 
	24/05/05 07:39:22 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:22.952 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8719723183391003 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:23.526 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:22 INFO CodeGenerator: Code generated in 129.968595 ms
	24/05/05 07:39:22 INFO CodeGenerator: Code generated in 48.106804 ms
	24/05/05 07:39:22 INFO ProducerConfig: ProducerConfig values: 
		acks = -1
		auto.include.jmx.reporter = true
		batch.size = 16384
		bootstrap.servers = [kafka:9092]
		buffer.memory = 33554432
		client.dns.lookup = use_all_dns_ips
		client.id = producer-1
		compression.type = none
		connections.max.idle.ms = 540000
		delivery.timeout.ms = 120000
		enable.idempotence = true
		interceptor.classes = []
		key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
		linger.ms = 0
		max.block.ms = 60000
		max.in.flight.requests.per.connection = 5
		max.request.size = 1048576
		metadata.max.age.ms = 300000
		metadata.max.idle.ms = 300000
		metric.reporters = []
		metrics.num.samples = 2
		metrics.recording.level = INFO
		metrics.sample.window.ms = 30000
		partitioner.adaptive.partitioning.enable = true
		partitioner.availability.timeout.ms = 0
		partitioner.class = null
		partitioner.ignore.keys = false
		receive.buffer.bytes = 32768
		reconnect.backoff.max.ms = 1000
		reconnect.backoff.ms = 50
		request.timeout.ms = 30000
		retries = 2147483647
		retry.backoff.ms = 100
		sasl.client.callback.handler.class = null
		sasl.jaas.config = null
		sasl.kerberos.kinit.cmd = /usr/bin/kinit
		sasl.kerberos.min.time.before.relogin = 60000
		sasl.kerberos.service.name = null
		sasl.kerberos.ticket.renew.jitter = 0.05
		sasl.kerberos.ticket.renew.window.factor = 0.8
		sasl.login.callback.handler.class = null
		sasl.login.class = null
		sasl.login.connect.timeout.ms = null
		sasl.login.read.timeout.ms = null
		sasl.login.refresh.buffer.seconds = 300
		sasl.login.refresh.min.period.seconds = 60
		sasl.login.refresh.window.factor = 0.8
		sasl.login.refresh.window.jitter = 0.05
		sasl.login.retry.backoff.max.ms = 10000
		sasl.login.retry.backoff.ms = 100
		sasl.mechanism = GSSAPI
		sasl.oauthbearer.clock.skew.seconds = 30
		sasl.oauthbearer.expected.audience = null
		sasl.oauthbearer.expected.issuer = null
		sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
		sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
		sasl.oauthbearer.jwks.endpoint.url = null
		sasl.oauthbearer.scope.claim.name = scope
		sasl.oauthbearer.sub.claim.name = sub
		sasl.oauthbearer.token.endpoint.url = null
		security.protocol = PLAINTEXT
		security.providers = null
		send.buffer.bytes = 131072
		socket.connection.setup.timeout.max.ms = 30000
		socket.connection.setup.timeout.ms = 10000
		ssl.cipher.suites = null
		ssl.enabled.protocols = [TLSv1.2]
		ssl.endpoint.identification.algorithm = https
		ssl.engine.factory.class = null
		ssl.key.password = null
		ssl.keymanager.algorithm = SunX509
		ssl.keystore.certificate.chain = null
		ssl.keystore.key = null
		ssl.keystore.location = null
		ssl.keystore.password = null
		ssl.keystore.type = JKS
		ssl.protocol = TLSv1.2
		ssl.provider = null
		ssl.secure.random.implementation = null
		ssl.trustmanager.algorithm = PKIX
		ssl.truststore.certificates = null
		ssl.truststore.location = null
		ssl.truststore.password = null
		ssl.truststore.type = JKS
		transaction.timeout.ms = 60000
		transactional.id = null
		value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	
	24/05/05 07:39:22 INFO KafkaProducer: [Producer clientId=producer-1] Instantiated an idempotent producer.
	24/05/05 07:39:22 INFO AppInfoParser: Kafka version: 3.4.1
	24/05/05 07:39:22 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
	24/05/05 07:39:22 INFO AppInfoParser: Kafka startTimeMs: 1714865962971
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:23.954 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7364620938628159 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:24.529 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	24/05/05 07:39:23 INFO Metadata: [Producer clientId=producer-1] Resetting the last seen epoch of partition gnews_preprocessed-0 to 0 since the associated topicId changed from null to vBpmtWa3QRWNv4qz8CVDGw
	24/05/05 07:39:23 INFO Metadata: [Producer clientId=producer-1] Cluster ID: xO1AMuOnS8uX6jJWW9t4bg
	24/05/05 07:39:23 INFO TransactionManager: [Producer clientId=producer-1] ProducerId set to 2 with epoch 0
	24/05/05 07:39:23 INFO PythonRunner: Times: total = 276, boot = -881, init = 1157, finish = 0
	24/05/05 07:39:23 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1621 bytes result sent to driver
	24/05/05 07:39:23 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 1719 ms on 02dc7f71660d (executor driver) (1/1)
	24/05/05 07:39:23 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
	24/05/05 07:39:23 INFO DAGScheduler: ResultStage 3 (save at NativeMethodAccessorImpl.java:0) finished in 1.744 s
	24/05/05 07:39:23 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
	24/05/05 07:39:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
	24/05/05 07:39:23 INFO DAGScheduler: Job 3 finished: save at NativeMethodAccessorImpl.java:0, took 1.827858 s
	24/05/05 07:39:24 INFO SparkContext: Invoking stop() from shutdown hook
	24/05/05 07:39:24 INFO SparkContext: SparkContext is stopping with exitCode 0.
	24/05/05 07:39:24 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 02dc7f71660d:40955 in memory (size: 23.4 KiB, free: 366.3 MiB)
	24/05/05 07:39:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 02dc7f71660d:40955 in memory (size: 9.0 KiB, free: 366.3 MiB)
	24/05/05 07:39:24 INFO SparkUI: Stopped Spark web UI at http://02dc7f71660d:4040
	24/05/05 07:39:24 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
	24/05/05 07:39:24 INFO MemoryStore: MemoryStore cleared
	24/05/05 07:39:24 INFO BlockManager: BlockManager stopped
	24/05/05 07:39:24 INFO BlockManagerMaster: BlockManagerMaster stopped
	24/05/05 07:39:24 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
	24/05/05 07:39:24 INFO SparkContext: Successfully stopped SparkContext
	24/05/05 07:39:24 INFO ShutdownHookManager: Shutdown hook called
	24/05/05 07:39:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe/pyspark-086967d4-1ff5-4253-806f-a2031502848a
	24/05/05 07:39:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-4f7ae4de-335d-48e2-9eb4-7629ab28cffe
	24/05/05 07:39:24 INFO ShutdownHookManager: Deleting directory /tmp/spark-3accf916-fb93-4913-9a80-ed6fe1d7bc44
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.533 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654, processId:3152 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.534 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.534 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.534 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.534 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.536 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3654
[WI-992][TI-3654] - [INFO] 2024-05-05 07:39:25.537 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:25.998 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7084468664850136 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-3654] - [INFO] 2024-05-05 07:39:26.334 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3654, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:26.359 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[53] - Receive TaskInstanceDispatchRequest: TaskInstanceDispatchRequest(taskExecutionContext=TaskExecutionContext(taskInstanceId=3655, taskName=move to archives, firstSubmitTime=1714865966352, startTime=0, taskType=SHELL, workflowInstanceHost=172.18.0.12:5678, host=172.18.1.1:1234, executePath=null, logPath=null, appInfoPath=null, taskJson=null, processId=0, processDefineCode=13475238524230, processDefineVersion=33, appIds=null, processInstanceId=992, scheduleTime=0, globalParams=[{"prop":"folder","direct":"IN","type":"VARCHAR","value":"in"}], executorId=1, cmdTypeIfComplement=0, tenantCode=default, processDefineId=0, projectId=0, projectCode=13010050770016, taskParams={"localParams":[{"prop":"raw_file_dir","direct":"OUT","type":"VARCHAR","value":""}],"rawScript":"# move file to the reddit archive folder\nmv ${raw_file_dir} ${GNEWS_HOME}/archive\n","resourceList":[]}, environmentConfig=export PYTHON_LAUNCHER=/bin/python3.11, definedParams=null, prepareParamsMap={GNEWS_HOME=Property{prop='GNEWS_HOME', direct=IN, type=VARCHAR, value='/local_storage/google_news'}, system.task.definition.name=Property{prop='system.task.definition.name', direct=IN, type=VARCHAR, value='move to archives'}, system.project.name=Property{prop='system.project.name', direct=IN, type=VARCHAR, value='null'}, REDDIT_HOME=Property{prop='REDDIT_HOME', direct=IN, type=VARCHAR, value='/local_storage/reddit'}, system.biz.curdate=Property{prop='system.biz.curdate', direct=IN, type=VARCHAR, value='20240505'}, system.task.instance.id=Property{prop='system.task.instance.id', direct=IN, type=VARCHAR, value='3655'}, system.task.definition.code=Property{prop='system.task.definition.code', direct=IN, type=VARCHAR, value='13475238524228'}, system.datetime=Property{prop='system.datetime', direct=IN, type=VARCHAR, value='20240505073926'}, folder=Property{prop='folder', direct=IN, type=VARCHAR, value='in'}, system.project.code=Property{prop='system.project.code', direct=IN, type=VARCHAR, value='13010050770016'}, system.workflow.instance.id=Property{prop='system.workflow.instance.id', direct=IN, type=VARCHAR, value='992'}, raw_file_dir=Property{prop='raw_file_dir', direct=IN, type=VARCHAR, value='/local_storage/google_news/processing/987-20240505062610.json'}, system.biz.date=Property{prop='system.biz.date', direct=IN, type=VARCHAR, value='20240504'}, system.workflow.definition.name=Property{prop='system.workflow.definition.name', direct=IN, type=VARCHAR, value='preprocess_gnews_article'}, system.workflow.definition.code=Property{prop='system.workflow.definition.code', direct=IN, type=VARCHAR, value='13475238524230'}, is_exists=Property{prop='is_exists', direct=IN, type=VARCHAR, value='1'}}, taskAppId=null, taskTimeoutStrategy=null, taskTimeout=2147483647, workerGroup=default, delayTime=0, currentExecutionStatus=TaskExecutionStatus{code=0, desc='submit success'}, resourceParametersHelper=null, endTime=0, sqlTaskExecutionContext=null, k8sTaskExecutionContext=null, resourceContext=null, varPool=[{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/987-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}], dryRun=0, paramsMap={}, dataQualityTaskExecutionContext=null, cpuQuota=-1, memoryMax=-1, testFlag=0, logBufferEnable=false, dispatchFailTimes=0))
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.361 +0800 o.a.d.s.w.r.o.TaskInstanceDispatchOperationFunction:[79] - Submit task: move to archives to wait queue success
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.361 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.362 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Initialize task context  ***********************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.362 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[191] - Begin to initialize task
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[195] - Set task startTime: 1714865966362
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[200] - Set task appId: 992_3655
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.362 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[202] - End initialize task {
  "taskInstanceId" : 3655,
  "taskName" : "move to archives",
  "firstSubmitTime" : 1714865966352,
  "startTime" : 1714865966362,
  "taskType" : "SHELL",
  "workflowInstanceHost" : "172.18.0.12:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240505/13475238524230/33/992/3655.log",
  "processId" : 0,
  "processDefineCode" : 13475238524230,
  "processDefineVersion" : 33,
  "processInstanceId" : 992,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"folder\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"in\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[{\"prop\":\"raw_file_dir\",\"direct\":\"OUT\",\"type\":\"VARCHAR\",\"value\":\"\"}],\"rawScript\":\"# move file to the reddit archive folder\\nmv ${raw_file_dir} ${GNEWS_HOME}/archive\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "GNEWS_HOME" : {
      "prop" : "GNEWS_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news"
    },
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "move to archives"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "3655"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524228"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240505073926"
    },
    "folder" : {
      "prop" : "folder",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "in"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "992"
    },
    "raw_file_dir" : {
      "prop" : "raw_file_dir",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/google_news/processing/987-20240505062610.json"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240504"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "preprocess_gnews_article"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13475238524230"
    },
    "is_exists" : {
      "prop" : "is_exists",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1"
    }
  },
  "taskAppId" : "992_3655",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "varPool" : "[{\"prop\":\"raw_file_dir\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"/local_storage/google_news/processing/987-20240505062610.json\"},{\"prop\":\"is_exists\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"1\"}]",
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.363 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.363 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Load task instance plugin  *********************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.363 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[209] - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.366 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[217] - TenantCode: default check successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[220] - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3655 check successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[227] - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.shell.ShellTaskChannel successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[232] - Download resources successfully: 
ResourceContext(resourceItemMap={})
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[235] - Download upstream files: [] successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.367 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[239] - Task plugin instance: SHELL create successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.367 +0800 o.a.d.p.t.s.ShellTask:[70] - Initialize shell task params {
  "localParams" : [ {
    "prop" : "raw_file_dir",
    "direct" : "OUT",
    "type" : "VARCHAR",
    "value" : ""
  } ],
  "varPool" : null,
  "rawScript" : "# move file to the reddit archive folder\nmv ${raw_file_dir} ${GNEWS_HOME}/archive\n",
  "resourceList" : [ ]
}
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[243] - Success initialized task plugin instance successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[246] - Set taskVarPool: [{"prop":"raw_file_dir","direct":"IN","type":"VARCHAR","value":"/local_storage/google_news/processing/987-20240505062610.json"},{"prop":"is_exists","direct":"IN","type":"VARCHAR","value":"1"}] successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Execute task instance  *************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[64] - Final Shell file is: 
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[65] - ****************************** Script Content *****************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[67] - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export PYTHON_LAUNCHER=/bin/python3.11
# move file to the reddit archive folder
mv /local_storage/google_news/processing/987-20240505062610.json /local_storage/google_news/archive

[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.s.BaseLinuxShellInterceptorBuilder:[68] - ****************************** Script Content *****************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.368 +0800 o.a.d.p.t.a.s.BaseShellInterceptor:[46] - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3655/992_3655.sh
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:26.386 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[154] - process start, process id is: 3373
[WI-0][TI-3655] - [INFO] 2024-05-05 07:39:27.330 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionRunningEventAckListenFunction:[44] - Receive TaskInstanceExecutionRunningEventAck: TaskInstanceExecutionRunningEventAck(taskInstanceId=3655, success=true)
[WI-0][TI-3655] - [INFO] 2024-05-05 07:39:27.334 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionInfoEventAckListenFunction:[45] - Receive TaskInstanceExecutionInfoEventAck: TaskInstanceExecutionInfoEventAck(success=true, taskInstanceId=3655)
[WI-0][TI-0] - [INFO] 2024-05-05 07:39:27.383 +0800 o.a.d.p.t.a.AbstractTask:[169] -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.387 +0800 o.a.d.p.t.a.AbstractCommandExecutor:[204] - process has exited. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3655, processId:3373 ,exitStatusCode:0 ,processWaitForStatus:true ,processExitValue:0
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - *********************************  Finalize task instance  ************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.388 +0800 o.a.d.p.t.a.l.TaskInstanceLogHeader:[1259] - ***********************************************************************************************
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.388 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[289] - Upload output files: [] successfully
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[294] - Send task execute status: SUCCESS to master : 172.18.1.1:1234
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[111] - Remove the current task execute context from worker cache
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.392 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[301] - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3655
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[316] - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13475238524230_33/992/3655
[WI-992][TI-3655] - [INFO] 2024-05-05 07:39:27.393 +0800 o.a.d.s.w.r.WorkerTaskExecutor:[342] - FINALIZE_SESSION
[WI-0][TI-3655] - [INFO] 2024-05-05 07:39:28.339 +0800 o.a.d.s.w.r.l.TaskInstanceExecutionFinishEventAckListenFunction:[44] - Receive TaskInstanceExecutionFinishEventAck: TaskInstanceExecutionFinishEventAck(taskInstanceId=3655, success=true)
[WI-0][TI-0] - [INFO] 2024-05-05 07:41:23.494 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7341040462427746 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:41:24.522 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7293447293447294 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:42:25.711 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7507082152974505 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:47:09.629 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.7616438356164383 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:47:10.639 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8089552238805969 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:47:11.640 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8885793871866295 is over then the MaxCpuUsagePercentageThresholds 0.7
[WI-0][TI-0] - [INFO] 2024-05-05 07:54:52.936 +0800 o.a.d.s.w.c.WorkerServerLoadProtection:[48] - Worker OverLoad: the TotalCpuUsedPercentage: 0.8067226890756303 is over then the MaxCpuUsagePercentageThresholds 0.7
