[INFO] 2024-04-29 00:41:34.402 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:41:34.413 +0800 - *********************************  Initialize task context  ***********************************
[INFO] 2024-04-29 00:41:34.414 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:41:34.414 +0800 - Begin to initialize task
[INFO] 2024-04-29 00:41:34.414 +0800 - Set task startTime: 1714322494414
[INFO] 2024-04-29 00:41:34.414 +0800 - Set task appId: 739_1995
[INFO] 2024-04-29 00:41:34.415 +0800 - End initialize task {
  "taskInstanceId" : 1995,
  "taskName" : "keyword filtering",
  "firstSubmitTime" : 1714322494375,
  "startTime" : 1714322494414,
  "taskType" : "PYTHON",
  "workflowInstanceHost" : "172.18.0.11:5678",
  "host" : "172.18.1.1:1234",
  "logPath" : "/opt/dolphinscheduler/logs/20240429/13377949373536/24/739/1995.log",
  "processId" : 0,
  "processDefineCode" : 13377949373536,
  "processDefineVersion" : 24,
  "processInstanceId" : 739,
  "scheduleTime" : 0,
  "globalParams" : "[{\"prop\":\"topics\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"\\\"reddit_post_preprocessed|reddit_post_filtered\\\"\"},{\"prop\":\"keywords\",\"direct\":\"IN\",\"type\":\"VARCHAR\",\"value\":\"[\\\"singapore\\\", \\\"sg\\\"]\"}]",
  "executorId" : 1,
  "cmdTypeIfComplement" : 0,
  "tenantCode" : "default",
  "processDefineId" : 0,
  "projectId" : 0,
  "projectCode" : 13010050770016,
  "taskParams" : "{\"localParams\":[],\"rawScript\":\"from pyspark.sql import SparkSession\\nfrom pyspark.sql.functions import col, from_json\\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\\n\\nspark = SparkSession.builder \\\\\\n    .master(\\\"local\\\") \\\\\\n    .appName(\\\"Reddit Keyword Filtering\\\") \\\\\\n    .config(\\\"spark.jars.packages\\\", \\\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\\\") \\\\\\n    .config(\\\"spark.sql.streaming.checkpointLocation\\\", r\\\"/tmp/\\\") \\\\\\n    .config(\\\"spark.driver.extraJavaOptions\\\", \\\"-Divy.cache.dir=/tmp -Divy.home=/tmp\\\") \\\\\\n    .getOrCreate()\\n\\n# define the schema for the JSON data\\nschema = StructType([\\n    StructField(\\\"user\\\", StringType(), nullable=True),\\n    StructField(\\\"content\\\", StringType(), nullable=True),\\n    StructField(\\\"url\\\", StringType(), nullable=True),\\n    StructField(\\\"context\\\", StringType(), nullable=True),\\n    StructField(\\\"score\\\", IntegerType(), nullable=True),\\n    StructField(\\\"type\\\", StringType(), nullable=True),\\n    StructField(\\\"id\\\", StringType(), nullable=True),\\n    StructField(\\\"datetime\\\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\\n])\\n\\n# parse the topics parameter\\ntopics = ${topics}\\nparsed_topics = topics.split(\\\"|\\\")\\n\\n# define the parameters for the input and output kafka broker and topic\\nkafka_broker = \\\"kafka:9092\\\"\\ninput_kafka_topic = parsed_topics[0]\\noutput_kafka_topic = parsed_topics[1]\\n\\n# Subscribe to the input topic\\ndf = spark \\\\\\n    .readStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"subscribe\\\", input_kafka_topic) \\\\\\n    .load()\\n\\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\\ndf = df.select(col(\\\"value\\\").cast(\\\"string\\\")) \\\\\\n    .withColumn(\\\"value\\\", from_json(\\\"value\\\", schema)) \\\\\\n\\nparsed_df = df \\\\\\n    .select(\\\"value.*\\\")\\n\\n# filter rows containing specific keywords\\nkeywords = ${keywords}\\n\\n# initialize the filter condition with False\\nfilter_condition = col(\\\"content\\\").contains(keywords[0])\\n\\n# loop through the rest of the keywords and update the filter condition\\nfor keyword in keywords[1:]:\\n    filter_condition = filter_condition | col(\\\"content\\\").contains(keyword)\\n\\n# apply the filter condition to the DataFrame\\nfiltered_df = parsed_df.filter(filter_condition)\\n\\n# stream the data to kafka\\nkafka_write = filtered_df \\\\\\n    .selectExpr(\\\"'' AS key\\\", \\\"to_json(struct(*)) AS value\\\") \\\\\\n    .writeStream \\\\\\n    .format(\\\"kafka\\\") \\\\\\n    .option(\\\"kafka.bootstrap.servers\\\", kafka_broker) \\\\\\n    .option(\\\"topic\\\", output_kafka_topic) \\\\\\n    .start()\\n\\n# Wait for the termination of the query\\nkafka_write.awaitTermination()\\n\",\"resourceList\":[]}",
  "environmentConfig" : "export HADOOP_HOME=/opt/hadoop-3.4.0\nexport SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3\nexport JAVA_HOME=/opt/java/openjdk\nexport PYSPARK_DRIVER_PYTHON=/bin/python3.11\nexport PYTHON_LAUNCHER=/bin/python3.11",
  "prepareParamsMap" : {
    "system.task.definition.name" : {
      "prop" : "system.task.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "keyword filtering"
    },
    "system.project.name" : {
      "prop" : "system.project.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : null
    },
    "REDDIT_HOME" : {
      "prop" : "REDDIT_HOME",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "/local_storage/reddit"
    },
    "keywords" : {
      "prop" : "keywords",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "[\"singapore\", \"sg\"]"
    },
    "topics" : {
      "prop" : "topics",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "\"reddit_post_preprocessed|reddit_post_filtered\""
    },
    "system.biz.curdate" : {
      "prop" : "system.biz.curdate",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240429"
    },
    "system.task.instance.id" : {
      "prop" : "system.task.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "1995"
    },
    "system.task.definition.code" : {
      "prop" : "system.task.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377752024032"
    },
    "system.datetime" : {
      "prop" : "system.datetime",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240429004134"
    },
    "system.project.code" : {
      "prop" : "system.project.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13010050770016"
    },
    "system.workflow.instance.id" : {
      "prop" : "system.workflow.instance.id",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "739"
    },
    "system.biz.date" : {
      "prop" : "system.biz.date",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "20240428"
    },
    "system.workflow.definition.name" : {
      "prop" : "system.workflow.definition.name",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "filter_data_subprocess"
    },
    "system.workflow.definition.code" : {
      "prop" : "system.workflow.definition.code",
      "direct" : "IN",
      "type" : "VARCHAR",
      "value" : "13377949373536"
    }
  },
  "taskAppId" : "739_1995",
  "taskTimeout" : 2147483647,
  "workerGroup" : "default",
  "delayTime" : 0,
  "currentExecutionStatus" : "SUBMITTED_SUCCESS",
  "endTime" : 0,
  "dryRun" : 0,
  "paramsMap" : { },
  "cpuQuota" : -1,
  "memoryMax" : -1,
  "testFlag" : 0,
  "logBufferEnable" : false,
  "dispatchFailTimes" : 0
}
[INFO] 2024-04-29 00:41:34.416 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:41:34.416 +0800 - *********************************  Load task instance plugin  *********************************
[INFO] 2024-04-29 00:41:34.417 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:41:34.419 +0800 - Send task status RUNNING_EXECUTION master: 172.18.1.1:1234
[INFO] 2024-04-29 00:41:34.420 +0800 - TenantCode: default check successfully
[INFO] 2024-04-29 00:41:34.421 +0800 - WorkflowInstanceExecDir: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995 check successfully
[INFO] 2024-04-29 00:41:34.421 +0800 - Create TaskChannel: org.apache.dolphinscheduler.plugin.task.python.PythonTaskChannel successfully
[INFO] 2024-04-29 00:41:34.421 +0800 - Download resources successfully: 
ResourceContext(resourceItemMap={})
[INFO] 2024-04-29 00:41:34.422 +0800 - Download upstream files: [] successfully
[INFO] 2024-04-29 00:41:34.422 +0800 - Task plugin instance: PYTHON create successfully
[INFO] 2024-04-29 00:41:34.422 +0800 - Initialize python task params {
  "localParams" : [ ],
  "varPool" : null,
  "rawScript" : "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, from_json\nfrom pyspark.sql.types import StringType, StructType, StructField, IntegerType\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"Reddit Keyword Filtering\") \\\n    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1\") \\\n    .config(\"spark.sql.streaming.checkpointLocation\", r\"/tmp/\") \\\n    .config(\"spark.driver.extraJavaOptions\", \"-Divy.cache.dir=/tmp -Divy.home=/tmp\") \\\n    .getOrCreate()\n\n# define the schema for the JSON data\nschema = StructType([\n    StructField(\"user\", StringType(), nullable=True),\n    StructField(\"content\", StringType(), nullable=True),\n    StructField(\"url\", StringType(), nullable=True),\n    StructField(\"context\", StringType(), nullable=True),\n    StructField(\"score\", IntegerType(), nullable=True),\n    StructField(\"type\", StringType(), nullable=True),\n    StructField(\"id\", StringType(), nullable=True),\n    StructField(\"datetime\", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed\n])\n\n# parse the topics parameter\ntopics = ${topics}\nparsed_topics = topics.split(\"|\")\n\n# define the parameters for the input and output kafka broker and topic\nkafka_broker = \"kafka:9092\"\ninput_kafka_topic = parsed_topics[0]\noutput_kafka_topic = parsed_topics[1]\n\n# Subscribe to the input topic\ndf = spark \\\n    .readStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"subscribe\", input_kafka_topic) \\\n    .load()\n\n# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data\ndf = df.select(col(\"value\").cast(\"string\")) \\\n    .withColumn(\"value\", from_json(\"value\", schema)) \\\n\nparsed_df = df \\\n    .select(\"value.*\")\n\n# filter rows containing specific keywords\nkeywords = ${keywords}\n\n# initialize the filter condition with False\nfilter_condition = col(\"content\").contains(keywords[0])\n\n# loop through the rest of the keywords and update the filter condition\nfor keyword in keywords[1:]:\n    filter_condition = filter_condition | col(\"content\").contains(keyword)\n\n# apply the filter condition to the DataFrame\nfiltered_df = parsed_df.filter(filter_condition)\n\n# stream the data to kafka\nkafka_write = filtered_df \\\n    .selectExpr(\"'' AS key\", \"to_json(struct(*)) AS value\") \\\n    .writeStream \\\n    .format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", kafka_broker) \\\n    .option(\"topic\", output_kafka_topic) \\\n    .start()\n\n# Wait for the termination of the query\nkafka_write.awaitTermination()\n",
  "resourceList" : [ ]
}
[INFO] 2024-04-29 00:41:34.423 +0800 - Success initialized task plugin instance successfully
[INFO] 2024-04-29 00:41:34.423 +0800 - Set taskVarPool: null successfully
[INFO] 2024-04-29 00:41:34.423 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:41:34.423 +0800 - *********************************  Execute task instance  *************************************
[INFO] 2024-04-29 00:41:34.423 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:41:34.424 +0800 - raw python script : from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, StructType, StructField, IntegerType

spark = SparkSession.builder \
    .master("local") \
    .appName("Reddit Keyword Filtering") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define the schema for the JSON data
schema = StructType([
    StructField("user", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("score", IntegerType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("id", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the topics parameter
topics = ${topics}
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# filter rows containing specific keywords
keywords = ${keywords}

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = parsed_df.filter(filter_condition)

# stream the data to kafka
kafka_write = filtered_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()

[INFO] 2024-04-29 00:41:34.424 +0800 - tenantCode :default, task dir:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995
[INFO] 2024-04-29 00:41:34.425 +0800 - generate python script file:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995/py_739_1995.py
[INFO] 2024-04-29 00:41:34.425 +0800 - #-*- encoding=utf8 -*-

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_json
from pyspark.sql.types import StringType, StructType, StructField, IntegerType

spark = SparkSession.builder \
    .master("local") \
    .appName("Reddit Keyword Filtering") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1") \
    .config("spark.sql.streaming.checkpointLocation", r"/tmp/") \
    .config("spark.driver.extraJavaOptions", "-Divy.cache.dir=/tmp -Divy.home=/tmp") \
    .getOrCreate()

# define the schema for the JSON data
schema = StructType([
    StructField("user", StringType(), nullable=True),
    StructField("content", StringType(), nullable=True),
    StructField("url", StringType(), nullable=True),
    StructField("context", StringType(), nullable=True),
    StructField("score", IntegerType(), nullable=True),
    StructField("type", StringType(), nullable=True),
    StructField("id", StringType(), nullable=True),
    StructField("datetime", StringType(), nullable=True)  # Assuming the datetime is string type, you can change it to TimestampType if needed
])

# parse the topics parameter
topics = "reddit_post_preprocessed|reddit_post_filtered"
parsed_topics = topics.split("|")

# define the parameters for the input and output kafka broker and topic
kafka_broker = "kafka:9092"
input_kafka_topic = parsed_topics[0]
output_kafka_topic = parsed_topics[1]

# Subscribe to the input topic
df = spark \
    .readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("subscribe", input_kafka_topic) \
    .load()

# parse the JSON data in the 'value' column using the defined schema then create a dataframe containing the reddit data
df = df.select(col("value").cast("string")) \
    .withColumn("value", from_json("value", schema)) \

parsed_df = df \
    .select("value.*")

# filter rows containing specific keywords
keywords = ["singapore", "sg"]

# initialize the filter condition with False
filter_condition = col("content").contains(keywords[0])

# loop through the rest of the keywords and update the filter condition
for keyword in keywords[1:]:
    filter_condition = filter_condition | col("content").contains(keyword)

# apply the filter condition to the DataFrame
filtered_df = parsed_df.filter(filter_condition)

# stream the data to kafka
kafka_write = filtered_df \
    .selectExpr("'' AS key", "to_json(struct(*)) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", kafka_broker) \
    .option("topic", output_kafka_topic) \
    .start()

# Wait for the termination of the query
kafka_write.awaitTermination()

[INFO] 2024-04-29 00:41:34.430 +0800 - Final Shell file is: 
[INFO] 2024-04-29 00:41:34.433 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-04-29 00:41:34.435 +0800 - #!/bin/bash
BASEDIR=$(cd `dirname $0`; pwd)
cd $BASEDIR
export HADOOP_HOME=/opt/hadoop-3.4.0
export SPARK_HOME=/opt/spark-3.5.1-bin-hadoop3
export JAVA_HOME=/opt/java/openjdk
export PYSPARK_DRIVER_PYTHON=/bin/python3.11
export PYTHON_LAUNCHER=/bin/python3.11
${PYTHON_LAUNCHER} /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995/py_739_1995.py
[INFO] 2024-04-29 00:41:34.437 +0800 - ****************************** Script Content *****************************************************************
[INFO] 2024-04-29 00:41:34.437 +0800 - Executing shell command : sudo -u default -i /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995/739_1995.sh
[INFO] 2024-04-29 00:41:34.451 +0800 - process start, process id is: 2233
[INFO] 2024-04-29 00:41:34.451 +0800 -  -> 
	sudo: unable to change directory to /home/default: No such file or directory
[INFO] 2024-04-29 00:41:39.457 +0800 -  -> 
	:: loading settings :: url = jar:file:/opt/spark-3.5.1-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
	Ivy Default Cache set to: /tmp
	The jars for the packages stored in: /tmp/jars
	org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
	:: resolving dependencies :: org.apache.spark#spark-submit-parent-c0f7e56c-080d-40ed-9b12-505dea8e3596;1.0
		confs: [default]
[INFO] 2024-04-29 00:41:45.469 +0800 -  -> 
		found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central
		found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central
[INFO] 2024-04-29 00:41:46.472 +0800 -  -> 
		found org.apache.kafka#kafka-clients;3.4.1 in central
		found org.lz4#lz4-java;1.8.0 in central
[INFO] 2024-04-29 00:41:47.474 +0800 -  -> 
		found org.xerial.snappy#snappy-java;1.1.10.3 in central
[INFO] 2024-04-29 00:41:48.476 +0800 -  -> 
		found org.slf4j#slf4j-api;2.0.7 in central
[INFO] 2024-04-29 00:41:52.516 +0800 -  -> 
		found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
		found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[INFO] 2024-04-29 00:41:56.521 +0800 -  -> 
		found commons-logging#commons-logging;1.1.3 in central
		found com.google.code.findbugs#jsr305;3.0.0 in central
[INFO] 2024-04-29 00:42:00.530 +0800 -  -> 
		found org.apache.commons#commons-pool2;2.11.1 in central
[INFO] 2024-04-29 00:42:01.531 +0800 -  -> 
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...
		[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (707ms)
	downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...
[INFO] 2024-04-29 00:42:02.534 +0800 -  -> 
		[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (406ms)
	downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[INFO] 2024-04-29 00:42:03.536 +0800 -  -> 
		[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (1175ms)
	downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
		[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (431ms)
	downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[INFO] 2024-04-29 00:42:04.537 +0800 -  -> 
		[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (635ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[INFO] 2024-04-29 00:42:08.549 +0800 -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (3812ms)
	downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
		[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (474ms)
	downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[INFO] 2024-04-29 00:42:09.562 +0800 -  -> 
		[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (601ms)
	downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
		[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (402ms)
	downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[INFO] 2024-04-29 00:42:11.573 +0800 -  -> 
		[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (2321ms)
[INFO] 2024-04-29 00:42:12.574 +0800 -  -> 
	downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
		[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (417ms)
	:: resolution report :: resolve 21211ms :: artifacts dl 11398ms
		:: modules in use:
		com.google.code.findbugs#jsr305;3.0.0 from central in [default]
		commons-logging#commons-logging;1.1.3 from central in [default]
		org.apache.commons#commons-pool2;2.11.1 from central in [default]
		org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
		org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
		org.apache.kafka#kafka-clients;3.4.1 from central in [default]
		org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]
		org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]
		org.lz4#lz4-java;1.8.0 from central in [default]
		org.slf4j#slf4j-api;2.0.7 from central in [default]
		org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
		---------------------------------------------------------------------
		|                  |            modules            ||   artifacts   |
		|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
		---------------------------------------------------------------------
		|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |
		---------------------------------------------------------------------
	:: retrieving :: org.apache.spark#spark-submit-parent-c0f7e56c-080d-40ed-9b12-505dea8e3596
		confs: [default]
		11 artifacts copied, 0 already retrieved (56767kB/145ms)
[INFO] 2024-04-29 00:42:13.576 +0800 -  -> 
	24/04/29 00:42:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[INFO] 2024-04-29 00:42:27.668 +0800 -  -> 
	24/04/29 00:42:26 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[INFO] 2024-04-29 00:42:29.672 +0800 -  -> 
	24/04/29 00:42:29 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[INFO] 2024-04-29 00:42:34.680 +0800 -  -> 
	
	[Stage 0:>                                                          (0 + 1) / 1]
[INFO] 2024-04-29 00:42:35.682 +0800 -  -> 
	
	                                                                                
[INFO] 2024-04-29 00:42:44.400 +0800 - process id:2233, cmd:sudo -u default kill -9 2233 2237 2240 2243 2244 2262 2263 2264 2265 2266 2267 2268 2269 2270 2271 2272 2273 2274 2275 2287 2289 2290 2292 2293 2294 2295 2296 2297 2298 2299 2300 2301 2302 2303 2304 2305 2306 2307 2308 2309 2310 2311 2312 2313 2314 2315 2316 2339 2340 2341 2342 2343 2344 2345 2346 2347 2370 2371 2394 2395 2396 2397 2398 2399 2401 2428 2433 2483 2484 2291 2400
[ERROR] 2024-04-29 00:42:44.487 +0800 - kill task error
org.apache.dolphinscheduler.common.shell.AbstractShell$ExitCodeException: kill: (2233): Operation not permitted
kill: (2263): No such process
kill: (2264): No such process
kill: (2265): No such process
kill: (2266): No such process
kill: (2267): No such process
kill: (2268): No such process
kill: (2269): No such process
kill: (2270): No such process
kill: (2271): No such process
kill: (2272): No such process
kill: (2273): No such process
kill: (2274): No such process
kill: (2275): No such process
kill: (2287): No such process
kill: (2289): No such process
kill: (2290): No such process
kill: (2292): No such process
kill: (2293): No such process
kill: (2294): No such process
kill: (2295): No such process
kill: (2296): No such process
kill: (2297): No such process
kill: (2298): No such process
kill: (2299): No such process
kill: (2300): No such process
kill: (2301): No such process
kill: (2302): No such process
kill: (2303): No such process
kill: (2304): No such process
kill: (2305): No such process
kill: (2306): No such process
kill: (2307): No such process
kill: (2308): No such process
kill: (2309): No such process
kill: (2310): No such process
kill: (2311): No such process
kill: (2312): No such process
kill: (2313): No such process
kill: (2314): No such process
kill: (2315): No such process
kill: (2316): No such process
kill: (2339): No such process
kill: (2340): No such process
kill: (2341): No such process
kill: (2342): No such process
kill: (2343): No such process
kill: (2344): No such process
kill: (2345): No such process
kill: (2346): No such process
kill: (2347): No such process
kill: (2370): No such process
kill: (2371): No such process
kill: (2394): No such process
kill: (2395): No such process
kill: (2396): No such process
kill: (2397): No such process
kill: (2398): No such process
kill: (2399): No such process
kill: (2401): No such process
kill: (2428): No such process
kill: (2433): No such process
kill: (2483): No such process
kill: (2484): No such process
kill: (2291): No such process
kill: (2400): No such process

	at org.apache.dolphinscheduler.common.shell.AbstractShell.runCommand(AbstractShell.java:205)
	at org.apache.dolphinscheduler.common.shell.AbstractShell.run(AbstractShell.java:118)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execute(ShellExecutor.java:125)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:103)
	at org.apache.dolphinscheduler.common.shell.ShellExecutor.execCommand(ShellExecutor.java:86)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeShell(OSUtils.java:345)
	at org.apache.dolphinscheduler.common.utils.OSUtils.exeCmd(OSUtils.java:334)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.killProcess(TaskInstanceKillOperationFunction.java:135)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.doKill(TaskInstanceKillOperationFunction.java:96)
	at org.apache.dolphinscheduler.server.worker.runner.operator.TaskInstanceKillOperationFunction.operate(TaskInstanceKillOperationFunction.java:69)
	at org.apache.dolphinscheduler.server.worker.rpc.TaskInstanceOperatorImpl.killTask(TaskInstanceOperatorImpl.java:49)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.dolphinscheduler.extract.base.server.ServerMethodInvokerImpl.invoke(ServerMethodInvokerImpl.java:41)
	at org.apache.dolphinscheduler.extract.base.server.JdkDynamicServerHandler.lambda$processReceived$0(JdkDynamicServerHandler.java:108)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
[INFO] 2024-04-29 00:42:44.504 +0800 - Get appIds from worker 172.18.1.1:1234, taskLogPath: /opt/dolphinscheduler/logs/20240429/13377949373536/24/739/1995.log
[INFO] 2024-04-29 00:42:44.527 +0800 - Start finding appId in /opt/dolphinscheduler/logs/20240429/13377949373536/24/739/1995.log, fetch way: log 
[INFO] 2024-04-29 00:42:44.548 +0800 - The appId is empty
[INFO] 2024-04-29 00:42:44.550 +0800 - Begin to kill process process, pid is : 2233
[INFO] 2024-04-29 00:42:44.551 +0800 - Success kill task: 739_1995, pid: 2233
[INFO] 2024-04-29 00:42:44.552 +0800 - kill task by cancelApplication, taskInstanceId: 1995
[INFO] 2024-04-29 00:42:45.328 +0800 - process has killed. execute path:/tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995, processId:2233 ,exitStatusCode:137 ,processWaitForStatus:true ,processExitValue:137
[INFO] 2024-04-29 00:42:45.329 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:42:45.329 +0800 - *********************************  Finalize task instance  ************************************
[INFO] 2024-04-29 00:42:45.329 +0800 - ***********************************************************************************************
[INFO] 2024-04-29 00:42:45.330 +0800 - Upload output files: [] successfully
[INFO] 2024-04-29 00:42:45.337 +0800 - Send task execute status: KILL to master : 172.18.1.1:1234
[INFO] 2024-04-29 00:42:45.339 +0800 - Remove the current task execute context from worker cache
[INFO] 2024-04-29 00:42:45.340 +0800 - The current execute mode isn't develop mode, will clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995
[INFO] 2024-04-29 00:42:45.342 +0800 - Success clear the task execute file: /tmp/dolphinscheduler/exec/process/default/13010050770016/13377949373536_24/739/1995
[INFO] 2024-04-29 00:42:45.343 +0800 - FINALIZE_SESSION
